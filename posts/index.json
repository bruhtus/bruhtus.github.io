[{"ref":"https://bruhtus.github.io/posts/pointer-to-array-in-c/","title":"Pointer to Array in C: Compression File Analogy","section":"posts","tags":["C"],"date":"2025.12.24","body":"DISCLAIMER Keep in mind that this is only an analogy to help newcomer understand pointer to array in C programming language. This does not mean there\u0026rsquo;s any sort of compression when we use pointer to array.\n\u0026amp;arr vs \u0026amp;arr[0] This is the analogy that help me understand what is pointer to array in C programming language, which can be confusing for new people that try learning C programming language for the first time.\nIn case you didn\u0026rsquo;t know, we can get the address of an array using the address-of operator like \u0026amp;array. Here\u0026rsquo;s an example:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { int arr[2]; printf(\u0026#34;\u0026amp;arr: %p\\n\u0026#34;, \u0026amp;arr); // Address to _whole array_.  printf(\u0026#34;\u0026amp;arr[0]: %p\\n\u0026#34;, \u0026amp;arr[0]); // Address to _element of array_.  return 0; } From the example above, you might be thinking, \u0026ldquo;isn\u0026rsquo;t \u0026amp;arr and \u0026amp;arr[0] print out the same value, what is the difference?\u0026rdquo;. The difference is not in the value itself, but in the implication.\n\u0026amp;arr and \u0026amp;arr[0] (or we can just use arr, because array name represent the index 0 memory address) has the same value, which is the memory address at index 0 because the beginning of the array is also at the index 0 of the array. The difference is at the end.\nWhen we use \u0026amp;arr, the end of that expression is the end on the last element of array. Let\u0026rsquo;s say we store integer value in 4 bytes and each memory address correspond to 1 byte. The whole array memory address can be represented like this (the empty line used to give clarity, the actual memory scheme is contiguous):\n001 \u0026lt;- the beginning of index 0 __and__ the beginning of array 002 003 004 \u0026lt;- the end of index 0 005 \u0026lt;- the beginning of index 1 006 007 008 \u0026lt;- the end of index 1 __and__ the end of array With the representation above, the expression \u0026amp;arr means that we got the beginning of array until the end of array. Meanwhile, the expression \u0026amp;arr[0] means that we got the beginning of index 0 until the end of index 0.\nPointer to Array With that out of the way, let\u0026rsquo;s get into pointer to array. We can declare pointer to array like this:\nchar (*arr)[4]; What the above declaration means is that \u0026ldquo;a pointer to an array of 4 char\u0026rdquo;. And then, you might be thinking, \u0026ldquo;how do we give value to those arr variable?\u0026rdquo;. One of them is like this:\nchar anu[4] = {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;\\0\u0026#39;}; char (*arr)[4] = \u0026amp;anu; // Assigning memory address of anu array. Another one is like this:\nchar (*arr)[4] = \u0026amp;\u0026#34;abc\u0026#34;; // Assigning memory address of string literal. Or we can split those initialization with declaration and assignment like this:\nchar (*arr)[4]; // Declaration of arr variable. arr = \u0026amp;\u0026#34;abc\u0026#34;; // Assignment of arr variable. Let\u0026rsquo;s talk about the first example. In the first example, we are using memory address of anu array of 4 characters for the value of arr, which is \u0026ldquo;a pointer to the array of 4 characters\u0026rdquo;.\nNow the question is that, how do we get the character b from arr variable? You might be thinking, \u0026ldquo;can\u0026rsquo;t we just use indexing like arr[1]?\u0026rdquo; Remember that we got the whole array inside arr variable and square bracket or array subscript is another form of dereference. So when we are trying to deference the arr variable like *arr or arr[0], we got the whole array instead of element of array. And when we do arr[1] for char (*arr)[4];, we are actually trying to access the next memory after the whole array which is unallocated memory and result in undefined behavior. We can\u0026rsquo;t be sure what is in there.\n Keep in mind that arr[1] is equivalent to *(arr + 1), which means that we are dereferencing 1 offset from the index 0 arr memory address. 1 offset in here depends on how many bytes the variable type stored in memory.\n As for the second and third example, we need to know that string literal have a lifetime of static storage duration, which means that string literal will exist the entire execution of the program.\nWith that in mind, the expression \u0026amp;\u0026quot;abc\u0026quot; means that we got the memory address of the entire string literal, which in a way is an array of characters with different storage type than char []. If i understand correctly, char [] has automatic storage duration, so it will be deallocated at some point during the execution, usually when the function returns or the declaration goes out of scope.\nThe expression char (*arr)[4] = \u0026amp;\u0026quot;abc\u0026quot;; means that we are passing the memory address of the entire string literal to arr variable.\nSo, how do we access the character in char (*arr)[4]? Well, we need to dereferencing the arr twice. The first dereference, like *arr or arr[0], is to get the entire array and the second dereference, like (*arr)[1] or *((*arr) + 1) or arr[0][1], is to get the element of array.\n Why do we use bracket around arr variable like (*arr)[1]? Because square bracket or array subscript have a higher priority than the dereference operator. If we write the expression like *arr[1], that expression is equal to *(arr[1]). We are parsing the square bracket or array subscript first and then parsing the dereference operator. By adding bracket, we can overwrite the default priority. This is called precedence.\nThe precedence only control how expression are parsed and which operators are grouped which operands. Precedence do not control the order of evaluation. To control the order of evaluation, we need to use sequence point.\n Compression File Analogy If you are new to C programming language, like i\u0026rsquo;m at the time of writing this blog post, you might get confused by the concept of \u0026ldquo;pointer to array\u0026rdquo;. If you are confused, this analogy might help you to understand what is going on.\nImagine we have a ZIP file, which is one of the compression file format. Inside those ZIP file, we have directory called 001. Inside those directory, we have a file called 001 and 005 (see the representation of whole array at the beginning of the post).\nHere\u0026rsquo;s the simple representation:\n The file 001 and 005 represent the array\u0026rsquo;s elements. The directory 001 represent the array itself. The ZIP file represent the pointer to the array.  Here\u0026rsquo;s the simple illustration:\npointer.zip |_ 001/ |_ 001 |_ 005 So, if we want to get the element inside the array 001/, we need to extract the pointer.zip first and then accessing the element inside the array 001/.\nBonus: accessing character of string literal Here\u0026rsquo;s another interesting thing about string literal that you might want to try:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;%c\\n\u0026#34;, \u0026#34;abc\u0026#34;[0]); return 0; } What is the output? Now, try to analyze what\u0026rsquo;s going on :)\nAlright, that\u0026rsquo;s it. See you next time!\nReferences  Stackoverflow answer about precedence and associativity . Example memory address of string literal . log2base2 explanation about pointer to array . Stackoverflow answer about dereferencing pointer to array . C-faq about 2-dimensional array .  "},{"ref":"https://bruhtus.github.io/posts/lvalue-in-c/","title":"Lvalue in C","section":"posts","tags":["C"],"date":"2025.12.23","body":"A Brief Into I am always confused when trying to understand what is considered an lvalue in C programming language, so let me try to explain that to make me have a better understanding of lvalue in C.\nWhat is Lvalue? From C committee draft definition :\n An lvalue is an expression (with an object type other than void) that potentially designates an object; if an lvalue does not designate an object when it is evaluated, the behavior is undefined.\n And from C committee draft the origin of lvalue term :\n The name \u0026ldquo;lvalue\u0026rdquo; comes originally from the assignment expression E1 = E2, in which the left operand E1 is required to be a (modifiable) lvalue. It is perhaps better considered as representing an object \u0026ldquo;locator value\u0026rdquo;.\n Here\u0026rsquo;s another explanation about lvalue from GNU C intro:\n An expression that identifies a memory space that holds a value is called an lvalue, because it is a location that can hold a value.\n With that in mind, lvalue is an expression that refers to the region of storage that can hold a value. Those storage can be memory or something else.\n The region of storage that can hold a value is called object.\n I think calling lvalue as locator value make it easier to understand than the usual terminology left value, because lvalue might not be on the left side of the statement.\nPointer Dereference is Lvalue Here\u0026rsquo;s an example of lvalue can be on the left side or right side:\nint num = 68; int *ptr = \u0026amp;num; /* * *ptr is an lvalue that\u0026#39;s on the left and right of statement. * The *ptr on the right undergo conversion from _lvalue to rvalue_. */ *ptr = *ptr + 1; The expression *ptr from example above is an lvalue because it refers to the memory space that can hold a value. Keep in mind that ptr is a variable that stored memory address as its value, that means ptr stored the address of another region of storage. And dereferencing ptr or *ptr means that we are using those region of storage stored in ptr.\nLet\u0026rsquo;s say we have memory space in address 001 and 002, the num variable in previous example using the memory space at address 001 and the ptr variable using the memory space at address 002 which stored 001 inside it. Here\u0026rsquo;s a simple representation:\n num used memory space at 001 and stored value 68 inside of memory. ptr used memory space at 002 and stored value 001 inside of memory.  Variable is Lvalue When we define a variable like this:\nint num = 69; num is an lvalue because num refers to the memory space that hold value 69.\nNon-modifiable Lvalue Apparently lvalue is not always modifiable, there\u0026rsquo;s some region of storage we can not modify.\nFrom C committee draft about lvalue :\n A modifiable lvalue is an lvalue that does not have array type, does not have an incomplete type, does not have a const-qualified type, and if it is a structure or union, does not have any member (including, recursively, any member or element of all contained aggregates or unions) with a const-qualified type.\n Basically we can not modify lvalue if it has:\n Array type. Incomplete type. Constant type. A structure or unions with one of its members as constant type.  Other than that, i assume we can modify the lvalue (?). I\u0026rsquo;m still not sure yet, at least by the time of writing this post. But the point is that, we can change the value in lvalue depending on lvalue modifiable or not. If the lvalue modifiable, we can change the value, if not, then we can\u0026rsquo;t change the value.\nAnyway, let\u0026rsquo;s check the constraint one by one.\nArray Type When we declare array like this:\nint array[2]; From what i understand, array is an lvalue because it refers to the memory space that can hold a value but, by the C standard, we can\u0026rsquo;t modify those lvalue with array type like this:\nint array[2]; array = 69; // This is invalid! I still have no idea why that\u0026rsquo;s the case, but if you are curious, you can read the development of the C language .\nIncomplete Type Incomplete type in here means that the type lacks size information, for example:\nstruct anu; anu is incomplete type because there is not enough information for the compiler to determine how much storage to set aside for struct anu type.\nHere\u0026rsquo;s another example of incomplete type:\nint itu[]; union ini; We cannot use the incomplete type like this:\nstruct anu nganu; unless we complete the type.\nBut, we can create a pointer to incomplete type like this:\nstruct anu *nganu; without completing the definition of struct anu. That is because a pointer just storing the memory address and we only need to know the pointer size, we don\u0026rsquo;t need to know the type size.\nConstant Type Well, as far as i understand, constant in C is read-only, which means that we can\u0026rsquo;t really change that.\nConstant Type in Struct or Union I\u0026rsquo;m still not sure about this one, but i kind of found an example. Here\u0026rsquo;s an example that will throw an error because of constant type in struct (using gcc):\nstruct something { const int size; }; int main(void) { struct something very_big; very_big.size = 69; return 0; } For some reason, the example above, which separate the declaration and assignment, trigger an error assignment of read-only member 'size', but if we use the initialization like this:\nstruct something { const int size; }; int main(void) { struct something very_big = {.size = 69}; return 0; } The example above didn\u0026rsquo;t give any error. So maybe the non-modifiable is during the assignment (?). Well, i\u0026rsquo;m still not sure.\nBonus: brief intro to rvalue This post is mainly exploring about lvalue because lvalue make me confused the most. Especially the dereference pointer statement, when i see some reference write about \u0026ldquo;*ptr is lvalue\u0026rdquo;, i\u0026rsquo;m like \u0026ldquo;but why can i put it on the right side like *ptr = *ptr + 1;?\u0026rdquo;. That is the primary reason i\u0026rsquo;m creating this post.\nBut other people usually also talking about rvalue. Here\u0026rsquo;s a footnotes from C committee draft :\n What is sometimes called \u0026ldquo;rvalue\u0026rdquo; is in this International Standard described as the \u0026ldquo;value of an expression\u0026rdquo;\n And that\u0026rsquo;s what i think about rvalue at the time of writing this post, a value of an expression. At the moment, i don\u0026rsquo;t really want to dive deep into what is considered an rvalue. One step at a time.\nThat\u0026rsquo;s it from me, see you next time!\nReferences  The Development of the C Language  Stackoverflow answer about lvalue in C . GNU C intro definition about lvalue in C . C committee draft about lvalue term origin . C committee draft about object . C committee draft lvalue definition . Stackoverflow answer why array type is a non-modifiable lvalue . Stackoverflow answer array type object is not modifiable . Eli\u0026rsquo;s post about lvalue and rvalue in C and C++ . cppreference about value categories .  "},{"ref":"https://bruhtus.github.io/posts/shell-redirection-and-process-substitution/","title":"Shell Redirection and Process Substitution Combination","section":"posts","tags":["Shell"],"date":"2025.12.17","body":"A Brief Intro Recently i watch \u0026ldquo;Bread on Penguins\u0026rdquo; video about shell process substitution on youtube. I already know about shell process substitution and write the blog post about shell process substitution as temp file , but what is interesting from the video is the combination of shell process substitution and shell redirection that i didn\u0026rsquo;t think about previously.\nShell Redirection If you didn\u0026rsquo;t know about shell redirection, it\u0026rsquo;s basically redirecting the output of a program to a file. Here\u0026rsquo;s an example of shell redirection:\necho \u0026#39;something\u0026#39; \u0026gt; big.txt The example above means that we write something to file big.txt. Keep in mind that the example about will replace any contents in the big.txt file, so be careful.\nIf we want to append, we can use \u0026gt;\u0026gt; operator like this:\necho \u0026#39;something\u0026#39; \u0026gt;\u0026gt; big.txt For more info, you can check on the posix specification about redirection in then references section below.\nShell Process Substitution Shell process substitution is basically a mechanism to save the output of a process inside a \u0026ldquo;file descriptor\u0026rdquo; and we can use those \u0026ldquo;file descriptor\u0026rdquo; like a temporary file, which means those \u0026ldquo;file descriptor\u0026rdquo; will disappear after the current command that use process substitution is done.\nIf you are still confused, think of the process like primary process and subprocess. The current command that use process substitution is the primary process and the process inside the process substitution is the subprocess.\nHere\u0026rsquo;s an example:\ndiff \u0026lt;(sort file1) \u0026lt;(sort file2) In the example about, we are sorting file1 and file2 first and the save the result in 2 \u0026ldquo;file descriptor\u0026rdquo;. And then we run diff command with those \u0026ldquo;file descriptor\u0026rdquo; like a regular file, and after diff command done, those \u0026ldquo;file descriptor\u0026rdquo; is gone.\nIn case you are wondering \u0026ldquo;what is file descriptor?\u0026rdquo;, let me try explaining that. I mean, by the time of writing this blog post, i\u0026rsquo;m still not sure yet what is \u0026ldquo;file descriptor\u0026rdquo; but i know that it\u0026rsquo;s not a regular file.\nWhen we open an existing file or create a new file, the kernel returns a \u0026ldquo;file descriptor\u0026rdquo; to the process, and as far as i understand, \u0026ldquo;file descriptor\u0026rdquo; is an information about opened file.\nSo what shell process substitution does is using those \u0026ldquo;file descriptor\u0026rdquo; mechanism as a temporary file to store the output of another process, which is quite neat.\nBut, keep in mind that not all shell have process substitution, the one that i know have process substitution is bash and zsh. So before you try it, please check if your shell have process substitution or not.\nCombination This is the example from the youtube video that is interesting, we can combine the shell redirection and shell process substitution to separate the output and error from a command.\nLet\u0026rsquo;s say we have a command called something and we want to check the output of those command in less command and also save the error messages in a file called errors.txt. We can do that like this:\nsomething \u0026gt; \u0026gt;(less) 2\u0026gt; \u0026gt;(tee errors.txt) The symbol \u0026gt; is the same as 1\u0026gt; which means redirect the output of the program, and symbol 2\u0026gt; means redirect the error messages of the program. If you want to know more about it, look up stdout and stderr.\nWith that in mind, with \u0026gt; \u0026gt;(less) means we are redirecting the output of something to process substitution with command less which i assume passing the output to less stdin (or the input) and similar to piping like something | less (?), i\u0026rsquo;m still not sure how it works. But the difference between piping and process substitution is that piping only able to run one process at time, meanwhile process substitution can run multiple process simultaneously, at least that is from my observation. Again, i\u0026rsquo;m still not sure how that work yet.\nAnd with 2\u0026gt; \u0026gt;(tee errors.txt) means we are redirecting the error messages of something to process substitution with command tee errors.txt. I guess it\u0026rsquo;s like we are passing something error message to tee stdin and save those input in errors.txt file.\nHere\u0026rsquo;s another example:\nsort file \u0026gt; \u0026gt;(diff file -) The command above is basically check the difference between unsorted and sorted file. We provide - in diff file - to take the stdin as the input for the second argument.\nAt first glance, that might look the same as sort file | diff file -, but in my observation, there\u0026rsquo;s a slight difference. When using sort file \u0026gt; \u0026gt;(diff file -), my shell prompt does not appear but i can execute command like normal. And when using sort file | diff file - or diff file \u0026lt;(sort file), my shell prompt appear like normal. Still not sure what is the difference.\nAlright, that\u0026rsquo;s it for now. See you next time!\nReferences  Bread on Penguins youtube video . POSIX specification about redirection . POSIX specification about file descriptor . Medium post about /proc/self/fd . Stackoverflow answer about file descriptor .  "},{"ref":"https://bruhtus.github.io/posts/square-bracket-as-dereference-operator-in-c/","title":"Square Bracket as Dereference Operator in C","section":"posts","tags":["C"],"date":"2025.12.15","body":"When we learn another programming language for the first time, we were told that to access an array, we can use indexing. For example, let\u0026rsquo;s say we have an array with 5 items and the index array start at 0. With that in mind, we can access the 5th item in those array like array[4] (remember, the index start from 0, so it\u0026rsquo;s 0, 1, 2, 3, 4).\nBut in C programming language, array can \u0026ldquo;decay\u0026rdquo; into pointer, which means that in some cases, array can be treated like a pointer. Let\u0026rsquo;s say we have this array initialization:\nint array[2] = {69, 42}; And then, when we print the memory address of that array, we found out that the memory address of the variable array is equivalent to the memory address of the index 0 of array. We can check that by printing the memory address like this:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { int array[2] = {69, 42}; printf(\u0026#34;%p\\n\u0026#34;, array); // The memory address of array variable.  printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;array[0]); // The memory address of index 0 or first item.  return 0; } Now, you might be thinking \u0026ldquo;what is the correlation between that and dereference operator?\u0026rdquo;. Well, the thing is, square bracket is not only for array but can also be used with pointer in general.\nRemember that array stored as a \u0026ldquo;contiguous\u0026rdquo; block in the memory, which means that if we have the array with 2 items and the memory address of index 0 is 001, the memory address of index 1 is after the index 0.\nFor example, if our machine use 4 bytes to store integer value and we have 2 integer values in the array, the array take up 8 bytes in memory. With that in mind, if we have the memory address of the index 0 of that array as 001, then the memory address of index 1 is 005. Here\u0026rsquo;s what\u0026rsquo;s going on (assuming 1 memory address equal to 1 byte):\n Memory address 001 until 004 is for index 0. Memory address 005 until 010 is for index 1.  When we are dealing with array, we are also dealing with memory address as the value, not the actual value itself. And what is a pointer again? A variable that have memory address as a value. I think this is where the misunderstanding come around. As far as i understand, array can store more than 1 memory address but pointer can only store 1 memory address at a time. That\u0026rsquo;s why even though we are dealing with the same memory address as a value, the treatment for array and pointer is not the same. The similarity between array and pointer is dereference operation of memory address as a value.\nWhen we want to get the value stored in the memory address that the pointer had, we need to dereference those pointer. Here\u0026rsquo;s an example:\nint main(void) { int life = 42; int *p_life = \u0026amp;life; int age = *p_life; // Dereference pointer p_life to get value 42.  return 0; } In the previous example, we use asterisk (*) as a dereference operator, but we can also use square bracket like accessing array to dereference the pointer like this:\nint main(void) { int life = 42; int *p_life = \u0026amp;life; int age = p_life[0]; // Dereference pointer p_life to get value 42.  return 0; } When we are using the square bracket, we can think of the number inside those square bracket as offset. From the cplusplus.com tutorial , it seems like the square bracket is called offset operator. This means that when we do p_life[0], we are dereferencing a pointer with offset 0 and offset 0 means the current memory address. So if we have p_life[1], it means we are dereferencing a pointer p_life with offset 1 and the memory address that we are trying to dereferencing is 1 block of memory address after the stored memory address in pointer p_life, which we have or not have access to.\nTo make things easier to understand, let\u0026rsquo;s say p_life pointer stored memory address 001 that has integer value in it, and our machine use 4 bytes to store integer value in memory (similar to the previous array example). With that in mind, when we do p_life[0], we are dereferencing memory address 001. And when we do p_life[1], we are dereferencing memory address 005.\nIf you are still confused why we have 005 for the p_life[1], it\u0026rsquo;s because we take up 4 bytes of memory address with 001, 002, 003, and 004 (assuming 1 byte equal to 1 memory address) for p_life or p_life[0] (p_life with offset 0), so the start of the next block of memory is 005. That\u0026rsquo;s why p_life[1] memory address is 005.\nHere\u0026rsquo;s a representation in code:\nint main(void) { int life = 42; // life has 001 memory address.  int *p_life = \u0026amp;life; // Saving 001 memory address into p_life pointer.  int age = p_life[0]; // Getting the value from 001 memory address.  return 0; } With that in mind, the expression \u0026amp;array[0] can also be written as \u0026amp;(*array) which means get the memory address of \u0026ldquo;dereferencing array with offset 0\u0026rdquo; (?), yeah, i\u0026rsquo;m not sure what that\u0026rsquo;s called.\nWe can also using offset with asterisk operator too. Let\u0026rsquo;s say we want to dereference array with offset 1, we can do array[1] or *(array + 1) to dereference array with offset 1.\nMaybe indexing of array in another programming language is also dereference operator under the hood? I am not sure. Anyway, this is something interesting for me to learn and that\u0026rsquo;s why i write a blog post about it. That\u0026rsquo;s it, see you next time!\n"},{"ref":"https://bruhtus.github.io/posts/artificial-intelligence-replacement-point-of-view/","title":"Artificial Intelligence: Replacement Point of View","section":"posts","tags":["Reflections"],"date":"2025.12.10","body":"When we talk about something that have \u0026ldquo;artificial\u0026rdquo; in it, we tend to associate that with something that \u0026ldquo;does not exist naturally\u0026rdquo;. But recently i read article titled Bag of words, have mercy on us , and this footnote caught my attention:\n Even the word “artificial” is wrong, because it menacingly implies replacement. Artificial sweeteners, flowers, legs—these are things we only use when we can’t have the real deal. So what part of intelligence, exactly, are we so intent on replacing?\n Well, i guess we can interpret \u0026ldquo;artificial\u0026rdquo; like that. And that footnote make wonder, what are we trying to replace with artificial intelligence?\nSome people might be saying \u0026ldquo;no one is gonna be replaced by artificial intelligence\u0026rdquo;, well, it depends. The problem with the current Artificial Intelligence (a.k.a AI) hype cycle is that some people too dependent on it, and what this implies is that, these people might be limiting themselves to the output of AI. So if the AI did not have the answer for them, they might not be able to find the answer themselves. In a way, we can think of this as a \u0026ldquo;replacement\u0026rdquo; process. Some people\u0026rsquo;s intelligence is slowly being replaced by artificial intelligence, and they might not even aware of it.\nWhen i heard some people said that they can\u0026rsquo;t live without AI anymore, and i feel like there\u0026rsquo;s some problem here, but i don\u0026rsquo;t know what the problem is. And when i read those footnote, the previous statement that some people can\u0026rsquo;t live without AI anymore make me thinking, \u0026ldquo;are we in the replacement process already?\u0026rdquo;.\nSo here\u0026rsquo;s another point of view, AI is not replacing the human themselves but replacing the human\u0026rsquo;s intelligence. With that in mind, use AI wisely or you are at the risk of your intelligence unconsciously being replaced by artificial intelligence.\n"},{"ref":"https://bruhtus.github.io/posts/instanity/","title":"Instanity: Insanity in Instant Culture","section":"posts","tags":["Reflections"],"date":"2025.12.04","body":"At the time of writing this post, there\u0026rsquo;s some sort of AI hype going around, and some of the testimonies is that AI make people \u0026ldquo;learn faster\u0026rdquo; or \u0026ldquo;doing stuff faster\u0026rdquo;. With that in mind, let\u0026rsquo;s talk about how this mentality can be bad.\nFirst off, some people want to \u0026ldquo;learn faster\u0026rdquo;, and i\u0026rsquo;m like and then what? What would these people do after learning as fast as they can? Scrolling social media? If that is your goal, then i think you are putting the horse before the cart.\nPersonally, i try to have this \u0026ldquo;student mentality\u0026rdquo; where i treat learning as daily activity. I don\u0026rsquo;t want to rush my learning process because i don\u0026rsquo;t want to have a premature knowledge. In my unscientific observation, the biggest problem of self-learner is that some of them tend to be \u0026ldquo;overconfident\u0026rdquo;. Those people tend to blindly believe with the only reference they have, without even considering that the reference can be misleading. As a self-learner myself, this is the reason i don\u0026rsquo;t want my learning phase to end as fast as possible. I don\u0026rsquo;t want to be overconfident and think that i\u0026rsquo;ve learn everything that is to learn. There\u0026rsquo;s a possibility that what i\u0026rsquo;ve learn this far is not accurate, especially if i don\u0026rsquo;t have any experience with the topic.\nIf you just blindly trust what AI say without any experience in the subject, then i think it can be more harmful rather than helpful. AI can be good to find the unknown, finding something that we didn\u0026rsquo;t know before, and then find some proper references for those subject. We could say that AI make the process of finding the unknown faster than reading the documentation ourself, but are we really learning faster that way? How can we be sure that we\u0026rsquo;re done learning the subject? If you are trying to learn as fast as possible, you might set up yourself for premature knowledge.\nNow, let go into \u0026ldquo;doing stuff faster\u0026rdquo;, again, and then what? In the past, i\u0026rsquo;ve been working so fast that my team leader asking me to slow down because other teammates can not keep up with my pace, and that was before the current AI hype cycle. Whether you believe me or not, i\u0026rsquo;ve been in a position to do things as fast as possible and that do more harm to the team rather than being helpful.\nYou might be thinking, \u0026ldquo;then your teammates need to raise up their pace\u0026rdquo;, and then what? We are racing to see who can finish the most tasks and got burned along the way? Believe it or not, i\u0026rsquo;ve experience that too. For some reason, the people on the teams trying to compete to see who can finish the most task in the team, this happen in the current AI hype cycle. In those team, i\u0026rsquo;m the slow one because i\u0026rsquo;ve seen how exhausted my teammates in previous team when i set the pace too high, so i intentionally slowing down pace. But, that does not mean i become slow, i just putting off my result out there.\nHere\u0026rsquo;s an example, let\u0026rsquo;s say i need to make some feature, i already done that with \u0026ldquo;mechanism A\u0026rdquo;, but i did not submit \u0026ldquo;mechanism A\u0026rdquo; and instead i am trying to brainstorming again and change \u0026ldquo;mechanism A\u0026rdquo; to \u0026ldquo;mechanism B\u0026rdquo;, and then i have another idea and change \u0026ldquo;mechanism B\u0026rdquo; to \u0026ldquo;mechanism C\u0026rdquo;. After that, i don\u0026rsquo;t really have any inspiration to improve \u0026ldquo;mechanism C\u0026rdquo; so i submit \u0026ldquo;mechanism C\u0026rdquo;. That\u0026rsquo;s what i mean by slowing down, i just putting off the result.\nThat\u0026rsquo;s why i feel like \u0026ldquo;doing stuff faster\u0026rdquo; is not always a good thing, especially when we are working with a team. Setting the pace too high might not be sustainable in the long run. Unless that\u0026rsquo;s your goal, you don\u0026rsquo;t want to stay long enough in the industry.\nAnother example of \u0026ldquo;doing stuff faster\u0026rdquo; is instant messaging. When the first instant messaging application appear, a lot of people enjoying it, they be like \u0026ldquo;wow, we can contact other people instantly\u0026rdquo; and then years latter realizing that instant messaging setting the bar too high for how fast we should respond. Because of instant messaging, a lot of people expect us to respond immediately, it\u0026rsquo;s like our entire time is just waiting on instant messaging application. These expectation can create anxiety in some cases, and one of them is when working remotely. When we are working remotely, we can working from anywhere as long as we have internet access, but the dark side of working remotely is that, when we don\u0026rsquo;t separate working and resting phase, we will be working 24 hours a day. And if our company did not have a clear policy, we will be in sword of damocles situation, basically it\u0026rsquo;s like we have a sword hanging above our head all day long.\nThese mindset that we need to be \u0026ldquo;faster\u0026rdquo; or the expectation for something to be \u0026ldquo;instant\u0026rdquo; can be harmful for the society in some cases, so we need to keep that mindset in check.\n"},{"ref":"https://bruhtus.github.io/posts/competing-on-the-worst-experience/","title":"Competing on the Worst Experience","section":"posts","tags":["Reflections"],"date":"2025.12.03","body":"Recently i watched Dr. K\u0026rsquo;s video on youtube . He talks about \u0026ldquo;why \u0026lsquo;the grind\u0026rsquo; isn\u0026rsquo;t meant for everyone\u0026rdquo; and one of things he said on those video is this:\n We live in a society that glorifies exhaustion at work.\n And that make me thinking, that\u0026rsquo;s not the end of it. If you ever talk to a friend and said \u0026ldquo;i have a lot of tasks at work\u0026rdquo; and then explain those all those tasks, your friend be like \u0026ldquo;you got off better, i need to do more work than that\u0026rdquo; like they are proud of it. When my friend did that, i am like \u0026ldquo;why does this sounds like we are competing to have the worst experience at work?\u0026rdquo; which honestly, kind of funny to me. Am i weird thinking like that? I hope not.\nWhen Dr. K said that the society glorifies exhaustion at work and proud of it, i want to add that some people also competing for it, to see which one can have the worst experience at work. And guess what? That\u0026rsquo;s not gonna make the work environment better because no one complains, and when someone complains, they will be the weirdo in those work environment.\nAnd this is not limited to work, this can also apply to another experience outside of work too. That\u0026rsquo;s why i\u0026rsquo;ve been thinking, is this a human nature? Or maybe the society standard just getting worst?\nPersonally, i don\u0026rsquo;t really encourage anyone to compete on who has the worst experience because, even if your experience is not the worst, that does not mean it is not bad. It\u0026rsquo;s still bad, it\u0026rsquo;s just not the worst thing that could happen. And by competing who has the worst experience, we are basically tolerating bad experience, and that behavior can lower the society standard even further if a lot of people doing that.\n"},{"ref":"https://bruhtus.github.io/posts/human-inconsistency-and-artificial-intelligence/","title":"Human's Inconsistency and Artificial Intelligence","section":"posts","tags":["Reflections"],"date":"2025.11.30","body":"At the time of writing blog post, human still trying to emulate human\u0026rsquo;s intelligence on machine. This concept usually called Artificial Intelligence. This attempt has gone for quite sometime, there\u0026rsquo;s even the hype cycle of it. When the artificial intelligence (a.k.a as AI) is not meet the current hype expectation, we will enter AI winter period.\nFrom my observation, if we want to make a machine behave like a human, we also need to know about human\u0026rsquo;s weakness, and one of them is inconsistency. In case anyone have not notice that yet, human can be inconsistent and that can be a good thing or a bad thing, depending on the circumstances. For example, if human doing something consistent all the time, when those human doing bad things, they will consistently doing bad things for the rest of their life. But that\u0026rsquo;s not always the case, sometimes bad human can be good human too or vice versa.\nI think this is something that can be forgotten when developing artificial intelligence. Now the question is that, do we want human\u0026rsquo;s inconsistency on the machine or not? Personally i prefer the machine to be consistent or at least predictable, but i am not sure what will happen. So let\u0026rsquo;s see how this artificial intelligence development will unfold.\n"},{"ref":"https://bruhtus.github.io/posts/programmer-is-designer/","title":"Programmer is Designer","section":"posts","tags":["Reflections"],"date":"2025.11.28","body":"Recently i finished reading Peter Naur\u0026rsquo;s essay called Programming as Theory Building and i think that essay describe what i like about programming, that is \u0026ldquo;building a theory\u0026rdquo; or i usually called it \u0026ldquo;designing a system\u0026rdquo;.\nI like to think of myself as \u0026ldquo;designer\u0026rdquo; when i\u0026rsquo;m programming, because in a way, i\u0026rsquo;m basically designing some sort of mechanism to meet some specific needs. The process of finding the right mechanism is what i like about programming. And like any designer out there, if we don\u0026rsquo;t have any information about the current design we have, it\u0026rsquo;s gonna be a lot harder to modify the current design.\nFrom Naur\u0026rsquo;s ideas, we get the insight that the designer\u0026rsquo;s job is not to pass along only \u0026ldquo;the design\u0026rdquo;, but to pass along \u0026ldquo;the design\u0026rdquo; and \u0026ldquo;the theories\u0026rdquo; behind the design. That\u0026rsquo;s why when someone told me to look at the code without any explanation about the context, i got confused, what should i do with this code? The code only telling me how the system works but not telling me what is the goal of the system. Some \u0026ldquo;senior programmers\u0026rdquo; even mocking me, \u0026ldquo;can\u0026rsquo;t you just understand from looking at the code?\u0026rdquo;, and i\u0026rsquo;m like, the most i can do is create a hypothetical theory about the goal of the system which might or might not be correct. I think this is what a lot programmers forgot, without context, designing a solution for specific needs become a lot harder. Even with the proper context, designing some mechanism for the system might still be challenging, let alone without any context.\nHere\u0026rsquo;s a little bit story about the \u0026ldquo;loss context\u0026rdquo;, let\u0026rsquo;s say the management moved me into another on going project, and it turns out that all members of the team is new in those project. Every programmers touching the code base for the first time, even the product manager (a.k.a PM) also new in those project, and to make things more interesting, there\u0026rsquo;s no documentation at all.\nAnd then one day, there\u0026rsquo;s some changes in the ranking system. We are changing the scoring mechanism and i\u0026rsquo;m curious about the sorting order, so i ask the PM, \u0026ldquo;Do we sort the ranking ascending or descending? Currently we are using ascending order for the ranking\u0026rdquo;. And then the PM said \u0026ldquo;Just use the same order from current implementation\u0026rdquo;. And i\u0026rsquo;m like, alright, let get working.\nAfter we finished those changes, the PM check the new implementation and surprised that the new implementation is not as the PM expected. So me and PM have a meeting, and then i try to create a simplified ranking mechanism in spreadsheet, with the data from staging environment and use that data in the score equation theory that i understand. And then we confirm that the result from equation on spreadsheet is the same as the result on the new ranking system, and the PM confirmed that the score is correct. Alright, at least we know it\u0026rsquo;s not the equation problem. After that, i tried to change the sorting order. At first, i use the ascending order on the spreadsheet, and i said \u0026ldquo;This is the current implementation, is this correct?\u0026rdquo;. After a little bit of observation by the PM, the PM said \u0026ldquo;It\u0026rsquo;s not correct\u0026rdquo;. After that, i use descending order, and i ask \u0026ldquo;Is this correct?\u0026rdquo;. After some observation from the PM, the PM said \u0026ldquo;Yeah, the ranking should be like that\u0026rdquo;. Okay, we found the problem and we changed the ordering from ascending to descending.\nNow, the example above makes me wonder, is the previous ranking system implementation incorrect to begin with or there\u0026rsquo;s new requirement about the order of the ranking system that the PM forgot to tell the team? At this point, we have no idea. If that is a bug, i guess we are lucky enough that no user notice that and we get to fix that because of new requirements.\nThat\u0026rsquo;s why i think the theory or context about the current system is important, we can create a documentation or explain the theory or context in the comment. The \u0026ldquo;clean something\u0026rdquo; evangelist will be mad at me, they be like \u0026ldquo;write an easy to understand code so you don\u0026rsquo;t have to write the comment\u0026rdquo;. And i\u0026rsquo;m like, if you\u0026rsquo;re gonna write a comment about how the mechanism works, then i agree with that take, just write easy to understand code so you don\u0026rsquo;t have to write those type of comment. But i think comment can be useful to write why we take that approach. So mindlessly following some advice to not writing comment in a code base can be bad.\nAnd this is another part of the essay that i like. What i understand from the essay is that, there\u0026rsquo;s no right method in programming, there\u0026rsquo;s only a collection of suggestions aiming at stimulating the mental activity of the programmer, by pointing out different modes of work that may be applied in any sequence.\nBy the time of writing this post, i\u0026rsquo;ve seen a lot of programmers trying so hard to find \u0026ldquo;one great method\u0026rdquo; for everything. For those kind of programmers, this quote from the internet represent my thought:\n \u0026ldquo;One size fit all\u0026rdquo; fit nothing.\n Programming is a design process, and the design process might be different depending on the situation. So as a programmer, we need to decide which techniques to use in the current situation. That\u0026rsquo;s why i agree with the essay, all of those \u0026ldquo;best practices\u0026rdquo; are good as \u0026ldquo;a collection of suggestions\u0026rdquo;, which means that we don\u0026rsquo;t need to follow that as is, we can adjust those \u0026ldquo;best practices\u0026rdquo; to adapt for current situation or even create a new \u0026ldquo;best practice\u0026rdquo; for current situation.\nIf we think of ourselves as programmer, i think it\u0026rsquo;s a good idea to have a designer mindset, which means that we are not only writing code but we are designing something with purpose and the purpose can be anything, it can be just for fun or something meaningful and if possible, pass along those purpose too so that anyone else can understand our design choices.\n"},{"ref":"https://bruhtus.github.io/posts/witch-allegation-in-ai-dystopia/","title":"Witch Allegation in AI Dystopia","section":"posts","tags":["Reflections"],"date":"2025.11.26","body":"Recently i found someone\u0026rsquo;s status about \u0026ldquo;getting accused of using AI when we didn\u0026rsquo;t\u0026rdquo;, so let\u0026rsquo;s talk about it.\nHere\u0026rsquo;s the status, in case anyone curious:\nBefore we start, i just want to say that the term AI in here means \u0026ldquo;Artificial Intelligence\u0026rdquo;, not \u0026ldquo;Another Idiot\u0026rdquo; or \u0026ldquo;Actual Indian\u0026rdquo;. With that in mind, let\u0026rsquo;s get started.\nAt the time of writing this post, AI become trending again. If anyone still remember, this is not the first time AI trending. There\u0026rsquo;s even the term AI winter and maybe we can create a machine learning model to predict the next \u0026ldquo;AI hype cycle\u0026rdquo;? Who knows.\nAnyway, because AI trending, a lot of people using AI to generate a bunch of stuff but the thing is that, some of those people who use AI did not admit that they use AI. They want other people to think that it is their own creation without putting a lot of effort. Because of these people, i have seen some kind of \u0026ldquo;witch allegation\u0026rdquo; going on, like \u0026ldquo;there\u0026rsquo;s no way you did this, you are using AI, right?!!\u0026rdquo; and if you notice, that is kind of similar to \u0026ldquo;you are a witch, aren\u0026rsquo;t you?!!\u0026rdquo;.\nIf people keep being dishonest about their usage of AI, this \u0026ldquo;witch allegation\u0026rdquo; might keep going until we enter \u0026ldquo;AI winter\u0026rdquo; again, or worse, this current \u0026ldquo;AI hype cycle\u0026rdquo; will leave \u0026ldquo;scar\u0026rdquo; in society, that is trust issue about any form of creation.\nP.S:\nIf you think that i wrote this post using AI, congratulation, you have just prove my point. Just to be clear, i did not use AI to write this post or any of my blog post, unless i explicitly said otherwise. I realize that my writing skill is still bad, so that is why i keep writing blog post, as a writing exercise. I want to learn how to write down my thoughts and that\u0026rsquo;s why this blog post exists.\n"},{"ref":"https://bruhtus.github.io/posts/to-be-open/","title":"To Be Open","section":"posts","tags":["Reflections"],"date":"2025.11.08","body":"Recently i watch a talk by Lu Wilson on youtube about What it means to be open and that talk kind of resonates with me because that\u0026rsquo;s exactly what i am trying to achieve, which is \u0026ldquo;to be open\u0026rdquo;.\nIt\u0026rsquo;s not only about the source code, it\u0026rsquo;s about being human. Nowadays i feel like there is some kind of pressure \u0026ldquo;to be perfect\u0026rdquo;, \u0026ldquo;to have no mistake\u0026rdquo;, \u0026ldquo;to never failed\u0026rdquo;, and so on. But that\u0026rsquo;s not what human is, human is imperfect being and how can some imperfect being define \u0026ldquo;perfection\u0026rdquo;?\nThose imperfection is not a bad thing, because we can explore the possibility to make something better. If we are perfect from the get go, what should we do next? Doing nothing and rot? If you say that we can do some \u0026ldquo;improvement\u0026rdquo;, that\u0026rsquo;s means we are not perfect, in case someone have not realized that yet. The perfect being won\u0026rsquo;t need improvement, what can we improve from something perfect? Nothing.\nFor me, to be open means to embrace those imperfection and to share the progress of improvement. To be open means to acknowledge there are things that we don\u0026rsquo;t know yet. And what i want to achieve by being open is to find some sort of fellowship with other like-minded people, to not be shy sharing what we are trying to do and help each other out.\n"},{"ref":"https://bruhtus.github.io/posts/pointer-in-c-warehouse-analogy/","title":"Pointer in C: Warehouse Analogy","section":"posts","tags":["C"],"date":"2025.11.05","body":"At the time of writing this post, i am still learning about C programming language, and one of the topics about C programming language is \u0026ldquo;pointer\u0026rdquo;. I know that there\u0026rsquo;s a lot of analogy about pointers out there, one of the common one is about house and house address, but i want to share the analogy that help me understand about pointer.\nImagine there\u0026rsquo;s a company called RAM and those company business is renting out warehouses. And then we, as the customer, want to rent one of their warehouses.\nWhen we rent the warehouse, we got the following item:\n The warehouse key. The keychain with warehouse information on the key.  Before we go further, think of the \u0026ldquo;warehouse\u0026rdquo; as data storage (a.k.a memory), the \u0026ldquo;warehouse key\u0026rdquo; as the pointer in C, and the \u0026ldquo;keychain\u0026rdquo; as the address that we save in those pointer. We can think of the \u0026ldquo;keychain\u0026rdquo; as the value of the \u0026ldquo;key\u0026rdquo;.\nIf we want to get \u0026ldquo;something\u0026rdquo; inside the warehouse, we need to use the \u0026ldquo;key\u0026rdquo; to open the warehouse. We can think of \u0026ldquo;opening\u0026rdquo; the warehouse using the \u0026ldquo;key\u0026rdquo; as dereferencing a pointer, and the effect of \u0026ldquo;opening\u0026rdquo; the warehouse is that we got \u0026ldquo;something\u0026rdquo; from inside the warehouse.\nNow, let\u0026rsquo;s say that we have a \u0026ldquo;key\u0026rdquo; but we don\u0026rsquo;t have the \u0026ldquo;keychain\u0026rdquo;, how do we know which warehouse we are supposed to open? This situation will cause confusion for us, and it\u0026rsquo;s also similar for the computer. This situation can be illustrated with this C code:\nint main(void) { int *p; *p = 69; return 0; } The above code means that we are trying to put 69 into the warehouse, but we did not give which warehouse we are supposed to use. If we try running those code (after we compiled it), we will get segmentation fault error, which basically means that the computer can not access the memory because there\u0026rsquo;s no information about the memory that the computer supposed to use.\nTo solve those problem, we can give information about which \u0026ldquo;warehouse\u0026rdquo; to use like this:\nint main(void) { int i; int *p; p = \u0026amp;i; *p = 69; return 0; } The above code means that we put the address of warehouse i in our \u0026ldquo;keychain\u0026rdquo; so that we know which warehouse to use, and we can put 69 in our warehouse. This is also the case with computer, now the computer is not confused anymore and won\u0026rsquo;t throw segmentation fault error.\nThere\u0026rsquo;s also a case with dangling pointer, which basically means that the \u0026ldquo;key\u0026rdquo; and the \u0026ldquo;keychain\u0026rdquo; is exist but the warehouse is already demolished. So when we are trying to \u0026ldquo;open\u0026rdquo; the warehouse, we can\u0026rsquo;t find the specified warehouse from the information on the \u0026ldquo;keychain\u0026rdquo; and here comes another confusion. I will leave it up to you to try out dangling pointer and other pointer case.\nAlright, that\u0026rsquo;s it. See you next time!\n"},{"ref":"https://bruhtus.github.io/posts/difference-between-i++-and-++i-in-c/","title":"Difference Between i++ and ++i in C","section":"posts","tags":["C"],"date":"2025.11.04","body":"A Brief Explanation Let\u0026rsquo;s say we have int i = 0;, and imagine there\u0026rsquo;s a temporary object (rvalue?) to store the result of i + 1 for the ++ operator.\n Object is an area of memory that is used by our program, and temporary object in here means that the object has a temporary duration and will be deleted when the containing full expression ends.\nFull expression here means:\n The complete expression that forms an expression statement (with terminating semicolon (;) at the end). One of the controlling expression if, switch, while, for, or do-while statement. The expression of an initializer (like int i = 0;). return statement.   This is what probably happen if we use i++ without assigned the result to another variable:\n// i++; tmp = i; i = i + 1; This is what probably happen if we use ++i without assigned the result to another variable:\n// ++i; i = i + 1; tmp = i; This is what probably happen if we use i++ when assigned the result to variable j:\n// int j = i++; tmp = i; j = tmp; i = i + 1; This is what probably happen if we use ++i when assigned the result to variable j:\n// int j = ++i; i = i + 1; tmp = i; j = tmp; The only difference between i++ and ++i is when we use the value of the operation in the same statement, like int j = ++i.\nP.S:\nAt the time of writing this post, i am still learning about C, so this illustration might be wrong. Please let me know if there are a better illustration about this.\ni++ or ++i in For Loop When using either i++ or ++i in for-loop like this:\nint i; for (i = 0; i \u0026lt; 5; i++) printf(\u0026#34;%d\\n\u0026#34;, i); for (i = 0; i \u0026lt; 5; i++) printf(\u0026#34;%d\\n\u0026#34;, i); Both of them will operate identically because the increment of i and the print statement is in different line. It\u0026rsquo;s like we are using i++ or ++i without any assignment like this:\nint i = 0; i++; ++i; printf(\u0026#34;%d\\n\u0026#34;, i); Side Note Please keep in mind that operation i++ and ++i is also prone to integer overflow.\nHere an overflow example:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;limits.h\u0026gt; int main(void) { unsigned int i = UINT_MAX; printf(\u0026#34;before increment i: %d\\n\u0026#34;, i); i++; printf(\u0026#34;after increment i: %d\\n\u0026#34;, i); return 0; } In the example above, as we store the maximum value of unsigned integer in i and when we increase the value of i, the result would be 0 because the value is more than the maximum value of unsigned integer, so it result in overflow.\nReferences  Stackoverflow about difference between i++ and ++i . About temporary object . About integer overflow . Unsigned integers overflow . Cppreference object lifetime . Cppreference statements .  "},{"ref":"https://bruhtus.github.io/posts/become-the-unofficial-leader/","title":"Become the Unofficial Leader","section":"posts","tags":["Reflections"],"date":"2025.10.28","body":"Recently i read about Simon Sinek\u0026rsquo;s post about would you still lead if no one ever knew your name? and my answer to those question is a solid \u0026ldquo;yes\u0026rdquo;. Here\u0026rsquo;s the story, back then we have two senior-level teammates on our team, one of them is in the official leader of the project and another one is not. Because i am new at my role, i am still struggling at the beginning of the project, and surprisingly the non-lead senior-level teammate reach out to me and saying \u0026ldquo;Hey, do you have some difficulty with your task? If so, you can ask me\u0026rdquo;. He didn\u0026rsquo;t have to, but he still reach out to me and as someone who is trying to understand how to do the task, that means a lot to me.\nSince then, i will always try to help if i can, even if i am not the \u0026ldquo;official\u0026rdquo; leader of the project. That\u0026rsquo;s because i want to create a team that will cover each other\u0026rsquo;s back. You might be surprised that by me helping other teammates in need, overtime, my teammates also helping me too. Like Simon\u0026rsquo;s said in his post, creating the \u0026ldquo;Circle of Safety\u0026rdquo;.\nThat\u0026rsquo;s why when i read Simon\u0026rsquo;s post, it resonates with me. Leadership is not about the position, it\u0026rsquo;s the act of service. These consistent act of service will create trust between each others, and those trust will create a \u0026ldquo;safety\u0026rdquo; environment where we can thrive together.\nSo, if you are not sure how to lead other people or how to become a leader, you might want to start helping other people, even if it\u0026rsquo;s not your responsibility.\nP.S: Here\u0026rsquo;s another quote from Simon Sinek about leadership:\n The joy of leadership comes from seeing others achieve more than they thought they were capable of.\n "},{"ref":"https://bruhtus.github.io/posts/psql-copy-functionality/","title":"Psql Copy Functionality","section":"posts","tags":["Postgresql"],"date":"2025.07.16","body":"A Brief Intro I need to replace data on one database server with data on another database server. Let\u0026rsquo;s call it server 1 for the source server and server 2 for the target server. But there are 2 limitations:\n We only need to replace data on specific table. We only able to access the database through psql with VPN (at least that\u0026rsquo;s how i understand it).  With that in mind, i\u0026rsquo;m trying to solve those problem with psql \\copy. Why did i use \\copy instead of sql copy? From postgres documentation:\n file accessibility and access rights depends on the client rather than the server when \\copy is used.\n If i want to retrieve the file on my local machine, using \\copy is more convenient as i use psql on my local machine.\n\\copy to Assuming we already inside psql, to copy the data from database table to csv file, we can do some like this:\n\\copy (SELECT * FROM meat) to '/home/user/meat.csv' with csv header Here\u0026rsquo;s the breakdown of the previous command means:\n SELECT * FROM meat is the sql query to get the data we want. to '/home/user/meat.csv' is the target where we want the data go into. In this case, the data will be saved in /home/user/meat.csv. with csv header is to include the column in the specified table from the select query as a csv header. This can be useful in case we want to debug the query result from csv file.  \\copy from Assuming we already inside psql, we already got the data source we want, and we already delete all the existing data (keep in mind that in this scenario, we need to replace the data). After the preparation is done, we can use this command:\n\\copy meat from '/home/user/meat.csv' delimiter ',' csv header Here\u0026rsquo;s the breakdown of the previous command means:\n meat is the target table we need to fill in. from '/home/user/meat.csv' is the target path that have the data we want. delimiter ',' is to indicate a delimiter to split the data. csv header is to indicate that the first row contains the header.  Conclusions Keep in mind that this is only one of the solution to solve the problem of replacing data from one database server to another database server and this might not be the best solution. So rather than focusing on the scenario, just treat it as something to help us understand the mechanism of psql \\copy. The point of this post is to let you know about the existence of psql \\copy and you can tweak it according to your need.\nAll right, that\u0026rsquo;s it. See you next time!\nReferences  Stackexchange psql copy answer . Postgresql copy sql documentation .  "},{"ref":"https://bruhtus.github.io/posts/postgres-update-multiple-rows-with-unnest-function/","title":"Postgresql Update Multiple Rows With Unnest Function","section":"posts","tags":["Postgresql"],"date":"2025.07.15","body":"A Brief Intro At work, i need to build an API that enable user to update specific info in multiple rows. Previously, i made a blog post about postgres update multiple data in one query , but those method is more convenient for running raw sql in the postgresql database directly rather than for API. I mean, sure, we can use FROM (values (...), (...), ...) but personally, i don\u0026rsquo;t really want to compose multiple values with positional parameters in postgres. And since we use pgx in golang as our postgres driver, i thought \u0026ldquo;can we put all the info we need in an array when trying to update multiple values?\u0026rdquo;, and looks like we can!\nUnnest Function I learn about the existence of unnest() from this website:\nhttps://www.w3resource.com/PostgreSQL/postgresql_unnest-function.php So, what i do is that i put all the data in a golang array. Let\u0026rsquo;s say we have pgx named arguments @ids to hold all ids that we want to update, which in this case let\u0026rsquo;s assume we use UUID as our id, and then @price to hold the data that we want to update. And the table we want to update is called meat.\nAfter validation and all those stuff, we get the necessary data and this is the query for updating multiple data with unnest() for pgx golang:\nUPDATEmeatasmSETprice=c.priceFROMunnest(@ids::uuid[],@price::int[])asc(id,price)WHEREm.id=c.idRETURNINGm.*The equivalent raw sql would be like this:\nUPDATEmeatasmSETprice=c.priceFROMunnest(array[\u0026#39;...\u0026#39;,\u0026#39;...\u0026#39;]::uuid[],array[69,42]::int[])asc(id,price)WHEREm.id=c.idRETURNINGm.*Please keep in mind that we need to specify the array type in postgres like uuid[] and int[] because apparently pgx using array in string format like unnest('{1, 2, 3}') so postgres get confused and throw an error unnest(unknown) ... unique (?). This is still an assumption and might be incorrect but the query is working fine after casting the type.\nAlso, i\u0026rsquo;m still not sure about the performance of using postgres unnest() so it\u0026rsquo;s something to keep in mind.\nAll right, that\u0026rsquo;s it. See you next time!\nReferences  w3resource about postgresql unnest() function  Stackoverflow answer about unnest(unknown) error   "},{"ref":"https://bruhtus.github.io/posts/flashing-skeletyl-keyboard-with-miryoku-layout/","title":"Flashing Skeletyl Keyboard With Miryoku Layout","section":"posts","tags":["Keyboard"],"date":"2025.07.13","body":"Setup Miryoku in QMK Disclaimer:\nBy the time of writing this post, there\u0026rsquo;s a breaking change in QMK that remove user keymaps, including miryoku. And miryoku QMK haven\u0026rsquo;t porting it\u0026rsquo;s config in the new QMK userspace. So this section might become obsolete in the future. For reference: https://github.com/manna-harbour/miryoku/discussions/287 First thing we need to do is to setup QMK. We can check the QMK documentation below:\nhttps://github.com/qmk/qmk_firmware/blob/master/docs/newbs_getting_started.md To check if QMK already installed on linux, we can use this command:\ncommand -v qmk if there\u0026rsquo;s any output, it means QMK is already installed. If there\u0026rsquo;s no output, try again.\nAfter QMK is installed, we need to setup QMK with this command:\nqmk setup By default, the installation directory will be on our home directory. If we don\u0026rsquo;t want to put the QMK installation on my home directory, we can add -H flag to move the installation directory to somewhere else, like this:\nqmk setup -H all-repos/qmk_firmware If the setup already finished, try running:\nqmk doctor and see if there\u0026rsquo;s any issue with the installation.\n If you see a warning about 50-qmk.rules, we might to copy 50-qmk.rules to /etc/udev/rules.d/. Like this: cp path/to/50-qmk.rules /etc/udev/rules.d\n After that, we need to change our working directory qmk_firmware. In qmk_firmware directory, we need to add miryoku_qmk fork like this:\ngit remote add miryoku_qmk https://github.com/manna-harbour/miryoku_qmk.git git fetch miryoku_qmk git checkout --track miryoku_qmk/miryoku And then we need to merge miryoku with QMK master branch like this:\ngit checkout -b miryoku-merge-master git revert --no-edit `git log --grep=\u0026#39;^\\[miryoku-github\\]\u0026#39; --pretty=\u0026#39;format:%H\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;` git fetch origin git merge origin/master and solve all conflict.\nAfter that, we need to copy layouts/community/split_3x5_3/manna-harbour_miryoku/ from miryoku branch into keyboards/bastardkb/skeletyl/keymaps/ in miryoku-merge-master branch. We also need to copy users/manna-harbour_miryoku/ from miryoku branch to users/ in miryoku-merge-master branch.\nAfter that, we need to run this command:\nmake git-submodule Next, we need to make sure that we can compile the miryoku layout by running this command:\nqmk compile -c -kb bastardkb/skeletyl/v2/splinky_3 -km manna-harbour_miryoku -e MIRYOKU_ALPHAS=QWERTY -e MIRYOKU_LAYERS=FLIP -e MIRYOKU_NAV=INVERTEDT  I prefer to use qwerty layout, so i add flag -e MIRYOKU_ALPHAS=QWERTY. Also my unit using rp2040, that\u0026rsquo;s why i\u0026rsquo;m using splinky_3. I\u0026rsquo;m not sure the difference between splinky_2 and splinky_3, and the reason i\u0026rsquo;m using splinky_3 instead of splinky_2 is because i think splinky_3 is the latest version of splinky variant. I prefer to use space on the right side, hence why i put flag -e MIRYOKU_LAYERS=FLIP to flip the base layer thumb keys and sub layers between right and left hands. I also prefer using arrow key in inverted T model, like asdw in fps game, hence why i put flag -e MIRYOKU_NAV=INVERTEDT.\n If there\u0026rsquo;s no error, we can go into the next section. If there\u0026rsquo;s some error, we might need to adjust a few things. Check this discussion and see if this changes fix the error:\n https://github.com/manna-harbour/miryoku/discussions/361  https://github.com/manna-harbour/miryoku/discussions/361#discussioncomment-12243200   Flashing Skeletyl Keyboard The first thing we need to do is make our keyboard enter bootloader mode. In skeletyl keyboard, we can use the update button at the bottom of the keyboard. For more info, we can check this guide:\nhttps://docs.bastardkb.com/fw/flashing.html  Because we using new firmware, we can flash both side with the same firmware.\n After our keyboard enter bootloader mode, we need to mount our micro controller to our linux system. We can use lsblk to find out our micro controller partition name and mount those partition. For example, if our micro controller partition name is sda1, we can use this command to mount our micro controller partition:\nsudo mount /dev/sda1 /mnt After that, we can run this command to flash our micro controller with firmware that has miryoku layout in it:\nqmk flash -c -kb bastardkb/skeletyl/v2/splinky_3 -km manna-harbour_miryoku -e MIRYOKU_ALPHAS=QWERTY -e MIRYOKU_LAYERS=FLIP -e MIRYOKU_NAV=INVERTEDT If there\u0026rsquo;s no error, we can use our skeletyl keyboard with miryoku layout after the flashing process finish. If there\u0026rsquo;s an error, you might want to take a look at miryoku discussion or issue on github.\nIf the skeletyl keyboard is not usable after booting up the computer, we can add #define USB_VBUS_PIN GP19 in file keyboards/bastardkb/skeletyl/v2/splinky_3/config.h. Defining USB_VBUS_PIN will allow us to tell the controller whether it\u0026rsquo;s master or slave right away, resulting in immediate boot for both sides. After defining USB_VBUS_PIN, we can flashing both side of the keyboard again using the command above.\nAll right, that\u0026rsquo;s it. See you next time!\nSide Note Currently the right side of miryoku qwerty layout using hjkl' instead of hjkl;, so you might be surprised the first time using miryoku qwerty layout. There\u0026rsquo;s a plan to customize that, but as of writing this post, it\u0026rsquo;s still work in progress. You can check this discussion to see the progress:\nhttps://github.com/manna-harbour/miryoku/discussions/85 References  Miryoku reference manual  Miryoku build guide  Miryoku breaking change QMK discussion  Skeletyl flashing firmware guide  Github issue QMK rp2040 not usable from boot  Skeletyl USB VBUS PIN for splinky controllers   "},{"ref":"https://bruhtus.github.io/posts/golang-simple-query-composer-with-text-template-and-pgx/","title":"Golang Simple Query Composer With Text Template and Pgx","section":"posts","tags":["Golang"],"date":"2025.06.22","body":"For backend development at work, we just transitioned from Node.js to golang recently. We use pgx as our PostgreSQL driver. I\u0026rsquo;m not quite sure why we choose pgx as our PostgreSQL driver and not using ORM like gorm, so we won\u0026rsquo;t talk about that here. My principal is to adapt to the tech stack that the project use rather than forcing what i\u0026rsquo;m familiar with, so i don\u0026rsquo;t really mind.\nJust to be clear, as my workplace tech stack move to golang, that is also the first time i learn golang. So i have no prior experience with golang. In fact, by the time i write this post, i have less than one year experience with golang. You could say that i\u0026rsquo;m an inexperience golang developer that are trying things out and see if it works or not.\nWith that in mind, let\u0026rsquo;s moving into the topic. The problem i\u0026rsquo;m trying to solve here is the mess when we\u0026rsquo;re trying to compose an SQL query for pgx. Currently at work, when we compose the SQL query is that we use += to append new string into existing SQL query string.\nHere\u0026rsquo;s an example, let\u0026rsquo;s say we have a table articles with this columns:\nid author title content created_at updated_at And then we want to see how many articles each author has. With raw SQL query, we can do something like this:\nselectarticles.author,count(*)astotalfromarticlesgroupbyarticles.author;If that\u0026rsquo;s all we need, then we can just write those SQL query as is in one string. Now the problem is, when we want to show only specific author. The raw SQL query would be something like this:\nselectarticles.author,count(*)astotalfromarticleswherearticles.authorilike\u0026#39;%robertus%\u0026#39;groupbyarticles.author;Now we have two state, the one with all the authors and the one with specific author (searching feature). Currently at work, we compose those SQL query like this:\nvar args []interface{} query := ` select articles.author, count(*) as total from articles ` // Assume `Author` type is `string`. if filter.Author != \u0026#34;\u0026#34; { query += `where articles.author ilike $1` args = append(args, filter.Author) } query += fmt.Sprintf(`\\n%s`, \u0026#34;group by articles.author\u0026#34;) // Assume `db` type is `*pgxpool.Pool`. rows, err := db.Query(ctx, query, args...) defer rows.Close() ... I see at least 2 problems with the current query composer:\n What if we want to add another where filter to the query? What if we want to add where filter in a sub-query?  Now, what i proposed is that we use golang text/template package. I just recently found out about text/template package and i think we can use that as a simple SQL query composer.\nThe concept of using text/template package as SQL query composer is to use a placeholder inside the base query string.\nWith the previous scenario, we can use text/template package like this:\n// Use named arguments instead of positional arguments. var ( composedFilter []string queryBuilder = new(strings.Builder) // Using `strings` package.  conditionalTmpl = make(map[string]string) filterArgs = make(pgx.StrictNamedArgs) ) // The `-` at `{{end -}}` is to remove whitespace after the line. baseQuery := ` select articles.author, count(*) as total from articles {{if.Where}}{{.Where}}{{end-}}group by articles.author ` // Assume `Author` type is `string`. if filter.Author != \u0026#34;\u0026#34; { composedFilter = append(composedFilter, `articles.author ilike @author`) // Key `author` is the named argument `@author`, so make sure the name is  // the same.  filterArgs[\u0026#34;author\u0026#34;] = fmt.Sprintf(\u0026#34;%%%s%%\u0026#34;, filter.Author) } if len(composedFilter) \u0026gt; 0 { // This is a placeholder in `{{if .Where}}...` so make sure the name  // is the same.  conditionalTmpl[\u0026#34;Where\u0026#34;] = fmt.Sprintf( \u0026#34;where %s\\n\u0026#34;, strings.Join(composedFilter, ` AND `), ) } queryTmpl, err := template.New(\u0026#34;baseQuery\u0026#34;).Parse(baseQuery) if err != nil { ... } if err := queryTmpl.Execute(queryBuilder, conditionalTmpl); err != nil { ... } query := queryBuilder.String() // Assume `db` type is `*pgxpool.Pool`. rows, err := db.Query(ctx, query, filterArgs) defer rows.Close() ... With the approach above, we can add some placeholder for SQL query where clause, or order by clause, or other things that is optional to the feature.\nMy concern with the text/template approach is that i don\u0026rsquo;t know the failure condition for template.New().Parse(), so that might become a consideration when deciding to use this approach or not. It\u0026rsquo;s similar to when google/uuid package panic when using uuid.New(), you can check it out on this issue about the context .\nI already proposed this mechanism at work but i\u0026rsquo;m not sure if this mechanism will be used. So, i\u0026rsquo;m posting this in case someone need an inspiration for simple SQL query composer.\nAlright, that\u0026rsquo;s it. See you next time!\nReferences  Stackoverflow format go string .  "},{"ref":"https://bruhtus.github.io/posts/configure-vim-as-intro-to-software-design/","title":"Configure Vim as Intro to Software Design","section":"posts","tags":["Reflections"],"date":"2025.05.31","body":"As vim gaining popularity these days, I\u0026rsquo;ve heard some complaint about configuring vim is hard. Not only learning vim motions is hard, making vim usable for the user is also hard. Well, in case someone haven\u0026rsquo;t realize, configuring vim is basically a programming activity which can be a good thing or a bad thing, depending on your preferences.\nBecause configuring vim is hard for new user, some people created a pre-configured vim config that they can use. At the beginning, pre-configured vim config can help new user to focus more on learning or getting used to vim motions instead of going the rabbit hole of configuring vim. Pre-configured vim config can lower the overhead for new user, but it\u0026rsquo;s not the final destination. There are some benefits we can get by configuring vim from scratch, as configuring vim is basically doing some programming. To use the programming term, you can think of vim as a \u0026ldquo;runtime\u0026rdquo; and your configuration as a script to run in those \u0026ldquo;runtime\u0026rdquo;.\nSome people like the concept of configuration like vim, which some might say \u0026ldquo;configuring by programming\u0026rdquo;, and some people don\u0026rsquo;t like it. If you\u0026rsquo;re interested in using vim, I suggest you to take a look of what vim has to offer, like \u0026ldquo;configuring by programming\u0026rdquo; concept. And if that\u0026rsquo;s not your cup of tea, then maybe vim is not for you and maybe you can learn only the vim motions instead. I treat vim, the text editor, and vim motions separately because I think both of them have their own benefits independently.\nSo, what kind of benefits we can get when we try to configure vim? One of the benefits of configuring vim from scratch is that we learn about \u0026ldquo;execution order\u0026rdquo;. You might be thinking, \u0026ldquo;isn\u0026rsquo;t that the basic of programming?\u0026rdquo;. Well, you might be surprised that some programmer or software developer didn\u0026rsquo;t even care or, worse, didn\u0026rsquo;t even know about the execution order. One example from my workplace is that some developer using forEach() in javascript to do some looping, regardless of asynchronous or synchronous operation. And then one of the developers use forEach() to looping through a list of asynchronous operation. And for some reason, those developer is moved to another project. And then I got assigned to those project, and let me tell you, those looping is causing a race condition in the software because forEach() didn\u0026rsquo;t handle asynchronous operation, and I\u0026rsquo;m the one who need to fix that. Because of configuring vim, I make a habit of being conscious about the execution order, which operation get executed first. And so, that\u0026rsquo;s the first thing I checked, what order the operation get executed? And that\u0026rsquo;s where I found those \u0026ldquo;illegal racing\u0026rdquo; in the source code.\nYou might be thinking, \u0026ldquo;isn\u0026rsquo;t that kind of thinking process will come with experience?\u0026rdquo;. Yes, you\u0026rsquo;re right, and configuring vim help me to have those kind of thinking process earlier than my peer, because vim \u0026ldquo;forced\u0026rdquo; me to be mindful of the execution order of my config. It\u0026rsquo;s like what my senior software developer always said, we need to hone our \u0026ldquo;common sense\u0026rdquo; and configuring vim helping me honing my \u0026ldquo;common sense\u0026rdquo;.\nAnother thing is that because of the concept \u0026ldquo;configuring by programming\u0026rdquo;, we can also extend the functionality of vim. For example, if you know about grep tools in unix-like environment, you might already know that vim has those functionality built-in with :grep command (for more info on :help :grep). Personally, I don\u0026rsquo;t really like the default functionality of :grep command, so I make my own implementation that have the same same functionality as :grep command, which is to search any line that have the same words as the input words. Honestly, that kind of experience, creating my own implementation of some feature is exciting because I get to design the mechanism from how the user interact with the feature until how to present the result.\nOf course what I did is not as complex as most software out there, but I think we should try to design a small mechanism first before we go into a complex mechanism. And that\u0026rsquo;s why I think configuring vim is a nice intro to designing a software. You get to try to make a simple mechanism that you need or want, and going up to something like lazy loading mechanism, which can be quite complex depending on your design and expected results.\nPlease keep in mind that this is not only limited to vim, we can also get the same benefits with other software that implement \u0026ldquo;configuring by programming\u0026rdquo; concept. I just use vim as an example because for me, it\u0026rsquo;s the easiest software to start with. When I start configuring vim back in the day, my vim config is not as complex as it is now. I only have one file for my vim config back then, nowadays tho, I need a whole directory to hold all my vim config. You could say that the more I know about vim, the more my vim config grow. It\u0026rsquo;s like our vim config is growing with us, so you don\u0026rsquo;t need to know everything from the start, take your time and enjoy the process of finding things out in vim.\nAlright, that\u0026rsquo;s all from me. Happy vimming!\n"},{"ref":"https://bruhtus.github.io/posts/git-rebase-chained-branch/","title":"Git Rebase Chained Branch","section":"posts","tags":["Git"],"date":"2025.04.18","body":"Let\u0026rsquo;s say we try to develop a feature B, but those feature need feature A which haven\u0026rsquo;t merged already. So, to prevent the diff changes getting larger, we create new branch to develop feature B. And then, in the middle of development of feature B, the branch that contains feature A got merged into main branch and all the commit from feature A got squashed into one commit. And when we try to rebase the feature B branch with main branch, we got a conflict from a commit which previously in feature A branch.\nTo make the situation clear, let\u0026rsquo;s take a look at this:\nBefore: A---B (main branch) \\ C---D (feature-A branch) \\ E---F (feature-B branch) After: A---B---G (main branch, with G commit as squashed C and D commit) \\ E---F (feature-B branch) The steps to make the conflict scenario are:\nmkdir git-test cd git-test git init -b main echo \u0026#39;first\u0026#39; \u0026gt; something git add something git commit -m \u0026#39;first\u0026#39; echo \u0026#39;second\u0026#39; \u0026gt;\u0026gt; something git commit -am \u0026#39;second\u0026#39; git checkout -b feature-A echo \u0026#39;third\u0026#39; \u0026gt;\u0026gt; something git commit -am \u0026#39;third\u0026#39; echo \u0026#39;fourth\u0026#39; \u0026gt;\u0026gt; something git commit -am \u0026#39;fourth\u0026#39; git checkout -b feature-B echo \u0026#39;fifth\u0026#39; \u0026gt;\u0026gt; something git commit -am \u0026#39;fifth\u0026#39; echo \u0026#39;sixth\u0026#39; \u0026gt;\u0026gt; something git commit -am \u0026#39;sixth\u0026#39; git checkout main git merge --squash feature-A git commit --no-edit git checkout feature-B git rebase main  To get out from the conflict, we can use git rebase --abort.\n Now, to prevent the conflict, instead of using git rebase, we can use git rebase --fork-point --onto like this:\ngit rebase --fork-point --onto main feature-A feature-B Flag --fork-point is to find the common commit point from main branch with feature-A branch and flag --onto is to change the parent commit from feature-A branch to main branch and apply the commit that exist in feature-B branch but not in feature-A branch \u0026ldquo;onto\u0026rdquo; main branch commit.\nNow, let\u0026rsquo;s say we have more than one branch that depends on feature A branch. Do we need to do git rebase --fork-point --onto ... multiple time? Well, fortunately there\u0026rsquo;s a flag --update-refs that will automatically do that for us.\nTo make the situation clear, let\u0026rsquo;s take a look at this:\nBefore: A---B (main branch) \\ C---D (feature-A branch) \\ E---F (feature-B branch) \\ G---H (feature-C branch) \\ I---J (feature-D branch) After: A---B---K (main branch, with K commit as squashed C and D commit) \\ E---F (feature-B branch) \\ G---H (feature-C branch) \\ I---J (feature-D branch) To use --update-refs, we need to rebase the top of the chained branch, which in the case above is feature-D branch. With that in mind, we can use this command:\ngit rebase --update-refs --onto main feature-A feature-D --onto main feature-A feature-D means rebase the range of commits in feature-A branch until feature-D branch on top of commits from main branch. We use feature-A because it\u0026rsquo;s the base branch for our chained branch and feature-D because it\u0026rsquo;s the top of the chained branch.\n We don\u0026rsquo;t need to use --fork-point to guess which common commit in all those branch because --update-refs already do that for us.\n Now, what should we do if we merge feature-B branch? The situation would be like this:\nBefore: A---B---K (main branch, with K commit as squashed C and D commit) \\ E---F (feature-B branch) \\ G---H (feature-C branch) \\ I---J (feature-D branch) After: A---B---K---L (main branch, with L commit as squashed E and F commit) \\ G---H (feature-C branch) \\ I---J (feature-D branch) We can use the same command as above but, instead of using feature-A, we use feature-B instead, like this:\ngit rebase --update-refs --onto main feature-B feature-D Next scenario, let\u0026rsquo;s say we create new branch feature-E from feature-D branch and add new commit in it. And then, we got some feedback for feature-C branch so we need to add new commit. The situation would be like this:\nBefore: A---B---K---L (main branch, with L commit as squashed E and F commit) \\ G---H---O (feature-C branch) \\ I---J (feature-D branch) \\ M---N (feature-E branch) With the situation above, we can checkout into feature-E branch or the top of the chained branch and use --update-refs like this:\ngit rebase --update-refs feature-C Alright, that\u0026rsquo;s it. See you next time!\nSide Note We can push all those branch as an atomic operation, which means, if one of the branch failed to push into a remote branch, the others branch won\u0026rsquo;t update the remote branch too. To do that, we can use flag --atomic in git push like this:\ngit push --atomic origin feature-C feature-D feature-E Please keep in mind that if you\u0026rsquo;re rebasing the git branch you want to push, you might need flag --force-with-lease in git push, like this:\ngit push --force-with-lease --atomic origin feature-C feature-D feature-E References  Stackoverflow git rebase \u0026ndash;onto answer . Women on rails git rebase \u0026ndash;onto an overview . Stackoverflow git rebase \u0026ndash;fork-point answer . Git rebase \u0026ndash;update-refs blog post   "},{"ref":"https://bruhtus.github.io/posts/use-arch-linux-vm-for-linux-kernel-testing/","title":"Use Arch Linux VM for Linux Kernel Testing","section":"posts","tags":["Linux"],"date":"2025.03.30","body":"A Brief Intro Have you wondered \u0026ldquo;how can we test a linux kernel without breaking our linux operating system installation?\u0026rdquo;. One of the method is using virtual machine (VM) and in this post we\u0026rsquo;ll use arch linux in virtual machine to try out different version of linux kernel.\nThere are 2 scenarios that we will discuss on this post:\n Using arch linux VM with arch linux as host system. Using arch linux VM with different linux distro as host system.  We\u0026rsquo;re going to work with 2 systems on 1 machine, so we need to know which command need to run on which machine. For that purpose, we\u0026rsquo;re going to use this indicator to differentiate:\n# @host # for real machine system # @guest # for virtual machine system Arch Linux as Host System The prerequisite for this scenario is that we need to use arch linux on the real machine or host machine we want to install the virtual machine.\nAfter we make sure that the host machine system we have is using arch linux, the first thing we need to do is create a file to store arch linux installation for the virtual machine. We can define how much space we\u0026rsquo;ll allocate to the installation, but in this post we\u0026rsquo;ll use 15G just to be save. To create a file for virtual machine OS installation, we can use this command:\n# @host truncate -s 15g arch-linux-disk.raw We can name the file whatever we want, but in this case, we\u0026rsquo;ll use arch-linux-disk.\nAfter that, we need to create a filesystem volume by formatting the file into ext4 with this command:\n# @host mkfs.ext4 arch-linux-disk.raw And then, we can mount the filesystem partition into /mnt like this (assuming we\u0026rsquo;re not login as root user):\n# @host sudo mount arch-linux-disk.raw /mnt  If your /mnt is occupied, you can use another directory such as vm, like this: sudo mount --mkdir arch-linux-disk.raw vm\n After mounting the filesystem, make sure arch-install-scripts and qemu-base package installed on our system by running this command:\n# @host sudo pacman -S arch-install-scripts qemu-base  arch-install-scripts contains some useful scripts to install arch linux such as pacstrap and arch-chroot. qemu-base is for creating the virtual machine we will use later.\n After those packages installed, we can use this command to install arch linux on arch-linux-disk.raw:\n# @host sudo pacstrap -c /mnt base base-devel vim dhcpcd  flag -c is to use package cache on host system, so we only need to download some out of date packages rather than downloading all the packages. base is a package for basic arch linux installation and base-devel is for packages such as sudo and grep. vim is my editor of choice, so you don\u0026rsquo;t have to install this if you don\u0026rsquo;t want to. dhcpcd is to connect the virtual machine system to the internet.\n After everything installed, we can go into the new filesystem with this command:\n# @host sudo arch-chroot /mnt After that, we can setup password for root user with this command:\n# @guest passwd Also, we can start dhcpcd for internet connection on our virtual machine with this command:\n# @guest systemctl enable --now dhcpcd To exit from the guest filesystem, we can use ctrl-d or exit command. After exit from the guest filesystem, we can unmount the filesystem using this command:\n# @host sudo umount /mnt Use Custom Linux Kernel on QEMU VM Before we spin up our virtual machine, we need to compile the linux kernel.\nFirst thing we need to do is download the linux kernel source code from one of available source on https://web.git.kernel.org/pub/scm/linux/kernel/git . In this post, we will download the source from Linux Torvalds source code. For example, let\u0026rsquo;s say we want to download the source code for tag v6.14, we can use wget like this:\n# @host wget https://web.git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/snapshot/linux-6.14.tar.gz Or we can just click on the Download link. Or we can clone the repo with this command:\n# @host git clone --depth=1 git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git torvalds-kernel  --depth=1 means that we download 1 commit instead of all the commits. torvalds-kernel is the directory name where we clone the linux kernel source code git repository (instead of naming it linux, we name it torvalds-kernel).\n After we download the linux kernel source code, we can change directory into it. And then, we can create a basic config x86_64 config using this command:\n# @host make x86_64_defconfig And then we need to add config for virtual machine with this command:\n# @host make kvm_guest.config After that, we can compile the linux kernel with this command:\n# @host make -j \u0026lt;n\u0026gt;  \u0026lt;n\u0026gt; is how much threads we use. For example, make -j 8 means we use 8 threads.\n We can change the config with command:\nmake nconfig  Please keep in mind, DO NOT directly edit the .config file. Use make nconfig instead.\n After we finish compiling the linux kernel, we can use that linux kernel in our QEMU virtual machine with this command:\n# @host qemu-system-x86_64 \\  -hda /path/to/arch-linux-disk.raw \\  -m 2g \\  -nographic \\  -kernel /path/to/linux-6.14/arch/x86/boot/bzImage \\  -append \u0026#34;root=/dev/sda rw console=ttyS0 loglevel=5\u0026#34; \\  -enable-kvm \\  -nic user,hostfwd=tcp::2222-:22 For example:\n# @host qemu-system-x86_64 \\  -hda $HOME/.local/state/qemu/kernel-dev-disk.raw \\  -m 2g \\  -nographic \\  -kernel $HOME/.local/state/qemu/linux-6.14-rc5/arch/x86/boot/bzImage \\  -append \u0026#34;root=/dev/sda rw console=ttyS0 loglevel=5\u0026#34; \\  -enable-kvm \\  -nic user,hostfwd=tcp::2222-:22 Here\u0026rsquo;s a brief explanation of each flags:\n -hda is to use a file as a hard disk. -hda is equivalent to sda partition, -hdb is equivalent to sdb, and so on. -m is RAM size for the virtual machine. -nographic make QEMU run on the terminal instead of graphical window. -kernel is to define the custom kernel we\u0026rsquo;ll use in virtual machine. -append is to define kernel parameters we\u0026rsquo;ll be using. We can\u0026rsquo;t use this without -kernel flag. For more info about kernel parameters, we can take a look the kernel docs . -enable-kvm is to enable KVM virtualization support (if available). -nic is to forward internet connection of host machine to virtual machine.  To exit the virtual machine in nographic mode, we can use ctrl-a x.\nAnother Linux Distro as Host System If we use another linux distro, we need to install arch linux in virtual machine from scratch. You can take a look at my previous post for how to install arch linux using QEMU. Please keep in mind that you need to install qemu and vncviewer package on your host system.\nUse Custom Linux Kernel on QEMU VM After installing arch linux in virtual machine, we need to compile the linux kernel based on the kernel config in the virtual machine.\nWe can run the virtual machine with shared directory with this command:\n# @host qemu-system-x86_64 \\  -m 2g \\  -enable-kvm \\  -fsdev local,id=fs1,path=/path/to/linux-kernel,security_model=none \\  -device virtio-9p-pci,fsdev=fs1,mount_tag=shared_directory \\  -nic user,hostfwd=tcp::2222-:22 \\  \u0026lt;qemu-image\u0026gt; For example:\n# @host qemu-system-x86_64 \\  -m 2g \\  -enable-kvm \\  -fsdev local,id=fs1,path=$HOME/.local/state/qemu/linux-6.14,security_model=none \\  -device virtio-9p-pci,fsdev=fs1,mount_tag=shared_directory \\  -nic user,hostfwd=tcp::2222-:22 \\  arch-linux-img  Please don\u0026rsquo;t forget to add the shared directory in /etc/fstab.\n In the example above, we\u0026rsquo;re using the linux kernel source code directory as a shared directory.\nInside the virtual machine, we need to copy the kernel config into the shared directory. Assuming our shared directory is in /home/user/shared, we can use this command:\n# @guest zcat /proc/config.gz \u0026gt; /home/user/shared/.config After we got the config, we need to adjust the config so that we only compile linux kernel with the module we need rather than compiling with all module enabled. This can save time to compile the linux kernel. To do that we can use this command:\n# @guest make localmodconfig Sometimes, there are new features or new config parameters in the source code repo and we need to confirm if we want to add those new config parameters in our .config file or not. To reduce the amount of confirmation we need to make, we can use this command:\n# @guest make olddefconfig Also, we need to add a config for virtual machine with this command:\n# @guest make kvm_guest.config If we want to edit some config parameters, we can use:\n# @guest make nconfig After that, we can shutdown the virtual machine and compile the linux kernel with this command:\n# @host make -j \u0026lt;n\u0026gt;  \u0026lt;n\u0026gt; is how much threads we use. For example, make -j 8 means we use 8 threads.\n And then, we need to compile the linux kernel modules with this command:\n# @host make modules After we compile both linux kernel and the modules, we can turn on the virtual machine again and put the modules into /lib/modules/\u0026lt;kernel-name\u0026gt; in the virtual machine with this command:\n# @guest sudo make modules_install And then, we need to copy the linux kernel into /boot with this command:\n# @guest sudo cp /home/user/shared/arch/x86/boot/bzImage /boot/vmlinuz-\u0026lt;name\u0026gt;  We can use any \u0026lt;name\u0026gt; we want, but to make it easier to organize, i suggest you use the same \u0026lt;name\u0026gt; in the following step.\n After that, we need to copy the default mkinitcpio preset like this:\n# @guest sudo cp /etc/mkinitcpio.d/linux.preset /etc/mkinitcpio.d/linux-\u0026lt;name\u0026gt;.preset And then, we need to replace ALL_kver, default_image, and fallback_image with the \u0026lt;name\u0026gt; we assign before. For example, let\u0026rsquo;s say we use torvalds-vm as \u0026lt;name\u0026gt;, we can use something like this:\n... ALL_kver=\u0026quot;/boot/vmlinuz-torvalds-vm\u0026quot; ... default_image=\u0026quot;/boot/initramfs-torvalds-vm.img\u0026quot; ... fallback_image=\u0026quot;/boot/initramfs-torvalds-vm-fallback.img\u0026quot; After that, we need to generate initramfs using this command:\n# @guest sudo mkinitcpio -p linux-\u0026lt;name\u0026gt; Assuming we\u0026rsquo;re using grub as our bootloader in a virtual machine, we need to update the grub config like this:\n# @guest sudo grub-mkconfig -o /boot/grub/grub.cfg Now, we need to reboot the virtual machine and check if everything works fine. If we check on grub \u0026ldquo;Advanced options for Arch Linux\u0026rdquo;, we should we able to see our custom linux kernel.\nAlright, that\u0026rsquo;s it. See you next time!\nReferences  Create an ArchLinux image for kernel testing  FLUSP Kernel Compilation and Installation   "},{"ref":"https://bruhtus.github.io/posts/install-arch-linux-on-virtual-machine/","title":"Install Arch Linux on Virtual Machine","section":"posts","tags":["Linux"],"date":"2025.03.02","body":"A Brief Intro In this post, we\u0026rsquo;ll install arch linux on virtual machine with QEMU and add a shared directory between host machine and virtual machine.\nBefore we start, make sure to install QEMU on your system. If your system using arch linux too, we can install qemu-base and tigervnc package. tigervnc package for viewing the virtual machine from QEMU. We will be using SeaBIOS, which is legacy BIOS implementation and the default BIOS on QEMU (at the time of writing this post).\nThis post assume that you use ext4 filesystem format. If you use another file format, like btrfs, you might need to use different configuration.\nWe\u0026rsquo;re going to work with 2 systems on 1 machine, so we need to know which command need to run on which machine. For that purpose, we\u0026rsquo;re going to use this indicator to differentiate:\n# @host # for real machine system # @guest # for virtual machine system QEMU Disk Image The first thing we need to do is to create a disk image. We can do that with this command:\n# @host qemu-img create -f qcow2 \u0026lt;image-path\u0026gt; 15g  qcow2 is the current QEMU image format, and one of the benefit of it is that it will increase the image size based on the virtual machine usage with the limit being the image size given when creating the image. \u0026lt;image-path\u0026gt; is where we store the image. For example: ~/arch-linux-qemu. 15g is the image size.  Installation The first thing we need to do is download the arch linux ISO on the download page .\nAfter that, we can use this command:\n# @host qemu-system-x86_64 -enable-kvm -cdrom \u0026lt;path-to-arch-linux-ISO\u0026gt;.iso -boot order=d -drive file=\u0026lt;image-path\u0026gt; -m 2g  -enable-kvm is to enable full virtualization using KVM. Our machine might not support KVM, so make sure we have KVM support when using this flag. We can read the arch wiki for the instruction. -cdrom \u0026lt;path-to-arch-linux-ISO\u0026gt;.iso is to use the file as CD-ROM image. -boot order=d is to specify that we want to boot from CD-ROM first. -drive file=\u0026lt;image-path\u0026gt; is to specify the disk image to use as the drive on the system. Image path should be a full path like $HOME/arch-linux-img -m 2g is to specify the RAM to use in the virtual machine.  Mostly we can follow the arch linux installation guide , but be careful of the partition. Because we will use the default QEMU BIOS, which is a legacy BIOS, we need to partition the drive with legacy BIOS in mind. We can check on the partition example layout on arch wiki . For this post, we\u0026rsquo;ll use BIOS/GPT layout to not deviate too much from the UEFI/GPT layout.\nIf after running the command above, the virtual machine is not automatically appear, we can use command:\n# @host vncviewer :\u0026lt;port\u0026gt; Make sure to install tigervnc or something similar before running the command. We can get the port from the QEMU command above. For example:\n# @host VNC server running on ::1:5900 5900 is the port we want.\nAfter inside the virtual machine, you can follow the usual arch linux installation guide. To be able to use host machine internet connection, we might want to install dhcpcd in the pacstrap step, like this:\n# @guest pacstrap -K /mnt base linux linux-firmware dhcpcd and enable it after arch-chroot like this:\n# @guest systemctl enable dhcpcd The important part is the drive partition, so we\u0026rsquo;ll skip into the partition part.\nPersonally, i don\u0026rsquo;t really need swap on my virtual machine so in this post we\u0026rsquo;ll only make 2 partition:\n BIOS boot partition (1 MB) with partition type BIOS boot. root partition (the rest of storage) with partition type linux root (x86-64).  We can use cfdisk /dev/\u0026lt;drive\u0026gt; to partition our drive. To look what our drive name is, we can use lsblk command. For example:\n# @guest cfdisk /dev/sda After the partition, we need to format the root partition to ext4 with this command:\n# @guest mkfs.ext4 /dev/\u0026lt;root-partition\u0026gt; for example:\n# @guest mkfs.ext4 /dev/sda2 After that, we can mount the root partition with this command:\n# @guest mount /dev/\u0026lt;root-partition\u0026gt; /mnt for example:\n# @guest mount /dev/sda2 /mnt After that we can follow the rest of the arch linux installation guide until we need to install the bootloader.\nFor this post, we\u0026rsquo;ll use grub2 as our bootloader. First, we need to install grub with this command (after running arch-chroot):\n# @guest pacman -S grub After grub installed, we can use this command to install the bootloader:\n# @guest grub-install --target=i386-pc /dev/\u0026lt;drive-name\u0026gt; keep in mind that we need to provide the drive name, not the partition name. For example:\n# @guest grub-install --target=i386-pc /dev/sda After that, we need to generate the grub config with this command:\n# @guest grub-mkconfig -o /boot/grub/grub.cfg To run the virtual machine, we can use this command:\n# @host qemu-system-x86_64 -enable-kvm -nic user,hostfwd=tcp::2222-:22 -m 2g -smp cores=4,cpus=4 \u0026lt;image-path\u0026gt; If we want to add shared directory between virtual machine and host machine, we can use -fsdev and -device like this:\n# @host qemu-system-x86_64 -enable-kvm -nic user,hostfwd=tcp::2222-:22 -fsdev local,id=fs1,path=\u0026lt;host-shared-directory-path\u0026gt;,security_model=none -device virtio-9p-pci,fsdev=fs1,mount_tag=\u0026lt;mount_tag\u0026gt; -m 2g -smp cores=4,cpus=4 \u0026lt;image-path\u0026gt; and then add to /etc/fstab like this:\n... \u0026lt;mount_tag\u0026gt; \u0026lt;mounting-path-on-virtual-machine\u0026gt; 9p uid=xxxx,gid=xxxx,trans=virtio 0 0 Let\u0026rsquo;s say we have \u0026lt;mount_tag\u0026gt; as shared_directory and we want to mount it on /home/user/shared. Also, to get the uid and gid we can use id \u0026lt;user\u0026gt;, for example id bruhtus. Let\u0026rsquo;s assume the uid and gid of the user is 1000. We can use the command like this:\n# @host qemu-system-x86_64 -enable-kvm -nic user,hostfwd=tcp::2222-:22 -fsdev local,id=fs1,path=\u0026lt;host-shared-directory-path\u0026gt;,security_model=none -device virtio-9p-pci,fsdev=fs1,mount_tag=shared_directory -m 2g -smp cores=4,cpus=4 \u0026lt;image-path\u0026gt; and in /etc/fstab like this:\nshared_directory /home/user/shared 9p uid=1000,gid=1000,trans=virtio 0 0 Alright, that\u0026rsquo;s it. See you next time!\nSide Note If we want to use shutdown or reboot command from non-root user, we can install polkit package, like this:\n# @guest pacman -S polkit We can also ssh into the virtual machine from host machine. First we need to install openssh on virtual machine and host machine, don\u0026rsquo;t forget to start sshd service. And then, we create ssh key with this command:\n# @host ssh-keygen -t rsa -f ~/.ssh/qemu (we can change the name qemu with something else).\nAfter that, we can use this command to copy the ssh key into the virtual machine:\n# @host ssh-copy-id -i ~/.ssh/qemu.pub -p 2222 $USER@127.0.0.1 please keep in mind that we need to have the same username on the host machine and virtual machine. If you use different username on your virtual machine, you can change $USER with the username on your virtual machine.\nTo delete the hashed key of virtual machine from ssh known_hosts file, we can use this command:\n# @host ssh-keygen -R \u0026#39;[127.0.0.1]:2222\u0026#39; References  FLUSP Use QEMU To Play with Linux Kernel  Create an ArchLinux image for kernel testing .  "},{"ref":"https://bruhtus.github.io/posts/unintegrated-development-environment/","title":"Unintegrated Development Environment","section":"posts","tags":["Reflections"],"date":"2024.10.20","body":"After watching video about acme in this article , which introduces the concept of Integrating Development Environment instead of Integrated Development Environment which usually abbreviated to IDE, i thought to myself \u0026ldquo;why not having a development environment without integration at all?\u0026rdquo;. This might sound crazy for some people, but this is my development environment at the time of writing this post.\nJust to be clear, this approach might not be suitable for some software development, such as game development. The point is, this post is just for reference about someone else\u0026rsquo;s development environment.\nIn my time as a linux user, not even as a software developer, i realize that there\u0026rsquo;s a cost of integration and one of them is maintenance. Here\u0026rsquo;s a little bit of a back story when i found that out. Back then when i use elementary OS, which is ubuntu-based linux distro, i try to upgrade into the newer version, and guess what? I can\u0026rsquo;t use the \u0026ldquo;default\u0026rdquo; image viewer. Because i can\u0026rsquo;t use the \u0026ldquo;default\u0026rdquo; image viewer, i need another image viewer which lead me to install another image viewer. Now i have 2 image viewer on my system which i feel like redundant. So, as a new linux user, i try to uninstall the \u0026ldquo;default\u0026rdquo; image viewer, and guess what? It break the entire desktop environment. Isn\u0026rsquo;t that nice? Like hell it is!\nThat incident make me realized that when we tightly integrated our software with each other, when one software break or missing, our system might falling apart.\nAnother example is the \u0026ldquo;breaking changes\u0026rdquo; culture of modern software development. In my observation, software development nowadays does not really care about improving user experience, it\u0026rsquo;s more like making the changes for the sake of \u0026ldquo;there\u0026rsquo;s some new changes\u0026rdquo;. Whether those new changes is useful to the user or not, that doesn\u0026rsquo;t really matter.\nThe point is, when something break because of an update in our integration system, it\u0026rsquo;s kind of hard to find out the problem. Especially if we\u0026rsquo;re using \u0026ldquo;modern software\u0026rdquo; which most likely introduce \u0026ldquo;breaking changes\u0026rdquo; on every release.\nMy base development environment is tmux, and my text editor is vim. Other than that? I use shell. The majority of my workflow is using shell. I didn\u0026rsquo;t integrate my text editor or shell with tmux. I could change my text editor or shell to something else, and it won\u0026rsquo;t effect my tmux configuration. When i changed my tmux configuration or tmux introduce breaking changes, it won\u0026rsquo;t effect my text editor or shell.\nBecause my development environment is not integrated with each other, i have the flexibility of changing things. Which can be a good thing or a bad thing, depends on personal preferences.\nSo, the point is that we need to approach integration with caution. Not everything need to be integrated with each other. We also need to realize that integration has a cost, and one of them is maintenance.\n"},{"ref":"https://bruhtus.github.io/posts/reflecting-on-intelligence/","title":"Reflecting on Intelligence","section":"posts","tags":["Reflections"],"date":"2024.05.26","body":"Over the years i\u0026rsquo;ve been thinking whether having a high intelligence is a blessing or a curse. Humans, who supposedly have the highest intelligence over other living creatures on earth, seems like having more stress than other living creatures.\nWith those high intelligence, human make a lot of innovation and one of them is electricity. One of the benefit of electricity is that electricity enable human doing activity at night, and that can cause some problems, one of them is lack of sleep. With electricity, we can even be productive at night which can cause us to forget about sleep or rest in general. When we lack of sleep, our performance will drop the next day and often times can cause some trouble during day-to-day activities. If we don\u0026rsquo;t address our lack of sleep, it will cause us some health issue.\nThe point is, with every innovations human made, human life getting more complex. For example, electricity enable us to do activity at night easily which make us need to keeping track of time so that we can have enough sleep. I\u0026rsquo;m sure there are some people who is counting how much they get to sleep before actually sleeping and in turn make those people anxious about oversleep which make their sleep not peaceful.\nThose complexity in human day-to-day life make me thinking, is having a high intelligence a good thing or a bad thing? For example, i\u0026rsquo;ve never seen an animal having lack of sleep and even if there\u0026rsquo;s an animal with lack of sleep, those animal probably in the minority. Let\u0026rsquo;s take cat for an example, have you ever see a cat keeping track of time when to sleep? Personally, i\u0026rsquo;ve never seen that kind of cat before. The kind of cat i\u0026rsquo;ve seen is the one who sleep whenever they want.\nIt\u0026rsquo;s like human trying so hard to make their life better at a cost of their freedom. I know that living in total freedom can cause a lot of problems, but that does not mean we should give up on our freedom. For example, with the innovation of instant messaging app like whatsapp, we can get an instant feedback when messaging someone. Matter fact, because of those instant messaging app, a lot of people expect we respond immediately after they send their message which can cause anxiety for some people. Let\u0026rsquo;s say you are already clock out from work at 5 PM, and then you want to enjoy dinner with your friends. In the middle of dinner, you get a message from your boss asking about some details of the project you\u0026rsquo;re currently in. Some people have the authority to ignore those kind of messages outside of working hours, but most people don\u0026rsquo;t. The point is, because of instant messaging app, the distinction between working and non-working hours almost non-existent because a lot of people expecting a fast response for their message. Yeah you should quit from those kind of company, but then again, most company do that and slowly those kind of things become a norm. It\u0026rsquo;s especially hard if you met a hypocrite who say \u0026ldquo;you should have another activity outside of working\u0026rdquo; and then the next thing they do is message you with \u0026ldquo;please submit a report for project X before 10 PM today\u0026rdquo; at 9 PM.\nThat\u0026rsquo;s why i keep thinking, \u0026ldquo;is having a high intelligence a blessing or a curse?\u0026rdquo;. If we compare our life with cat\u0026rsquo;s life, our life seems more miserable than a cat. Most of the cat i\u0026rsquo;ve seen enjoying their life to the fullest without anxiety of the next day, unlike most human.\n"},{"ref":"https://bruhtus.github.io/posts/manage-dotfiles-with-git/","title":"Manage Dotfiles With Git Working Repository","section":"posts","tags":["Linux","Git"],"date":"2024.04.08","body":"A Brief Intro For those who read this article without knowing what dotfiles are, it\u0026rsquo;s basically a configuration files. The purpose of backing up dotfiles is so that we don\u0026rsquo;t need to reconfigure our tools on a new system.\nThere are a lot of ways to backup and manage dotfiles, and one of them is using git.\nIn this article we will explore how to backup and manage dotfiles using git, the full working git repository rather than the bare one which a lot of people using.\nHow? What i like to do on my free time is browsing random stuff on internet without a goal, just mindlessly browsing some topic that i\u0026rsquo;m interested in and see if i could find something interesting. Letting your mind wondering, some people said.\nWhile i\u0026rsquo;m browsing on the internet about zsh, i found this github issue discussion about managing dotifles . And when i read through those discussions, i wonder if i can use the full working git repository, which basically the default git repository, rather than the bare one. So let\u0026rsquo;s try that, i guess.\nBack when i first trying to backup my dotfiles, i only know two method:\n Using git bare repository from atlassian article . Using GNU stow which i forgot where i learn that from.  The method i pick for my first dotfiles manager was git bare repository. The reason i go with git bare repository instead of GNU stow is that i don\u0026rsquo;t really want to mindlessly spend my inodes by creating a bunch of symlinks. I have experience with running out of inodes, which basically means that you have some storage left but you cannot create a new file. I know that linux or maybe unix-like system these day has quite a large number of inodes, but that does not mean i need to be wasteful of inodes, right?\nWhat bugging me for quite a while when using git bare repository to manage my dotfiles is that, do i really need to use the bare version of git repository to manage my dotfiles? And just recently i found that the answer is no.\nSo, how do we use the full git working directory, the default one instead of the bare one? It turns out that we can separate the git directory (the .git directory) and the working directory without using the bare repository!\nPlease keep in mind that my experience with git at the time is less than 6 months, maybe around 3 months. So it took me everything i got to just following the git bare dotfiles manager guide. Now, fast forward 2 years later, i made my own dotfiles manager using git. You can check them on my dotfiles repo here .\nThe key point of using git working repository instead of bare repository is this:\ngit --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; init What the command above do is initialize git repository with the specified git directory and working directory. If we didn\u0026rsquo;t specified the working directory but specified the git directory, git will create the bare repository instead of working repository. The key part of this method is --git-dir=\u0026quot;...\u0026quot; --work-tree=\u0026quot;...\u0026quot;, so you might want to create an alias like this:\nalias config=\u0026#39;git --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34;\u0026#39; Now you might be wondering, why should we use working repository instead of bare repository? It\u0026rsquo;s because we will most likely change our dotfiles, which basically means we\u0026rsquo;re still \u0026ldquo;working\u0026rdquo; on the dotfiles. So it make more sense to use working repository instead of bare repository because we\u0026rsquo;re still changing something to repository. From what i know, git bare repository is good for sharing and backup files that we don\u0026rsquo;t really change. For example, let\u0026rsquo;s say you are afraid that github will suspend your account and you don\u0026rsquo;t have any backup for your private repository, so you basically lost your private repository. With git bare repository, you can setup a system that only save the git objects of the repo (the one inside .git directory) and update it regularly by pulling from the remote repository. And because you only save the git objects instead of the actual files, your storage usage is lower than storing the actual files. As far as i know, that\u0026rsquo;s the benefit of git bare repository. With that in mind, it\u0026rsquo;s make more sense to use a working repository instead of bare repository in this case.\nThere are 3 important step when we want to get the existing dotfiles on a new system:\n Initialize new git repo. Doing git reset, the default or --mixed one. Doing git checkout.  We already talking about the initialize part for a bit before, which basically just the usual git init. Now let\u0026rsquo;s talk about the git reset part. git reset or git reset --mixed is basically reset the index or staging area on current branch with specific commit. That means, if we don\u0026rsquo;t have any commit on current branch, git reset or git reset --mixed will fill up our index or staging area with commit we specify. Now after we fill up our index or staging area with specific commit, we can generate all or specific files with git checkout -- \u0026lt;path\u0026gt;.\nHonestly the most interesting part is the git checkout part. With git checkout, we can do full dotfiles init or partial init. What i mean by partial init is that when we don\u0026rsquo;t need all the files on the new system, let\u0026rsquo;s say we only our vim config, we can just do git checkout -- ~/.vim and git will only generate the .vim directory with all of the files in it. The benefit of that, is we can put all our config in one git repository and still able to only generate the files we need rather than all the files in the git repository. To initialize all the files in our dotfiles repository, we can do git checkout -- ~.\nSo, to sum up the 3 important command for this to work is:\ngit --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; init git --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; reset git --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; checkout -- \u0026lt;path\u0026gt; Alright, that\u0026rsquo;s it for this article. I will let you explore the implementation and the possibilities of this method yourself. See you next time!\nReferences  Romkatv\u0026rsquo;s bootstrap dotfiles  List of dotfiles manager .  "},{"ref":"https://bruhtus.github.io/posts/the-time-artificial-intelligence-take-over-humanity/","title":"The Time Artificial Intelligence Take Over Humanity","section":"posts","tags":["Reflections"],"date":"2024.02.19","body":"One evening i read an article about if bill gates could ask a time traveller, he\u0026rsquo;d want to know whether AI eventually doomed or helped humanity .\nThose question is quite interesting, no one really know if artificial intelligence (AI) will helped or doomed humanity. So, let\u0026rsquo;s talk about it.\nRather than explaining about the history of AI that only a few people cares, let\u0026rsquo;s talk about what make people scared about AI.\nThere\u0026rsquo;s this topic about AI will take over humanity, or doomed humanity, or ending the humankind, etc. In my opinion, those kind of topic can become reality on one condition, when the human intelligence is lower than the artificial intelligence.\nNow, you might be thinking, \u0026ldquo;well, the current AI is smarter than me because i can\u0026rsquo;t solve those specific problem\u0026rdquo;, and guess what? The AI might not be able to solve the problem that you can solve. The AI will give a better answer if they have a better material for their training, and you know what? Human is not that different, the AI might be able to solve the problem you can\u0026rsquo;t solve because they have resources that you don\u0026rsquo;t have as their learning material (which can be questionable from the legal perspective).\nThe point is, AI also need training to increase their intelligence. Now, this is something that scared me personally. As the AI become more advance, human\u0026rsquo;s training or learning slowing down. I\u0026rsquo;ve seen a few people mindlessly take any output from the AI and present them as their answer or solution. It\u0026rsquo;s like they use AI as a reason to decrease or stop learning altogether.\nI understand that laziness can be the source of innovation, and one of those innovation is automation. Most of the time, human use automation to handle repetitive task. Will automation doomed humanity? Not really, because the most advance automation still need human supervision from time to time. Some people might lose their job because of automation, but at the same time, those kind of jobs, which do repetitive task should be left to the machine instead of human to increase human\u0026rsquo;s quality of life. If you want an example of a job that get replaced by machine, you can check the history of glass bottle, how people back in the day make glass bottle manually.\nClearly teaching a machine or a system to do some task can be beneficial for human. You could say those machine or system that human teach, is more like an assistant instead of a replacement. We, as a human, still need to supervise those machine or system, and to be able to do that, we can\u0026rsquo;t stop our learning process so that we can stop the machine or system when they misbehave and potentially endanger human life.\nSo, in this post i want to remind people that if they are scared AI will doomed humanity, then they should keep learning and prevent that from happening themselves. Do you think that human intelligence will be lower than artificial intelligence in the future? We will see.\n"},{"ref":"https://bruhtus.github.io/posts/postgres-export-query-result-from-remote-to-local-csv/","title":"Postgresql Export Query Result From Remote to Local in CSV file","section":"posts","tags":["Postgresql"],"date":"2023.07.25","body":"A Brief Intro Have you ever thought of how to get postgresql query result from remote server to our local machine?\nIf so, this short post might be for you.\nHow? Before we start, please keep in mind that there are other methods to get postgresql query result from remote server to local machine, the method that i mentioned after this is the easier method for me.\nAlright, let\u0026rsquo;s get to it!\nBefore we start, you need to configure the ssh stuff like private key and so on. So i will assume you already set it up in your ssh config.\nIn this post we\u0026rsquo;ll use psql to get the query result and copy() function from postgresql.\nNow, let\u0026rsquo;s say we have this query:\nSELECTcount(*)asuser_countFROMusers;and we want to put those query result in user-count.csv.\nThe first thing we need to do is make a query file with the sql query above and copy() like this:\ncopy(SELECTcount(*)asuser_countFROMusers;)TOstdoutWITHcsvheaderFor simplicity, we will put those sql query in a file called query. After that, we can use this command:\nssh server-config \u0026#39;psql -d postgresql://user:password@localhost:5432/db-name\u0026#39; \u0026lt; query Please keep in mind that server-config is our ssh configuration in ~/.ssh/config, and postgresql://user:password@localhost:5432/db-name is our postgresql data source name (DSN).\nIf our query has result, we will see them in our terminal. Now to put those result in csv file, we can use redirection like this:\nssh server-config \u0026#39;psql -d postgresql://user:password@localhost:5432/db-name\u0026#39; \u0026lt; query \u0026gt; user-count.csv And, that\u0026rsquo;s it. Now we have our query result in csv file. Alright, see you next time!\nReferences  Stackoverflow ssh with psql . Stackoverflow psql using stdin .  "},{"ref":"https://bruhtus.github.io/posts/postgres-exist-clause/","title":"Postgresql Exist Clause","section":"posts","tags":["Postgresql"],"date":"2023.07.20","body":"A Brief Intro Have you ever wondering, rather than show the data, can we show the true or false if the data exist or not?\nIf so, then this short post might be for you!\nHow? In this post we will take a look at exists postgresql function. Before we start, please keep in mind that exists use subquery, so there is a penalty of subquery here.\nThe exists will evaluate if the subquery return any rows. If it returns at least one row, the result of exists is true, if not, then the result of exists is false.\nWe can use that in SELECT statement like this:\nSELECTexists(SELECTusers.emailFROMusersWHEREusers.email=participants.email)FROMparticipants;Or we can use it in WHERE statement like this:\nSELECTparticipants.emailFROMparticipantsWHEREexists(SELECTusers.emailFROMusersWHERE(users.role=\u0026#39;sponsor\u0026#39;ANDusers.email=participants.email))This can be useful if we don\u0026rsquo;t want to join the table. Alright, that\u0026rsquo;s it for this post. See you next time!\nReferences  Postgresql subquery expressions documentation . Stackoverflow return false if no rows .  "},{"ref":"https://bruhtus.github.io/posts/postgres-count-data-on-different-condition/","title":"Postgresql Count Data on Different Condition in The Same Query","section":"posts","tags":["Postgresql"],"date":"2023.07.18","body":"A Brief Intro Have you ever wondering how to count all the data and specific data with a certain condition on one query? If so, then this short post might be for you.\nLet\u0026rsquo;s get started!\nHow? Let\u0026rsquo;s say we organize an event and we store the participants data on our postgresql database.\nNow we want to know all the participants that registered for the event and all the participants that actually attended the event. That means, even though someone register for the event, they might not attend the event.\nWith that in mind, we can use FILTER clause in our query like this:\nSELECTcount(*)ASregistered_participants,count(*)FILTER(WHEREparticipants.status=\u0026#39;attended\u0026#39;)ASattended_participantsFROMparticipants;Alright, that\u0026rsquo;s it for now. See you next time!\nReferences  Postgresql sql expressions documentation . Stackexchange count with different condition on the same query .  "},{"ref":"https://bruhtus.github.io/posts/postgres-get-current-database-size/","title":"Postgresql Get Current Database Size","section":"posts","tags":["Postgresql"],"date":"2023.07.18","body":"A Brief Intro Have you ever wondering the size of our current postgresql database? If so, this short post might be for you.\nLet\u0026rsquo;s get started!\nHow? In postgresql, there\u0026rsquo;s a function called pg_database_size() which computes the total disk space used by the database with specified database name.\nWe can use pg_database_size() like this:\nSELECTpg_database_size(\u0026#39;db-name\u0026#39;);The output of pg_database_size() is in bytes, so if we want to display the output in a human-readable format (kb, mb, and so on), we can use function pg_size_pretty() like this:\nSELECTpg_size_pretty(pg_database_size(\u0026#39;db-name\u0026#39;));If we want to check all available database, we can use pg_database system table like this:\nSELECTpg_database.datnameASdb_name,pg_size_pretty(pg_database_size(pg_database.datname))ASdb_sizeFROMpg_databaseORDERBYpg_database_size(pg_database.datname)DESC;Alright, that\u0026rsquo;s all for now. See you next time!\nReferences  Postgresql System Administration Functions Documentation . Stackoverflow how to get db names and size .  "},{"ref":"https://bruhtus.github.io/posts/systemd-reset-failed-status/","title":"Systemd Reset Failed Status","section":"posts","tags":["Linux"],"date":"2023.07.18","body":"A Brief Intro Have you ever experience a failed systemd service and already trying to restart using systemctl restart \u0026lt;some-service\u0026gt; but the failed status still there?\nIf so, this short post might be for you! Let\u0026rsquo;s get started.\nReset Failed Status Now, if our systemd service has a failed status, we can try using command:\nsystemctl reset-failed \u0026lt;some-service\u0026gt; to remove the failed status and reset it to recent status.\nPlease keep in mind that reset-failed is dependent on Restart option, so if you use option Restart=always, there\u0026rsquo;s a chance that the failed status still there because systemd still trying to restart our service which result in another failed status.\nThere are still a lot more i need to learn about systemd service, so for now that\u0026rsquo;s all. See you next time!\n"},{"ref":"https://bruhtus.github.io/posts/postgres-count-weekly-with-bigint-data-type/","title":"Postgresql Count Weekly With Bigint Data Type","section":"posts","tags":["Postgresql"],"date":"2023.06.26","body":"A Brief Intro Let\u0026rsquo;s say we have a created_at column in our database with data type big integer that has a value with this equation:\nEXTRACT(EPOCHFROMNOW())*1000 Don\u0026rsquo;t ask me why the value of created_at like that because i have no idea.\n Now, the goals is to count every data per week.\nHow? How to do that? Because we use a unix timestamp which look like this 1687746597339, we need to convert those into timestamp using postgresql to_timestamp() function like this:\nto_timestamp(created_at/1000) We divide by 1000 because we multiple it by 1000 when we insert the value.\n And then, we need to truncate into a specific date. We can use date_trunc() function to truncate the date.\nAccording to the documentation, this is the parameter for date_trunc():\ndate_trunc(field,source[,time_zone])Valid values for field are:\n microseconds milliseconds second minute hour day week month quarter year decade century millennium  source is either timestamp or interval and return timestamp or interval depending on the source.\ntime_zone is an optional parameter to specify a different timezone. By default, truncation is done with the current timezone setting.\nIf we combine date_trunc() and to_timestamp(), we will get something like this:\ndate_trunc(\u0026#39;week\u0026#39;,to_timestamp(created_at/1000))If we make the entire query, it would be something like this (let\u0026rsquo;s say the table name is participants):\nSELECTdate_trunc(\u0026#39;week\u0026#39;,to_timestamp(created_at/1000))ASweekly,COUNT(*)AStotal_participantsGROUPBYweeklyORDERBYweeklyASC;And the result would be something like this:\nweekly|total_participants------------------------+-------------------------- 2023-06-1900:00:00+07|694202023-06-2600:00:00+07|69 00:00:00+07 is because the timezone setting on my database is asia/jakarta which is basically UTC+7. If your timezone setting is different, the timestamp might be different.\nStill not sure why the date_trunc() with field week start at monday, and, at the time of writing this post, i haven\u0026rsquo;t found a settings to change that.\n Alright, that\u0026rsquo;s it. See you next time!\nReferences  Postgresql Date/Time Functions and Operator Documentations . Postgresql tutorial date_trunc() function . Stackoverflow group by week in postgresql .  "},{"ref":"https://bruhtus.github.io/posts/postgres-update-multiple-rows-in-one-query/","title":"Postgresql Update Multiple Rows with One Query","section":"posts","tags":["Postgresql"],"date":"2023.06.01","body":"A Brief Intro Have you ever wondering how to update multiple rows with one query? Let\u0026rsquo;s say you want to change value on table A where the name is anu on year 2019, 2020, and 2021.\nRather than doing 3 query with different WHERE statement, we can do that with one query. Without further ado, let\u0026rsquo;s go straight to it!\nFROM statement Postgresql has FROM clause in UPDATE statement which let us use columns on other table, it use the same syntax as FROM clause in SELECT statement.\nWe can combine FROM clause with VALUES expression. VALUES in postgresql let us create a constant table which means we can generate a table with constant values that can be used in a query without having to actually create and populate the table on disk.\nNow here\u0026rsquo;s an example case:\nLet\u0026rsquo;s say we need to update the price rows on area tokyo at table meat with year 2019, 2020, and 2021. We can make the query like this:\nUPDATEmeatSETprice=new.priceFROM(values(6942,2019),(69420,2020),(69690,2021),)ASnew(price,year)WHEREmeat.areaILIKE\u0026#39;tokyo\u0026#39;ANDmeat.year=new.yearRETURNING*;You can change the new in new(price, year) with anything you like.\nAnother example case:\nLet\u0026rsquo;s say we need to update the city\u0026rsquo;s population on specific regencies. So we need to update data on population column in city table which has reference to regencies id. We can make the query like this:\nUPDATEcitySETpopulation=new.populationFROMregencies,(values(\u0026#39;city 1\u0026#39;,69420),(\u0026#39;city 2\u0026#39;,6969))ASnew(regency,population)WHEREregencies.id=city.regency_idANDregencies.regencyilikenew.regencyRETURNINGregencies.regency,city.population; The RETURNING clause is optional, it\u0026rsquo;s a way to check if we really changed the right rows or not. The ILIKE clause is an expression for insensitive case.\n Alright, that\u0026rsquo;s it. See you next time!\nReferences  Stackoverflow example . Postgresql update documentation . Postgresql values documentation .  "},{"ref":"https://bruhtus.github.io/posts/postgres-delete-using/","title":"Postgresql's USING Clause on DELETE Statement","section":"posts","tags":["Postgresql"],"date":"2023.06.01","body":"A Brief Intro The USING clause on DELETE statement is basically let us join multiple table and delete only the data from those join.\nIf you ever use mysql DELETE JOIN with INNER JOIN, then postgresql USING is kind of similar to that except it can only delete from one table by default.\nHow? How to do that? Here\u0026rsquo;s an example:\nLet\u0026rsquo;s say we have table A and table B which is referenced to table C by id. If we use SELECT statement on table C with INNER JOIN on table A, it would look like this:\nSELECT*FROMCINNERJOINAONA.C_id=C.id;Now, if we want to only delete all records from table C related to table A without deleting all records from table C related to table B, we can use DELETE with USING clause like this:\nDELETEFROMCUSINGAWHEREA.C_id=C.id; Please keep in mind that if you haven\u0026rsquo;t set the ON DELETE CASCADE for table A on reference C_id, that query will throw an error.\n Alright, that\u0026rsquo;s it. See you next time!\nReferences  Postgresql delete documentation . Postgresql delete using tutorial . MySQL delete join tutorial .  "},{"ref":"https://bruhtus.github.io/posts/docker-compose-down-without-yaml-file/","title":"Docker Compose Down Without Yaml File","section":"posts","tags":["Docker"],"date":"2023.03.22","body":"A Brief Intro Have you ever feel that you have too much docker container from spin up a bunch of docker-compose.yaml file? And when you want to take down all those docker container from the docker-compose.yml, you already delete the file?\nIf that\u0026rsquo;s the case, then this post might be for you.\nBefore we start, we will be using docker compose version 2.16.0 in this post. If you are using the older version of docker compose, then this method might not work. Please keep that in mind.\nCheck Docker Compose Project Name The first thing we need to do is to check the project name. We can do that with command:\ndocker-compose ls or if we use docker compose v2, we can use this command instead:\ndocker compose ls If the container already stopped, we can use flags -a or --all like this:\ndocker-compose ls -a or this:\ndocker compose ls -a By default, docker compose will use the docker-compose.yml directory name as project name. We can specify the project name with flag -p or --project-name like this:\ndocker-compose -p anu up -d or this:\ndocker compose -p anu up -d Please keep in mind that we need to use the flags -p or --project-name before the command such as up or down.\nTake Down Docker Compose Component After we know the project name, we can take down all docker component when we spin up the docker container from docker-compose.yml. The component in here means volume, network, or anything that we define inside docker-compose.yml.\nTo take down component from docker compose without the docker-compose.yml, we can use this command:\ndocker-compose -p project-name down \u0026lt;docker-compose-down-flags\u0026gt; or this:\ndocker compose -p project-name down \u0026lt;docker-compose-down-flags\u0026gt; For example, let\u0026rsquo;s say we want to take down docker component from project anu and also remove the volume from those project. We can do that with this command:\ndocker-compose -p anu down -v or this:\ndocker compose -p anu down -v Alright, that\u0026rsquo;s it!\nI am not sure since when the ls command appear in docker compose, so just to be safe i recommend using at least docker compose version 2.16.0.\nSide Note I\u0026rsquo;ve tried this method with docker compose version 1.18.0, but failed. After i downloaded the docker compose binary version 2.16.0 from github, i can use this method.\nStill not sure what was wrong. When i use command docker-compose --help, the flag -p in there. But when i actually spin up a new docker instance with command:\ndocker-compose -p project-name up -d service-name and then tried to take it down with command:\ndocker-compose -p project-name down -v i got an error docker-compose.yml not found or along those lines.\nIf someone know what\u0026rsquo;s going on, please let me know!\n"},{"ref":"https://bruhtus.github.io/posts/simple-guide-qmk-flashing-cli/","title":"Simple Guide Flashing Mechanical Keyboard with QMK CLI","section":"posts","tags":["Keyboard"],"date":"2023.02.26","body":"A Brief Intro So recently, around the time i wrote this post, i need to flash my mechanical keyboard. The harsh thing is that, the GUI QMK toolbox only available on windows and macOS, meanwhile i use linux.\nSo, i need to use the QMK CLI to flash my mechanical keyboard. This post is some kind of notes for my future self.\n\u0026ldquo;Humans live by forgetting\u0026rdquo;, some people said. Without further ado, let\u0026rsquo;s get started!\nInstall QMK Firmware First thing first, we need to check if QMK firmware already installed on our system. We can do that with the command below:\ncommand -v qmk If the output is empty, then we need to install QMK firmware first. If the output is not empty, we can skip this process.\nThere\u0026rsquo;s a newbie guide on how to install QMK firmware on linux, we can them out here .\nThere are a few options to install QMK firmware on linux. We can install QMK firmware with our linux distro package manager or from python pip.\nIn this post, we will use python pipx to install QMK firmware. We can install QMK firmware from python pipx with this command:\npipx install qmk  Make sure you already have pipx installed on your machine.\n Follow the instruction from the installation process, including installing dependencies on your system and copy the 50-qmk.rules to /etc/udev/rules.d/.\n Copying the 50-qmk.rules make QMK can detect the bootloader of our mechanical keyboard. If we didn\u0026rsquo;t do that, even if our mechanical keyboard already in bootloader mode, QMK won\u0026rsquo;t be able to detect our mechanical keyboard.\n After we install QMK firmware, we need to setup the QMK CLI. We can do that with this command:\nqmk setup By default, the installation directory will be on our home directory. Personally i don\u0026rsquo;t really want to put the QMK installation on my home directory, so i added -H flag to move the installation directory to somewhere else, like this:\nqmk setup -H all-repos/qmk_firmware If the setup already finished, try running:\nqmk doctor and see if there\u0026rsquo;s any issue with the installation.\nFlashing with QMK CLI There\u0026rsquo;s a newbie guide on how to flash our mechanical keyboard using QMK CLI, we can find it here .\nThe point is, we need to know if our mechanical keyboard supported by QMK. We can check the supported mechanical keyboard here .\nLet\u0026rsquo;s say our mechanical keyboard is dz60rgb with ansi layout version 1, we can flash our mechanical keyboard with this command:\nqmk flash -kb dztech/dz60rgb_ansi/v1 -km default  For more info about the flags, we can use qmk flash --help.\n Unfortunately, my current mechanical keyboard is not supported, which is synthesis60 version 2. I need to get the hex file from the mechanical keyboard designer (and fortunately he\u0026rsquo;s within reach).\n In case you also need the synthesis60 version 2 hex file, you can check it here .\n So, to use our own hex file rather than using the default that QMK supported, we can use this command:\nqmk flash \u0026lt;path-to-hex-file\u0026gt; Here\u0026rsquo;s an example:\nqmk flash ~/downloads/dyz_synthesis60_atmega_vial.hex All right, that\u0026rsquo;s it from me. See you next time!\nReferences  QMK newbie getting started guide . QMK newbie flashing guide . Using QMK flash with the external file .  "},{"ref":"https://bruhtus.github.io/posts/remove-specific-line-in-vim/","title":"Remove Specific Line in Vim","section":"posts","tags":["Vim"],"date":"2022.09.03","body":"A Brief Intro In this post, we will talk about how to remove specific line in vim. We will be using this javascript snippet code as an example:\nconst app = require(\u0026#39;./jest-example\u0026#39;); const math = require(\u0026#39;./math\u0026#39;); describe(\u0026#39;app operation\u0026#39;, () =\u0026gt; { const multiplyMock = jest.spyOn(math, \u0026#39;multiply\u0026#39;); multiplyMock.mockReturnValue(\u0026#39;itu\u0026#39;); test(\u0026#39;call math.add()\u0026#39;, () =\u0026gt; { const add = jest.spyOn(math, \u0026#39;add\u0026#39;); add.mockImplementation(() =\u0026gt; \u0026#39;anu\u0026#39;); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(\u0026#39;anu\u0026#39;); add.mockRestore(); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(3); }); test(\u0026#39;call math.subtract()\u0026#39;, () =\u0026gt; { const subtract = jest.spyOn(math, \u0026#39;subtract\u0026#39;); subtract.mockImplementation(() =\u0026gt; \u0026#39;nganu\u0026#39;); console.log(app.doSubtract(1, 2)); expect(app.doSubtract(1, 2)).toBe(\u0026#39;nganu\u0026#39;); }); test(\u0026#39;call math.subtract() again\u0026#39;, () =\u0026gt; { console.log(app.doSubtract(1, 2)); }); test(\u0026#39;call math.multiply()\u0026#39;, () =\u0026gt; { console.log(app.doMultiply(1, 2)); }); }); And the goal is to remove line that has console.log() in it. Let\u0026rsquo;s get started!\nGlobal Command The first solution is that, we can use global command to delete the line that has console.log() like this:\n:g/console.log/dPlease keep in mind that the term delete in vim, means cut. So, rather than deleting the content, vim will put that into the unnamed register. For more info about unnamed register, we can check on :help registers.\nSo, if we didn\u0026rsquo;t want to clutter our unnamed register, we can use black hole register (yup, that\u0026rsquo;s a thing) which actually delete rather than cut like this:\n:g/console.log/d_On a side note, we don\u0026rsquo;t need to use / as the separator between command and pattern, we can use most characters except \\, \u0026quot;, or |. If we use vim9 script, we can use # as a separator either, so please keep that in mind. For more info, we can check on :help pattern-delimiter.\nIn other words, we can write the previous global command, like this:\n:g;console.log;d_The global command that we just gone through will delete all the console.log in current file. If we want to specify the area of console.log we want to delete, we can add range to global command.\nLet\u0026rsquo;s take a look at the snippet we are using:\nconst app = require(\u0026#39;./jest-example\u0026#39;); const math = require(\u0026#39;./math\u0026#39;); describe(\u0026#39;app operation\u0026#39;, () =\u0026gt; { const multiplyMock = jest.spyOn(math, \u0026#39;multiply\u0026#39;); multiplyMock.mockReturnValue(\u0026#39;itu\u0026#39;); test(\u0026#39;call math.add()\u0026#39;, () =\u0026gt; { const add = jest.spyOn(math, \u0026#39;add\u0026#39;); add.mockImplementation(() =\u0026gt; \u0026#39;anu\u0026#39;); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(\u0026#39;anu\u0026#39;); add.mockRestore(); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(3); }); test(\u0026#39;call math.subtract()\u0026#39;, () =\u0026gt; { const subtract = jest.spyOn(math, \u0026#39;subtract\u0026#39;); subtract.mockImplementation(() =\u0026gt; \u0026#39;nganu\u0026#39;); console.log(app.doSubtract(1, 2)); expect(app.doSubtract(1, 2)).toBe(\u0026#39;nganu\u0026#39;); }); test(\u0026#39;call math.subtract() again\u0026#39;, () =\u0026gt; { console.log(app.doSubtract(1, 2)); }); test(\u0026#39;call math.multiply()\u0026#39;, () =\u0026gt; { console.log(app.doMultiply(1, 2)); }); }); And let\u0026rsquo;s say that our cursor in the line test('call math.subtract()', () =\u0026gt; { and we want to delete the console.log in test('call math.add()', () =\u0026gt; { which is before the line we\u0026rsquo;re currently in.\nWith that in mind, we can do something like this:\n:?test?;/});/g;console.log;d_Here\u0026rsquo;s the breakdown of those command:\n ?test? will search word test on previous line. ; after ?test? is a special offset which tell vim that we will use another search command. For more info, we can check :help //;. /});/ will search after the result of previous search. g;console.log;d_ is a global command to delete console.log in the scope of previous search.  Substitute Command The downside of global command is that there\u0026rsquo;s no confirm option, so we might delete something we don\u0026rsquo;t want to. If we want a confirm option, we can use substitute command.\nIf we want to delete all the console.log in current file, we can do it like this:\n:%s;.*console.log.*\\n;;cHere\u0026rsquo;s the breakdown of those command:\n % symbol to tell vim that the range is the current file or all the line in the file. s is the abbreviation of substitute command. .*console.log.*\\n is the pattern that select the line that has console.log in it, which include the newline character at the end. ;; is basically telling vim to replace it with nothing. c is a confirm option.  We can also combine substitute command with global command, like this:\n:g;console.log;s;.*\\n;;cConclusion There\u0026rsquo;s a lot of possibility with global and substitute command and we won\u0026rsquo;t be able to cover all of those in one blog post. So, i will leave it to you to explore those possibility.\nYou might want to learn regular expression to enhance the global and substitute command experience.\nAlright, that\u0026rsquo;s it for this post. Thank you for reading!\n"},{"ref":"https://bruhtus.github.io/posts/clean-up-untracked-file-in-git-repo/","title":"Clean Up Untracked File in Git Repo","section":"posts","tags":["Shell","Git"],"date":"2022.08.20","body":"Have you ever feels like deleting all the untracked file or directory in git repository is such a pain?\nIf you do, then this post might be for you!\nOk, first thing first, there is a git command clean which help us delete the untracked file. We can even use it with interactive interface. To invoke the interactive interface, we can use this command:\ngit clean -i . What that command does is prompt us an interactive interface with all the untracked file in current directory.\nTo make it start from the root of git repository, we can use this command:\ngit clean -i $(git rev-parse --show-toplevel) The git rev-parse --show-toplevel will get us the path of current git repository root. If you don\u0026rsquo;t know what i mean by git repository root, we can think of it as the directory or path that has .git directory.\nBut, the clean command has a downside. The clean command does not let us select specific file under untracked directory. It will only let us delete the entire untracked directory with all the file in it.\nTo get around this issue, we can make a shell alias like this:\nalias gurm=\u0026#39;git ls-files --others --exclude-standard | fzf --multi | xargs -r rm -v\u0026#39;  You can change gurm to anything you want.\n The dependencies from shell alias above are:\n git fzf xargs  git ls-files --others --exclude-standard will list all the untracked file and also exclude the file from .gitignore.\nfzf is a fuzzy finder to select the untracked file we want to remove, we also give flags --multi so that we can select multiple file with tab key.\nFinally, we execute rm command with xargs -r. What xargs -r do is make sure that we have some input from standard input (stdin), if there is no input from stdin, xargs will not execute the command. It is useful to prevent an error from a command that require an argument.\nAlright, that\u0026rsquo;s all. Have a nice day!\n"},{"ref":"https://bruhtus.github.io/posts/awk-print-row-instead-of-column/","title":"Awk Print Row Instead of Column","section":"posts","tags":["Awk"],"date":"2022.06.18","body":"Main Course Let\u0026rsquo;s say we have an output from xrandr --listactivemonitor like this:\nMonitors: 2 0: +*eDP-1 1920/344x1080/194+1920+0 eDP-1 1: +HDMI-2 1920/480x1080/270+0+0 HDMI-2 Now, we want to display only the second row which is this line:\n0: +*eDP-1 1920/344x1080/194+1920+0 eDP-1 We can do that using awk with this command:\nxrandr --listactivemonitor | awk \u0026#39;NR==2\u0026#39; or using process substitution command, like this:\nawk \u0026#39;NR==2\u0026#39; \u0026lt;(xrandr --listactivemonitor) If we also want to limit the row and the column, let\u0026rsquo;s say the second row and the third column which is result in this:\n1920/344x1080/194+1920+0 We can do that with this command:\nxrandr --listactivemonitor | awk \u0026#39;NR==2 {print $3}\u0026#39; or using process substitution command, like this:\nawk \u0026#39;NR==2 {print $3}\u0026#39; \u0026lt;(xrandr --listactivemonitor) Now, if we want to display the second to last row, which is this line:\n0: +*eDP-1 1920/344x1080/194+1920+0 eDP-1 1: +HDMI-2 1920/480x1080/270+0+0 HDMI-2 We can do that with this command:\nxrandr --listactivemonitor | awk \u0026#39;NR\u0026gt;=2\u0026#39; or using process substitution command, like this:\nawk \u0026#39;NR\u0026gt;=2\u0026#39; \u0026lt;(xrandr --listactivemonitor) Alright, that\u0026rsquo;s all. Thanks for reading and happy shell scripting!\nReference  Stackoverflow answer about awk for specific row .  "},{"ref":"https://bruhtus.github.io/posts/shell-process-substitution-as-temp-file/","title":"Shell Process Substitution as Temp File","section":"posts","tags":["Shell"],"date":"2022.06.11","body":"A Brief Intro This is all started when i\u0026rsquo;m trying to make a shell alias using fzf and git add -p command. And i was surprised that the interactive selection from git add -p immediately terminated before i can even press any key. Let\u0026rsquo;s get started, shall we?\nFirst Attempt My first attempt to make that shell alias was something like this:\nalias gap=\u0026#39;git status -s | awk \u0026#34;{print \\$2}\u0026#34; | fzf | xargs -r git add -p\u0026#39; Let me briefly explain one by one the command that i use in my shell alias:\n  git status -s -\u0026gt; The short format of git status, without unnecessary info (at least for me).\n  awk \u0026quot;{print \\$2}\u0026quot; -\u0026gt; Only use the second column. The backslash is to prevent shell to expand $2 into a second argument instead of a second column in awk.\n  fzf -\u0026gt; Fuzzy finder by Junegunn1.\n  xargs -r -\u0026gt; Do not run the command if there\u0026rsquo;s no standard input.\n  git add -p -\u0026gt; To add a chunk of changes, only works if the file already tracked by git.\n  This alias is where the problem arise. After i select the file name using fzf, the interactive interface of git add -p only appear for a few seconds and then terminated.\nAccording to stackoverflow answer2:\n Without further arguments xargs does not work with interactive (command line) applications.\nThe reason for that is, by default xargs gets its input from stdin but interactive applications also expect input from stdin.\nTo prevent the applications from grabbing input that is intended for xargs, xargs redirects stdin from /dev/null for the applications it runs.\nThis leads to the application just receiving an EOF3.\n With that in mind, we need to use --arg-file=\u0026lt;file\u0026gt; or -a \u0026lt;file\u0026gt; flag, which means that xargs will read from a \u0026lt;file\u0026gt; instead of stdin ( standard input) so that the stdin remains unchanged. Alright, let\u0026rsquo;s go to the second attempt.\nSecond Attempt Following the stackoverflow answer2, my second attempt was something like this:\nalias gap=\u0026#39;xargs -a \u0026lt;(git status -s | awk \u0026#34;{print \\$2}\u0026#34; | fzf) git add -p\u0026#39; The expression \u0026lt;(git status -s | awk \u0026quot;{print \\$2}\u0026quot; | fzf) is what we called process substitution (or to be precise, shell process substitution).\nInstead of using a file, we use a command to act like a temporary file.\nNow, the problem is, in my experiment i can\u0026rsquo;t use fzf with the shell process substitution because shell process substitution produces a special file that can only be opened and read, but not written or seeked4.\nCommands that treat their arguments as pure streams will works with shell process substitution, but the commands that seek a file they are given (or write to a file) won\u0026rsquo;t work4.\nAnd that is why we can\u0026rsquo;t use interactive command such as fzf with shell process substitution.\nConclusion With all the information from my experiment before, i decided to remove fzf from the alias, which result with this alias:\nalias gap=\u0026#39;xargs -a \u0026lt;(git status -s | awk \u0026#34;{print \\$2}\u0026#34;) git add -p\u0026#39; Is it fulfilling what i need? Not really, i might replace that with shell function instead of shell alias soon. But, i learn something new about shell process substitution.\nExtra Note For more info about the xargs and git flags, check the manpage. For example: man xargs or man git-add.\nIf we use echo with shell process substitution like this:\necho \u0026lt;(git status -s) That will show us where the temporary file created.\n  Fuzzy finder by Junegunn .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Stackoverflow: xargs explanation .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Triggering EOF explanation .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Process substitution result in special file .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"ref":"https://bruhtus.github.io/posts/replacement-for-info-command/","title":"Replacement For Info Command","section":"posts","tags":["Shell"],"date":"2022.06.11","body":"A Brief Intro Sometime ago, someone posted a tweet about a replacement for rm -rf $HOME/directory that is mv $HOME/directory /dev/null. When i took a look of it, using the ls -l /dev/null command, i found something like this:\ncrw-rw-rw- 1 root root 1, 3 Jun 11 04:21 /dev/null Now, what is c in the crw below\ncrw-rw-rw- 1 root root 1, 3 Jun 11 04:21 /dev/null After going around on internet, i found an answer in stackexchange1. Which explain that c stands for character special file. Please keep in mind that everything in unix-like system is a file, even a directory (or some people called it folder) is a file with type directory.\n The file we usually use has a type of regular file.\n In the stackexchange, someone mentioned about info ls to show the file type with their respective character symbol. That\u0026rsquo;s what pique my interest, the info command.\nReplacement For info Command When i use the command info ls, i noticed that i can\u0026rsquo;t use j and k for navigation, which is a nightmare.\nSo, the first thing that i need to figure out is, how to change the pager used in info command. According to this stackexchange answer2, \u0026ldquo;info doesn\u0026rsquo;t use separate pager because it handles navigation\u0026rdquo;. So, basically there\u0026rsquo;s no hope with info command? Probably.\nAnd then, the person who answer on the stackexchange also give a suggestion about pinfo which, at least use j and k as down or up movement. Now i need to read the manpage about pinfo to configure it.\nWhat\u0026rsquo;s Next? Other than trying to configure pinfo, i might need to figure out what is the info documents is all about. This might be a new kind of documentation other than manpage that i can use (if the developer support it). Also, figure out about the character special file type.\n  Stackexchange: The meaning of c in crw-rw-rw- \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Stackexchange: Replacement for info command \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"ref":"https://bruhtus.github.io/posts/vim-statusline/","title":"Guide to Make Your Own Vim/Neovim Statusline","section":"posts","tags":["Vim"],"date":"2021.08.20","body":" We don\u0026rsquo;t have to install vim plugin to get a statusline if we don\u0026rsquo;t want to.\n Introduction Hi everyone! In this post I will talk about making your custom statusline in vim. There are a lot of plugins out there that makes vim statusline way better and works out of the box.\n\u0026ldquo;Why would someone going through all the trouble while there\u0026rsquo;s a plugin for that?\u0026rdquo;, you might ask. Well, for me personally, having one less plugin is a good thing. I don\u0026rsquo;t really want to depend on the plugin for something simple such as statusline. If I can build it myself, then I will build it myself rather than using a plugin. Also, it save me a lot of time to figure out which plugin causes the issue if I have less plugin! Oh, another thing, some vim statusline plugin can slow down your startup time (I\u0026rsquo;m looking at you vim-airline) so be careful about that. Alright then, let\u0026rsquo;s get into it!\nRequirements Before we start, we need to prepare a few things:\n Vim/Neovim (we will use vim script or VimL in this post) set laststatus=2 (always display the statusline) Patience (don\u0026rsquo;t be scared when you see an error, calm down!)  If all is set, then let\u0026rsquo;s get started!\nDisable Old Statusline Before we move further, you need to disable or remove your old statusline plugin and the config. If you don\u0026rsquo;t want to remove your old statusline plugin config, you can commented out those line/config in your vimrc or init.vim.\nDifferent statusline for active and inactive window You can have different statusline for active and inactive window by using autocmd event. \u0026ldquo;What is autocmd?\u0026rdquo; you might ask, go take a look at :help autocmd for more info. To make it simple, autocmd is to automatically execute a command on certain event which you can check on :help autocmd-events for more info.\nSo, how can we have different statusline for active and inactive window? First, we need to make a function to define our active or inactive statusline component. You can create the function similar to this:\n\u0026#34; component for active windowfunction! StatuslineActive()\u0026#34; the component goes hereendfunction\u0026#34; component for inactive windowfunction! StatuslineInactive()\u0026#34; the component goes hereendfunction\u0026#34; load statusline using `autocmd` event with this functionfunction! StatuslineLoad(mode) if a:mode ==# \u0026#39;active\u0026#39;\u0026#34; to make it simple, %! is to evaluate the current changes in the window\u0026#34; it can be useful for evaluate current mode in statusline. For more info:\u0026#34; :help statusline. setlocal statusline=%!StatuslineActive() else setlocal statusline=%!StatuslineInactive() endifendfunctionand also the autocmd similar to this:\n\u0026#34; so that autocmd didn\u0026#39;t stack up and slow down vimaugroup statusline_startup autocmd!\u0026#34; for more info :help WinEnter and :help BufWinEnter autocmd WinEnter,BufWinEnter * call StatuslineLoad(\u0026#39;active\u0026#39;) autocmd WinLeave * call StatuslineLoad(\u0026#39;inactive\u0026#39;)augroup ENDNow we can compose our statusline component. We can take a look at :help statusline for supported items like for example f for relative path to the file in the buffer. You can choose whatever item you like in your statusline, and put it in the previous function similar to this:\nfunction! StatuslineActive()\u0026#34; if we want to add `f` items in our statusline let l:filename = \u0026#39;%f\u0026#39;\u0026#34; if we want to add \u0026#39;m\u0026#39; items in our statusline let l:mod = \u0026#39;%m\u0026#39;\u0026#34; the `.` is basically to ignore whitespace before and put it right after the previous component return l:filename.l:modendfunctionWhy we need to do that? well, I\u0026rsquo;ll explain it in next section\nCurrent Mode in Statusline If you want to put your current mode in your statusline, you can do it with a function similar to this:\nfunction! StatuslineMode() abort let l:currentmode={ \\ \u0026#39;n\u0026#39;: \u0026#39;N\u0026#39;, \\ \u0026#39;v\u0026#39;: \u0026#39;V\u0026#39;, \\ \u0026#39;V\u0026#39;: \u0026#39;VL\u0026#39;, \\ \u0026#39;^V\u0026#39;: \u0026#39;VB\u0026#39;, \\ \u0026#39;s\u0026#39;: \u0026#39;S\u0026#39;, \\ \u0026#39;S\u0026#39;: \u0026#39;SL\u0026#39;, \\ \u0026#39;^S\u0026#39;: \u0026#39;SB\u0026#39;, \\ \u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;, \\ \u0026#39;R\u0026#39;: \u0026#39;R\u0026#39;, \\ \u0026#39;c\u0026#39;: \u0026#39;C\u0026#39;, \\ \u0026#39;t\u0026#39;: \u0026#39;T\u0026#39;} let l:modecurrent = mode()\u0026#34; use get() -\u0026gt; fails safely, since ^V doesn\u0026#39;t seem to register\u0026#34; 3rd arg is used when return of mode() == 0, which is case with ^V\u0026#34; thus, ^V fails -\u0026gt; returns 0 -\u0026gt; replaced with \u0026#39;VB\u0026#39; let l:modelist = toupper(get(l:currentmode, l:modecurrent, \u0026#39;VB\u0026#39;)) let l:current_status_mode = l:modelist return l:current_status_modeendfunctionand put it inside of your statusline function like this:\nfunction! StatuslineActive() let l:filename = \u0026#39;%f\u0026#39; let l:mod = \u0026#39;%m\u0026#39;\u0026#34; `w:` is basically local variable to current window\u0026#34; and `l:` is basically local variable to function. For more info :help E121 let w:mode = \u0026#39;%{StatuslineMode()}\u0026#39; return w:mode.l:filename.l:modendfunctionNow, if we want to change the current mode background based on the current mode, we can do something like this:\n\u0026#34; define Normal mode color, Insert mode color, and so onhi NormalModeColor ctermbg=... ctermfg=... guifg=#... guibg=#...hi InsertModeColor ctermbg=... ctermfg=... guifg=#... guibg=#...function! StatuslineActive() let l:filename = \u0026#39;%f\u0026#39; let l:mod = \u0026#39;%m\u0026#39; if mode() ==# \u0026#39;n\u0026#39; let w:mode = \u0026#39;%#NormalModeColor#%{StatuslineMode()}\u0026#39; elseif mode() ==# v:insertmode let w:mode = \u0026#39;%#InsertModeColor#%{StatuslineMode()}\u0026#39; endif\u0026#34; %* is basically to restore highlight to StatusLine highlight group return w:mode.\u0026#39;%* \u0026#39;.l:filename.l:modendfunctionGit Branch in Statusline If you install vim-fugitive plugin, then you can use fugitive#head() in your statusline like this:\nfunction! StatuslineActive() let l:filename = \u0026#39;%f\u0026#39; let l:mod = \u0026#39;%m\u0026#39; if mode() ==# \u0026#39;n\u0026#39; let w:mode = \u0026#39;%#NormalModeColor#%{StatuslineMode()}\u0026#39; elseif mode() ==# v:insertmode let w:mode = \u0026#39;%#InsertModeColor#%{StatuslineMode()}\u0026#39; endif\u0026#34; make sure it doesn\u0026#39;t throw an error if `vim-fugitive` is not installed let l:git = \u0026#34;%{exists(\u0026#39;*FugitiveHead\u0026#39;) ? fugitive#head() : \u0026#39;\u0026#39;}\u0026#34;\u0026#34; to separate left and right side let l:sep = \u0026#39;%=\u0026#39; return w:mode.\u0026#39;%* \u0026#39;.l:filename.l:mod.l:sep.l:gitendfunctionalternatively, you can use system() command to get the current git branch (for more info :help system()) like this:\nfunction! StatuslineActive() let l:filename = \u0026#39;%f\u0026#39; let l:mod = \u0026#39;%m\u0026#39; if mode() ==# \u0026#39;n\u0026#39; let w:mode = \u0026#39;%#NormalModeColor#%{StatuslineMode()}\u0026#39; elseif mode() ==# v:insertmode let w:mode = \u0026#39;%#InsertModeColor#%{StatuslineMode()}\u0026#39; endif\u0026#34; for more info :help E121 let g:gitbranchcmd = \u0026#34;git branch --show-current 2\u0026gt;/dev/null | tr -d \u0026#39;\\n\u0026#39;\u0026#34;\u0026#34; use system() if vim-fugitive not installed let l:git = \u0026#34;%{exists(\u0026#39;*FugitiveHead\u0026#39;) ? fugitive#head() : system(g:gitbranchcmd)}\u0026#34; let l:sep = \u0026#39;%=\u0026#39; return w:mode.\u0026#39;%* \u0026#39;.l:filename.l:mod.l:sep.l:gitendfunctionSame Statusline for active and inactive window Now, if we want our statusline to be the same whether in active or inactive window. We can make simplify it by only make one function and not using autocmd. It will look something like this:\nfunction! StatuslineComponent()\u0026#34; your component goes hereendfunctionset statusline=%!StatuslineComponent()and you can use some tips from previous section too!\nConclusion This guide is for do-it-yourself kind of people, so it\u0026rsquo;s only giving some pointer you can use to make your own statusline. I don\u0026rsquo;t want to tell you what to put in your statusline, it is your OWN statusline after all, so you need to know what you want in it. Also, I\u0026rsquo;m not a vim script expert so please forgive me if I miss something. If you have any question regarding this post, feel free to hit me up on twitter (@diawanchris)! See you later!\nReferences  Kade Killary blog post . Junegunn\u0026rsquo;s statusline .  "},{"ref":"https://bruhtus.github.io/posts/ssh-google-colab/","title":"Access Google Colab Through SSH and SSHFS","section":"posts","tags":["Linux"],"date":"2021.06.22","body":" Have you ever want a terminal emulator in google colab instead of jupyter notebook? Well, here it is bois!\n Skip-able Part First thing first, i prefer to use vim/neovim instead of jupyter notebook and that\u0026rsquo;s why i prefer to use terminal or terminal emulator to do things as much as possible. I know jupyter notebook has it\u0026rsquo;s own vim emulation, but it\u0026rsquo;s not as good as the OG vim/neovim. Also, i know you can have neovim inside your browser using firenvim plugin , but i\u0026rsquo;m not sure how useful it is and do i really want that? i mean, my IDE is a terminal or terminal emulator. So, unless it\u0026rsquo;s a terminal or terminal emulator then i won\u0026rsquo;t feel comfortable using it.\nThat\u0026rsquo;s why i\u0026rsquo;m trying to figure out how to access google colab through terminal emulator, and it turns out i can do that!\n As far as i know, google colab support terminal emulator but only for pro user. I haven\u0026rsquo;t try it yet, so i might be wrong.\n Requirements To make this work, please make sure that SSH and SSHFS installed on your system. You can install both of them using your distro package manager or something similar (there are a lot of tutorial on how to install SSH or SSHFS).\nA Brief Intro to SSH According to wikipedia , Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network. Typical applications include remote command-line login and remote command execution, but any network service can be secured with SSH.\nTo make it simple, SSH let you access another computer/server and you can use command line interface (CLI) to interact with those computer/server.\nA Brief Intro to SSHFS According to the project github repo , SSHFS allows you to mount a remote filesystem using SFTP.\nTo make it simple, SSHFS let you mount the computer/server storage to your local system and you can treat it like the usual storage on your local system, like using mv and cp command.\nAccess Google Colab Using SSH First thing first, you need to have ngrok account. If you doesn\u0026rsquo;t have ngrok account, then you can register here .\nAfter that, you need to make a new google colab notebook. And then, paste the code below:\n#CODE #Generate root password import random, string password = \u0026#39;\u0026#39;.join(random.choice(string.ascii_letters + string.digits) for i in range(20)) #Download ngrok ! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ! unzip -qq -n ngrok-stable-linux-amd64.zip #Setup sshd ! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen \u0026gt; /dev/null #Set root password ! echo root:$password | chpasswd ! mkdir -p /var/run/sshd ! echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config ! echo \u0026#34;PasswordAuthentication yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config ! echo \u0026#34;LD_LIBRARY_PATH=/usr/lib64-nvidia\u0026#34; \u0026gt;\u0026gt; /root/.bashrc ! echo \u0026#34;export LD_LIBRARY_PATH\u0026#34; \u0026gt;\u0026gt; /root/.bashrc #Run sshd get_ipython().system_raw(\u0026#39;/usr/sbin/sshd -D \u0026amp;\u0026#39;) #Ask token print(\u0026#34;Copy authtoken from https://dashboard.ngrok.com/auth\u0026#34;) import getpass authtoken = getpass.getpass() #Create tunnel get_ipython().system_raw(\u0026#39;./ngrok authtoken $authtoken \u0026amp;\u0026amp; ./ngrok tcp 22 \u0026amp;\u0026#39;) #Print root password print(\u0026#34;Root password: {}\u0026#34;.format(password)) #Get public address ! curl -s http://localhost:4040/api/tunnels | python3 -c \\ \t\u0026#34;import sys, json; print(json.load(sys.stdin)[\u0026#39;tunnels\u0026#39;][0][\u0026#39;public_url\u0026#39;])\u0026#34; What that code does is basically create a connection from google colab back-end with ngrok so that we can access the google colab back-end using SSH and SSHFS. You can check the github gist for more detail.\nAfter that, you can decide whether to mount your google drive to google colab to or not. If you decide to mount your google drive to google colab, then later on you can mount your google drive using SSHFS.\nAfter you setup all of that, run the code. After you run the code, it gonna request you to copy your ngrok authtoken from ngrok dashboard . If there\u0026rsquo;s an error after you input ngrok authtoken, you can just re-run the code.\nAfter that, you can see the Root password and something similar to tcp://n.tcp.ngrok.io:xxxxx (which n and x represent any number from 0-9)\nNext, you can access the google colab using SSH with the following command:\nssh -p xxxxx root@n.tcp.ngrok.io If there\u0026rsquo;s a question \u0026ldquo;Are you sure you want to continue connecting?\u0026rdquo;, answer yes. That was to save the SSH address so that you can connect to it again in the future. But, if you use google google colab then you need to make new SSH address because each SSH session restart when google colab restart. That is one of the downside for this method.\nAccess Google Drive in Google Colab If you mount google drive before you activate google colab, then you can access google drive too. You can access google drive in google colab using SSHFS.\nIf you can already access google colab back-end using SSH, then all you need too do is to run this command:\nsshfs -p xxxxx root@n.tcp.ngrok.io:/content/drive/MyDrive /mnt/colab The command is almost the same as SSH command from before, the difference is that you need to specify which directory you want to mount on the google colab (in above example is google drive which has path /content/drive/MyDrive) and which directory you want to mount it on the local system (in above example is on /mnt/colab).\nYou can choose to mount google drive on any other directory you want, even on your home directory. I usually mount it on /mnt because it\u0026rsquo;s already a habit of mine to mount an external drive to either /mnt or /run/media. And also, make sure the directory you want to mount your drive into is already exist. I already create colab directory beforehand, so i can mount the google drive into /mnt/colab.\nUpside and Downside There\u0026rsquo;s always an upside and downside of things, and this method is also the same. Below is a few upside and downside that i notice when using this method.\nThe Upside  You can use it like the usual terminal or terminal emulator, with familiar command and vim/neovim in it. If you use SSHFS, you can update your script directly from your terminal emulator to google colab through google drive. Basically you can update your script in google drive directly from your terminal emulator.  The Downside  SSH connection with ngrok only last around 40 minutes, so if you need a long time to run your code then you should use the google colab front-end instead. Use SSH and SSHFS to update script instead of running the script. If you have slow internet connection, then it\u0026rsquo;s gonna be a bit of lag when you interact with google colab back-end CLI. SSH session restart when the google colab restart, and SSH session only last for around 40 minutes so you might need to start over again.  Conclusion When you see that the method has a lot more downside rather than upside, you might be thinking \u0026ldquo;why should i use this method?\u0026rdquo;. Well, only use this method if you feel more comfortable using terminal emulator rather than jupyter notebook, other than that, it\u0026rsquo;s not really worth your time.\nUse this method while it lasts, as far as i know, there was a time when this method doesn\u0026rsquo;t work. At the time of writing this post, this method still works but i\u0026rsquo;m not sure if google gonna let this happen for a long time. Anyway, updating the script through SSHFS is really convenient for me.\nReferences  Google colab SSH github gist . How to SSH into google colab medium post . Access your google drive using ssh medium post . Distrotube youtube video on SSHFS .  "},{"ref":"https://bruhtus.github.io/posts/pyv/","title":"Pyv: Minimalist Python Venv Management Tool","section":"posts","tags":["Shell","Python"],"date":"2021.04.16","body":" This is a continuation from my previous post (you can check it here ). Long story short, this is a minimalist way to manage your python virtual environment. All you need is python and git.\n What Is Pyv? Here\u0026rsquo;s a brief intro to what is pyv:\n Pyv is a simple shell function that let you manage python virtual environment that decoupled from project directory.\n To put it simply, pyv move the virtual environment to $PYV_dir which by default is $HOME/.cache/pyv\nHow Pyv Manage Virtual Environment Pyv can create, remove, activate, deactivate, and list python virtual environment. Below is the explanation for each action that pyv can do.\nCreate Virtual Environment Pyv create python virtual environment using python default command python -m venv to $PYV_DIR/{given-name}. The given-name can be from git repo name or user input.\nIf the user give pyv command an argument then pyv gonna create python virtual environment with that argument name. For example:\npce something-big With the command above, pyv gonna create python virtual environment with the name something-big. pce is pyv command to to create python virtual environment (why would i made something with long command?).\nPyv also can create python virtual environment with git repo name. If you give no argument and just enter pce in a working git repo then pyv gonna create python virtual environment using the git repo name.\n\u0026ldquo;How does pyv do that?\u0026rdquo; you might ask, well, pyv use the command git rev-parse --show-toplevel to get the git root directory name (with a lot of trimming of course). For those who don\u0026rsquo;t know what git root directory, to make simple, git root directory is the directory where you first use git init command or the directory that have .git directory.\nPlease do remember tho, you can only use pce without any argument in git working directory and not in git bare directory. What i mean by git working directory is the normal git repo that has .git directory in it.\nHere\u0026rsquo;s an example of pce without an argument (in case you\u0026rsquo;re still confused):\npce Yup, only that. Simple right?\nRemove Virtual Environment After creating virtual environment, how do you delete the virtual environment?\nYou can remove the virtual environment using pre command with or without argument similar to creating virtual environment.\nFor example, we already create something-big python virtual environment. And now we want to remove those virtual environment. All we need to do is something like this:\npre something-big Those command gonna invoke the rm command to remove something-big virtual environment directory in $PYV_DIR. So the pre command depends on how you setup your rm command in your shell.\nIf you create virtual environment using the git repo name, and you want to remove those environment, you can use pre without an argument inside those git repo directory (assumming you haven\u0026rsquo;t changed the directory name). How to use the command is the same as when you create the virtual environment, just type\npre and you\u0026rsquo;re done.\nActivate Virtual Environment Ok, now you know how to create and remove the virtual environment using pyv. Now, how do you activate those virtual environment?\nBecause we use default python python -m venv command, we need to know how the activate the virtual environment created using those command.\nAccording to python venv documentation , we need to source the activate file in \u0026lt;venv-dir\u0026gt;/bin/activate.\n Did you know that the activate file is also a shell function? Now you know.\n So, the pyv command to activate the virtual environment is pae. Similar to previous command, you can use pae with or without an argument. pae command basically to source activate file in virtual environment directory that located in $PYV_DIR.\nLike previous command, let\u0026rsquo;s say we have something-big virtual env and we want to activate those command. All we need to do is\npae something-big or if you create a virtual environment using git repo name (assuming you didn\u0026rsquo;t change the git repo directory name), then you can just use\npae without any argument.\nDeactivate Virtual Environment After you know how to create, remove, and activate virtual environment with pyv. Now it\u0026rsquo;s time for you to know how to deactivate virtual environment.\nYou can deactivate the virtual environment either using default deactivate command or using pde. pde is just an alias for deactivate command provided by python venv. You do you.\nPlease keep in mind that you don\u0026rsquo;t need any argument to deactivate virtual environment.\nList Virtual Environment The last thing is how to list all the virtual environment that available?\nTo list all the virtual environment that ever created, you can use pve command without an argument or ls -l $PYV_DIR. pve is just an alias for ls -l $PYV_DIR.\nConclusions There\u0026rsquo;s always an upside/downside to a project, and this project is no exception.\nThe upside is that, it\u0026rsquo;s minimal. If you didn\u0026rsquo;t do anything fancy with your python virtual environment then pyv probably gonna fit your need.\nThe downside is also \u0026ldquo;it\u0026rsquo;s minimal\u0026rdquo;. If you use default options that come with default python venv command, then you can\u0026rsquo;t do that with pyv. Pyv only able to handle python -m venv \u0026lt;env-directory\u0026gt;. That\u0026rsquo;s all.\n At the time of writing this article, i\u0026rsquo;ve only tested pyv in bash and zsh. I might provide support for fish, csh/tcsh in the future.\n References  A few alternative to manage python virtual environment . Python venv documentation .  "},{"ref":"https://bruhtus.github.io/posts/split-up-vimrc/","title":"Split Up Vimrc","section":"posts","tags":["Vim"],"date":"2021.03.22","body":" If you\u0026rsquo;re the type of person who like to place all your source code in one file, then this article is not for you. But, if you\u0026rsquo;re the type of person who like to split up your source code into a few sub-modules, then this article is for you.\n Skip-able Part When i look at my vimrc (vimrc is a vim config file for those who don\u0026rsquo;t know), i always feel confused where should i add new configuration for new plugin. I want to organize my vimrc so that it is easier to maintain and adding stuff, and that\u0026rsquo;s where the problem comes in.\nOvertime, when you keep adding configuration to vim, slowly your vimrc gonna become a huge mess and at some point it gonna feels cumbersome to access your vimrc. And that\u0026rsquo;s why i thought \u0026ldquo;can i split up my vimrc so that i don\u0026rsquo;t feel overwhelm every time i want to add new configuration or mapping to vim?\u0026rdquo; and it turns out i can.\nUpside and Downside Before we move on, you need to know the upside and downside for split up your vimrc that i\u0026rsquo;ve found. Here we go.\nUpside  Easier to manage, you can have a dedicated config file for every plugin you have Doesn\u0026rsquo;t overwhelm you with the piled up line of text, because it\u0026rsquo;s on separate file You can take a part of your config such as defaults setting and mappings, rather than a whole vim config (useful for accessing server with ssh)  Downside  You need to use grep or something similar if you want to check if the mapping already exist or not. If you didn\u0026rsquo;t split up your vimrc, you can just use vim built-in search function. You need to download a lot of file if you want to have full experience of your vim.  Split Up Vimrc If you still want to go on despite the downside, then it\u0026rsquo;s time to split up your vimrc.\nFirst thing first, you need to know that vim has runtime path which gonna be loaded everytime you start vim. And we can use the default runtime path to our advantage. For the full list, you can check here , but to make it simple, we\u0026rsquo;re only gonna use ~/.vim/plugin (for vanilla vim) or ~/.config/nvim/plugin (for neovim) directory. Every file in those directory get loaded every time you open vim, so you can add your config in those directory.\nFor example, you can move your defaults config such as set number relativenumber into file defaults.vim and place it in ~/.vim/plugin (for vanilla vim) or ~/.config/nvim/plugin (for neovim) and defaults config such as number and relativenumber gonna get loaded everytime you open vim.\nYou can also make config file for your plugin and give it the plugin name such as fugitive.vim and place it into ~/.vim/plugin (for vanilla vim) or ~/.config/nvim/plugin.\nThe Conclusion It\u0026rsquo;s quite easy to manage vim if you split up your vimrc, but it comes back to your personal preference. This article is just to remind vim user that they can split up their vimrc if they want to.\nReferences  Manage plugin in dark ages .  "},{"ref":"https://bruhtus.github.io/posts/python-venv/","title":"Managing Python Virtual Environment","section":"posts","tags":["Python"],"date":"2021.03.02","body":" Have you ever want python virtual environment that decoupled from the project directory like conda but not actually conda (dejavu)? That\u0026rsquo;s what this post is about, a simple way to manage a python virtual environment similar to how conda manage virtual environment without python version management.\n A Brief Intro to Python Virtual Environment To makes thing simpler, python virtual environment is a self-contained directory tree that contains a python installation for a particular version of python plus a number of additional packages.\nWith virtual environment you can minimize the conflicting requirements for each python script you made. For example, application A can have its own virtual environment with python package at version 1.0 installed and application B can have its own virtual environment with python package at version 2.0. If applicatin B need to upgrade the python package to version 3.0 then this will not affect application A\u0026rsquo;s environment with python package at version 1.0.\nLet\u0026rsquo;s Get Started In this post we\u0026rsquo;re only going to use the default python venv to create a virtual environment, the command is something like this:\npython -m venv \u0026lt;directory-name\u0026gt; In case you\u0026rsquo;re wondering, \u0026ldquo;if we only gonna use the default python command, then what\u0026rsquo;s so special about it?\u0026rdquo;. Let me tell you this, what special about the default python venv command is that we can specify the path of the directory and we can enhance that with a shell script. Please keep in mind that this is a minimalist approach to manage python virtual environment without installing other tools except python and git (we\u0026rsquo;ll get to that later).\nCreate Python Virtual Environment Here\u0026rsquo;s how we make a simple shell script to manage our python environment. First thing first, you should decide where you want all your virtual environment located. If you use miniconda, usually it\u0026rsquo;s in miniconda3/envs/ directory or something along those path, i forgot. I personally want to place my python virtual environment in .cache/python-venv directory because i rarely check my .cache directory and the virtual environment not gonna disturb other shell scripts i have.\nAfter that, we decide whether we give the name to virtual environment ourself or just use git repo root name. Like i told you before, we can use git repo to decide the name of our virtual environment. To make things simpler, what i mean git repo root is the directory where you use command git init to initialize git repo.\nFor example, if you use git init command in nganu directory then you can have your virtual environment named nganu without you enter any name, but if you didn\u0026rsquo;t want to use your git repo root name then you can also insert the name you want, similar to conda create -n \u0026lt;name-env\u0026gt;. With that brief intro, here\u0026rsquo;s the code:\n#!/bin/sh  gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; projectdir=\u0026#34;${trim##*/}\u0026#34; venvdir=$HOME/.cache/python-venv mkdir -pv $venvdir if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then python -m venv $venvdir/$1 2\u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Created $1python venv\u0026#34; elif [ \u0026#34;$projectdir\u0026#34; != \u0026#34;\u0026#34; ]; then python -m venv $venvdir/$projectdir 2\u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Created $projectdirpython venv\u0026#34; else echo \u0026#34;Not git repo, please insert a name for virtual env (for example: pce nganu)\u0026#34; fi We can get git repo root path using the command git rev-parse --show-toplevel but the problem is, those command give the full path to git repo but what i want is only the name of the git repo root so we need to trim the full path and only give the directory name we want. That\u0026rsquo;s what trim and projectdir in those code did. So the name of the git repo dir is in projectdir variable.\nAfter that, venvdir is for the path you want to save all your virtual environment and the command mkdir -pv $venvdir is to make sure if the directory doesn\u0026rsquo;t exist then it\u0026rsquo;s gonna create the directory.\nThe \u0026quot;$1\u0026quot; != \u0026quot;\u0026quot; to make sure if no argument is given then don\u0026rsquo;t create any virtual environment and skip to next statement, but if there\u0026rsquo;s an argument then make the virtual environment with the argument as the name.\nThe \u0026quot;$projectdir\u0026quot; != \u0026quot;\u0026quot; to make sure if it\u0026rsquo;s not a git repo then don\u0026rsquo;t create any virtual environment and skip to next statement, but if it\u0026rsquo;s a git repo then make the virtual environment with the git repo root as the name.\nLet\u0026rsquo;s say we save those script with the name pce (python create env), you can choose different name, you do you. And don\u0026rsquo;t forget to place those script into your PATH so you can use that without typing the full path. With that in mind, here\u0026rsquo;s a few scenario we can do:\n Without any argument, if it\u0026rsquo;s git repo then it\u0026rsquo;s create virtual env with git repo root name and if it\u0026rsquo;s not git repo then it\u0026rsquo;s not gonna create any virtual env.  pce With an argument, it\u0026rsquo;s gonna create virtual env with the argument as virtual env name.  pce nganu #create nganu virtual env Remove Python Virtual Environment After creating virtual environment, we can also delete virtual environment using a shell script. The concept is similar to create virtual environment, the difference is if there\u0026rsquo;s no virtual environment with the same name as user input or git repo root, then the script not gonna delete anything. With that in mind, here\u0026rsquo;s the code:\n#!/bin/sh  gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; projectdir=\u0026#34;${trim##*/}\u0026#34; venvdir=$HOME/.cache/python-venv if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then rm -r $venvdir/$1 2\u0026gt; /dev/null || \\ \techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; elif [ \u0026#34;$projectdir\u0026#34; != \u0026#34;\u0026#34; ]; then rm -r $venvdir/$projectdir 2\u0026gt; /dev/null || \\ \techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; else echo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; fi Activate and Deactivate Python Virtual Environment For activating the virtual environment, i use shell function and then source that function with your shell config, like .bashrc or .zshrc or something else.\nI did that because i can\u0026rsquo;t really source inside a shell script (it\u0026rsquo;s just gonna create a subprocess and didn\u0026rsquo;t effect the current shell, so you can\u0026rsquo;t activate the virtual environment).\nThe concept is similar to create and remove script above, if the virtual environment name not found then give message \u0026ldquo;Python virtual environment not found, create new one\u0026rdquo;, the difference is that we\u0026rsquo;re sourcing the virtual environment to activate it. With that in mind, here\u0026rsquo;s the code:\nfunction pae(){ local gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; local trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; local projectdir=\u0026#34;${trim##*/}\u0026#34; local venvdir=$HOME/.cache/python-venv if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then . $venvdir/$1/bin/activate 2\u0026gt; /dev/null || echo \u0026#34;Python virtual environment not found, create new one\u0026#34; else . $venvdir/$projectdir/bin/activate 2\u0026gt; /dev/null || echo \u0026#34;Python virtual environment not found, create new one\u0026#34; fi } To deactivate the virtual environment, you just need to use deactivate command. And to list all the virtual environment you made, you can use ls command. For example, ls ~/.cache/python-venv (or the path you choose to hold all your virtual environment).\nThe Conclusion What i mean \u0026ldquo;simple\u0026rdquo; earlier is not \u0026ldquo;easier to setup\u0026rdquo;, what i mean is you only need the tools that you probably already have and make a workflow with only that tools to be similar to how conda handles virtual environment.\nThis post is only an alternative way to manage python virtual environment because most python virtual environment tools usually make the virtual environment inside the project directory and i don\u0026rsquo;t really like that, i prefer how conda manage virtual environment but i think conda is overkill to only manage virtual environment and that\u0026rsquo;s why i make those shell script and function to behave the same as conda (without python version control yet).\nI may make this a standalone project if i ever know how to make the installer. And also please keep in mind that this was based on my needs so maybe there\u0026rsquo;s some glitch that i haven\u0026rsquo;t found yet. Ok that\u0026rsquo;s all, hopefully you gain something from this (probably not, like always).\nReferences  A few python virtual environment tools . Python virtual environments documentatton .  "},{"ref":"https://bruhtus.github.io/posts/arch-installation-guide/","title":"Arch Linux Installation Guide (Feb 2021)","section":"posts","tags":["Linux"],"date":"2021.02.23","body":" Don\u0026rsquo;t install arch linux, install TempleOS instead. All hail HolyC! (R.I.P Terry). Jokes aside, please keep in mind that this guide might become obsolete because the nature of rolling release so you should always check the arch wiki installation guide .\n A Brief Intro I\u0026rsquo;m trying to install an OS as minimal as possible but not from scratch like Linux From Scratch or compiling everything like Gentoo . I might try to install both of them but i don\u0026rsquo;t think i\u0026rsquo;m gonna use one of them as my main OS.\nPre-Installation First thing to do is download the iso from here and make bootable usb drive. After that check if your machine using legacy BIOS or UEFI by doing ls /sys/firmware/efi/efivars. If it shows directory without error then the system is booted in UEFI mode, and if the directory doesn\u0026rsquo;t exist then the system may be booted in legacy BIOS (or CSM). Most modern computer usually using UEFI but please check first because it could save you a lot of time during the installation.\nFor this guide, i\u0026rsquo;m installing arch linux on machine with UEFI so i won\u0026rsquo;t cover the legacy BIOS installation here.\nInstallation Here\u0026rsquo;s step-by-step arch linux installation\nLive Environment After booting into live environment, there\u0026rsquo;re a few things you need to do. Below is the things you need to do inside live environment.\nSet Keyboard Layout The default keyboard layout is US so if you want to use US keyboard layout then you can skip this step. You can check available layouts using this command\nls /usr/share/kbd/keymaps/**/*.map.gz and to modify the layout you can use loadkeys. For example, to set a german keyboard layout you can do\nloadkeys de-latin1 Verify The Boot Mode If you already know which one your machine use (legacy BIOS or UEFI), then boot into the right boot mode. You can verify the boot mode by using the same command\nls /sys/firmware/efi/efivars Connect To The Internet At the time of writing this guide, the installation ISO using IWD (Internet Wireless Daemon) to connect to wifi. You can check if you have internet connection or not by doing this\nping google.com #or any website you like such as pornhub.com If there\u0026rsquo;s no internet connection, you can do:\n If you\u0026rsquo;re using ethernet, plug in the cable. If you\u0026rsquo;re using wifi, you can use iwctl (IWD command).  Here\u0026rsquo;s the basic command after you use iwctl (don\u0026rsquo;t forget to press enter to execute the command):\ndevice list #to check the device name station \u0026lt;device-name\u0026gt; scan #to scan the wifi station \u0026lt;device-name\u0026gt; get-networks #to check the available networks station \u0026lt;device-name\u0026gt; connect \u0026lt;wifi-name\u0026gt; #and then enter the password if there\u0026#39;s any exit #exit iwctl After that check again using the ping command.\nUpdate The System Clock After there\u0026rsquo;s an internet connection, you can update the system clock by using\ntimedatectcl set-ntp true to check the service status, use timedatectl status\nPartition The Disks  This is for UEFI, for legacy BIOS please check arch wiki .\n To check the disks that available, you can use lsblk command and pick which disk you want to install arch linux on. The disk start with sd and without a number at the end like sda\nFor the disk partitions, i use cfdisk command. For example, cfdisk /dev/sda (replace sda with the disk you want to use).\n I didn\u0026rsquo;t make swap partition so if you want to make one please refer to arch wiki .\n The first partition is bootloader, usually around 120M and 512M (M means megabytes) depends on what bootloader you want to use. And then change the partition type to EFI System.\nThe second partition is root directory. I use everything that\u0026rsquo;s left but you can spare some for another partition. And then change the partition type to Linux Filesystem.\nAfter you\u0026rsquo;re sure about your disk partitions, then write the changes and then quit.\nFormat The Partitions The next step is format the disk partition. For bootloader partition, format it to FAT32 by using this command\nmkfs.fat -F32 /dev/\u0026lt;efi-system-partition\u0026gt; For root directory, format it to ext4 by using this command\nmkfs.ext4 /dev/\u0026lt;root-partition\u0026gt; For example:\nmkfs.fat -F32 /dev/sda1 mkfs.ext4 /dev/sda2 Mount The Filesystems  More info please refer to EFI System Partition Arch Wiki . The next step is mount the partition that we\u0026rsquo;ve made. First, mount root partition to /mnt using this command\n mount /dev/\u0026lt;root-partition\u0026gt; /mnt after that create two directories, /mnt/boot and /mnt/boot/efi (basically create efi directory inside boot directory which both of them doesn\u0026rsquo;t exist so you need to create one by one). You can create both directories using this command\nmkdir -p /mnt/boot mkdir -p /mnt/boot/efi after that mount your bootloader to /mnt/boot/efi using this command\nmount /dev/\u0026lt;efi-system-partition\u0026gt; /mnt/boot/efi For example:\nmount /dev/sda2 /mnt mount /dev/sda1 /mnt/boot/efi Install Essential Packages You can install essential package by using this command\npacstrap /mnt base base-devel linux linux-firmware vim  /mnt is where you mount your root partition. base is base packages. base-devel is development tool such as sudo and grep. linux is the kernel. linux-firmware is the necessary part of linux kernel. vim is the text editor (i don\u0026rsquo;t even install nano, sorry).  Generate Fstab  Fstab is for telling the kernel what to load in the booting process.\n To generate fstab use this command\ngenfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab -U means using unique ID instead of human readable ID such as sda. You can check the content of fstab by opening /mnt/etc/fstab file using your text editor (in my case is vim, so i use vim /mnt/etc/fstab).\nChange Root Into New System To take a look inside our new system, we can change root to our new system by using this command\narch-chroot /mnt Configure New System After setup our new system in live environment, now we need to configure our system inside the system (not in live environment anymore) after change root into our new system. There\u0026rsquo;re a few things you need to configure, below is the things you need configure inside new system.\nSet The Time Zone To set the time zone, we can use this command\nln -sf /usr/share/zoneinfo/Region/City /etc/localtime For example:\nln -sf /usr/share/zoneinfo/Asia/Jakarta /etc/localtime You can check the region or the city by doing this\nls /usr/share/zoneinfo/Region and\nls /usr/share/zoneinfo/Region/City or just double \u0026lt;tab\u0026gt; while typing ln -sf /usr/share/zoneinfo/\nAfter that, generate the clock using hwclock with this command\nhwclock --systohc Select Your OS Language First, you need to edit etc/locale.gen using your favorite text editor (which is not nano for me) and uncomment the language you want.\nFor example, you want to use US English as your OS language then what you need to uncomment is en_US.UTF-8 UTF-8 and en_US ISO-8859-1.\nAfter that, use this command to generate locales\nlocale-gen and then create the locale.conf file. For example:\nvim /etc/locale.conf #you can replace vim with your favorite text editor the content of locale.conf is\nLANG=en_US.UTF-8 #if you choose US English If you set the keyboard layout (at the beginning of installation), then you make the changes persistent in vconsole.conf. For example:\nvim /etc/vconsole.conf with the content\nKEYMAP=de-latin1 Network Configuration For the network configuration, create the hostname file with the name you want (it\u0026rsquo;s not the user name, it\u0026rsquo;s more like your machine name). For example:\nvim /etc/hostname with the content\nbruhtus After that, add this content to /etc/hosts\n127.0.0.1\tlocalhost ::1\tlocalhost 127.0.1.1\t\u0026lt;yourhostname\u0026gt;.localdomain\t\u0026lt;yourhostname\u0026gt; For example: The previous step i define my hostname as bruhtus, so what i need to do is add this content to /etc/hosts\n127.0.0.1\tlocalhost ::1\tlocalhost 127.0.1.1\tbruhtus.localdomain\tbruhtus  If the system has a permanent IP address, it should be used instead of 127.0.1.1.\n After that, install network manager using this command pacman -S networkmanager wpa_supplicant and then activate on startup using systemctl enable NetworkManager and systemctl enable wpa_supplicant.\nSet The Root Password To set the root (or admin) password, use this command\npasswd and then enter your password.\nAdd New User  The default user is only root (or admin).\n To create new user, use this command\nuseradd -m \u0026lt;username\u0026gt; after that create the password for that username by using this command\npasswd \u0026lt;username\u0026gt; and then enter your password.\nFor example:\nuseradd -m bruhtus #i like my hostname and my username to be the same passwd bruhtus and then enter the password.\nAdd New User To Group  It\u0026rsquo;s so that the new user can have root access using sude\n To add new user to group, we can use this command\nusermod -aG wheel,audio,video,optical,storage \u0026lt;username\u0026gt; Edit Sudoers File  Add the wheel group to have root access with sudo\n Use visudo and look for wheel and then uncomment those line.\nThe line would be like this\n%wheel ALL=(ALL) ALL By default, visudo using vi to edit the file. If you didn\u0026rsquo;t want to use vi then you can do\nEDITOR=vim visudo Install Bootloader  In my case, i use grub as the bootloader. So, i\u0026rsquo;m just gonna cover grub installation in this section.\n First, install the bootloader and efibootmgr using this command\npacman -S grub efibootmgr After that, use this command to install grub on system partition\ngrub-install /dev/\u0026lt;disk-partition\u0026gt; For example:\ngrub-install /dev/sda #without any number at the end If there\u0026rsquo;s no error, then generate grub config using this command\ngrub-mkconfig -o /boot/grub/grub.cfg  If there\u0026rsquo;s an error, please check here or use search engine. I can\u0026rsquo;t cover every single error.\n Reboot If there\u0026rsquo;s no error, exit from chroot using exit command. After that, unmount the system using umount -R /mnt and then reboot (please make sure the disk partition is priority in bios).\nGraphical User Interface If you only use command line interface, then you don\u0026rsquo;t need to continue. But, if you still need graphical user interface (GUI) to open browser or something like that, then keep going.\nLogin To Your Arch Installation After reboot, login to your user you just created (in my case, it\u0026rsquo;s bruhtus). After that check the internet connection by using ping command (for example: ping google.com). If there\u0026rsquo;s an error then connect to your network using nmtui command. After that check the connection using ping command again. If there\u0026rsquo;s still an error, please check arch wiki or use your search engine.\nInstall And Configure Graphical Environment Install a few package using this command\nsudo pacman -S xorg xorg-init alacritty lightdm lightdm-gtk-greeter lightdm-gtk-greeter-settings i3-gaps  xorg and xorg-init is for graphical interface. alacritty is the terminal emulator i choose (you can replace that with your favorite terminal emulator). lightdm, lightdm-gtk-greeter, and lightdmn-gtk-greeter-settings is for login manager (unless you want to start your graphical environment manually like a champ, then you should probably install one). i3-gaps is the window manager i choose (you can replace that with your favorite window manager).   Please keep in mind that i use standalone window manager so i need to install xorg and xorg-init manually, but if you choose desktop environment then i think you don\u0026rsquo;t need to install those two.\n After everything installed, copy the xinitrc using this command\ncp /etc/X11/xinit/xinitrc ~/.xinitrc What that command do is copy xinitrc to your home directory. What .xinitrc do is start the window manager or desktop environment you choose (we\u0026rsquo;ll get to that later in this post).\nThe next step is to edit .xinitrc, you can do the following step:\n Remove the last 5 row (from twm \u0026amp; until exec xterm -geometry 80x66+0+0 -name login) And then add exec i3 in the last row (basically replace the last 5 row with this exec i3).   Twm is default xorg window manager, in case you\u0026rsquo;re wondering.\n Please keep in mind that i use i3 window manager so i use exec i3. To makes thing simpler, exec is where you start your window manager or desktop environment so the usual syntax would be like this exec \u0026lt;window-manager\u0026gt;.\nAfter that, enable login manager by using this command:\nsudo sysmtemctl enable lightdm The last step is to check if there\u0026rsquo;s an error or not by using this command startx to start xorg. If everything fine (you can access your window manager or desktop environment) then reboot.\n For intel user, if there\u0026rsquo;s an error while using startx, you can try installing xf86-video-intel first and see if that fix the error or not. If that doesn\u0026rsquo;t fix your error then you know what to do.\n The Conclusion That\u0026rsquo;s all you need to do to install arch linux. After that you can install all the program you want. I suggest you to try it first inside virtual machine such as virtualbox, virt-manager or something else.\nReferences  Arch wiki installation guide . Arch wiki grub installation . Arch wiki mount EFI system partition . Mental outlaw arch installation video part 1 . Mental outlaw arch installation video part 2 . Iwd usage video . Distrotube arch installation video part 1 . Distrotube arch installation video part 2 .  "},{"ref":"https://bruhtus.github.io/posts/linux-ownership-permissions/","title":"Linux Ownership and Permissions","section":"posts","tags":["Linux"],"date":"2021.02.06","body":" How to change ownership and permission on linux (and probably other unix-like system? i\u0026rsquo;m not sure). Plain boring and important.\n A Brief Intro File/directory ownership is basically who owns the file/directory, for example, if we create a file/directory as admin or root in this case and if we want to edit those file/directory as user then we need to change the file/directory ownership because that file/directory owned by the root and we have no access to change that file/directory.\nTo check the ownership and permission you can use ls -l, to make it simpler, -l means long format and the file ownership on column 3 (username) or 4 (group). Below is the example output when you run ls -l command:\nAs you can see from example above, most of the files owned by bruhtus user and only one file owned by root (the admin) so if we want to change that file then we need to change the ownership or access root privilege everytime we want to change that file (which is such a pain). Your choice.\nThere\u0026rsquo;re 10 character on the most left side or the first column of the ls -l result, let\u0026rsquo;s take a look on nganu file as an example. On the most left side of nganu there\u0026rsquo;re this squence -rw-r--r--. The - means that there\u0026rsquo;s no permission for that file or directory, or if it on the most left then it means it\u0026rsquo;s not directory (or folder). So if nganu is directory rather than a file then the squence gonna be like this drwxr-xr-x.\nr means that the file or directory has read permission (we can take a look at it), w means that the file or directory has write permission (we can edit the file), and lastly is x which means that the file or directory is executable (we can run the file). Usually if we make a folder then it\u0026rsquo;s already have executable permission.\nHow To Change Ownership Change ownership is quite simple, the format is like this\nchown user:group \u0026lt;file\u0026gt; if you want to change ownership from or to root (or admin) then you need to access root privilege. Take the example from the image above, if i want to change ownership of file nganu from root to bruhtus user, then what i need to do is\nsudo chown bruhtus:bruhtus nganu Please keep in mind that the user is on column three and the group is on column four on ls -l result if you\u0026rsquo;re unsure about the user and the group. chown command is not only for file but also for directory (or folder).\nIf you want to change all the files within the directory (not only the directory itself), then you can do\nchown -R user:group \u0026lt;directory\u0026gt; For example, let\u0026rsquo;s assume that nganu is a directory with a file anu and owned by root like this\nIf we want to change ownership of the nganu directory and anu file from root to bruhtus user, then what we need to do is\nsudo chown -R bruhtus:bruhtus nganu Please keep in mind that we need sudo because the previous owner is root (or admin), if the previous owner isn\u0026rsquo;t root then we don\u0026rsquo;t need sudo.\nHow To Change Permission The permission section has 10 character like i explained above, here\u0026rsquo;s the breakdown of those permission:\n   Directory Owner Group Others     d or - rwx or --- rwx or --- rwx or ---     r or read is worth 4 points w or write is worth 2 points x or executable is worth 1 points  So if you have read, write, and execute permission of the owner, group, and others then it\u0026rsquo;s gonna be 777. Here\u0026rsquo;s the example:\nfile anu only has read and write permission for the owner, and only read permission for group and others (or everyone else), in this case it\u0026rsquo;s worth 644. Why is this important? because we can use that to change file or directory permission precisely.\nSo if you want to change anyfile to have the same arrangement as file anu then you can do\nchmod 644 \u0026lt;file\u0026gt; if we want to change those file to have execute permission on owner, group, and others then we can do this\nchmod 755 \u0026lt;file\u0026gt; or if you want everyone to have the same permission you can use +, like this one (have execute permission on owner, group, and others)\nchmod +x \u0026lt;file\u0026gt; or you want to specify one of them (either owner/user, group, or others) you can do one of this\nchmod u+x \u0026lt;file\u0026gt; #execute permission only for user/owner chmod g+x \u0026lt;file\u0026gt; #execute permission only for group chmod o+x \u0026lt;file\u0026gt; #execute permission only for others chmod ug+x \u0026lt;file\u0026gt; #execute permission only for user/owner and group chmod a+x \u0026lt;file\u0026gt; #similar to +x, give user/owner, group, and others execute permission You can also change permission recursively like change ownership, by doing this\nchmod -R u+x \u0026lt;directory\u0026gt; or\nchmod u+x -R \u0026lt;directory\u0026gt; The Conclusion That\u0026rsquo;s all about changing ownership and permission on linux. Hopefully that\u0026rsquo;s useful (probably not, optimistic).\nReferences  DistroTube Video .  "},{"ref":"https://bruhtus.github.io/posts/git-repo-backup/","title":"Git Repository Backup","section":"posts","tags":["Git"],"date":"2021.01.18","body":" Have you ever thought that some day your git remote repository service account got banned or even their server went down? It always crossed my mind and if that happen i would lose all my source code. So in this post i\u0026rsquo;m gonna explain how to backup your git repository locally and to other git remote repository services.\n Skip-able Part Git is a version control system for tracking any changes in any set of files. It\u0026rsquo;s basically a tool that makes tracking changes on your code easier, so you would know which line fix a certain bug and so on.\nThere\u0026rsquo;re a lot of git remote repository such as github, gitlab, bitbucket, codeberg and so on. The most popular one is github, but like other platform, github could decide whether to let you continue in their platform or banned you in their platform. You can check an article about github banned developer account on here and it seems like github in the process of restoring access to all developer in iran, you can check the github CEO tweet here .\nYou might be thinking \u0026ldquo;well, i\u0026rsquo;m not on those country so why should i worry about that?\u0026rdquo;. My point is not about the country that got banned, but about what would you do if your account got banned and all access to you code was gone. We can\u0026rsquo;t take anything for granted these days so better start backup your git repository rather than regret it later.\nBackup Git Repository First of all, there\u0026rsquo;re two ways you can backup a git repository that i knew. The first one is to backup locally in your machine, and the second one is to backup on multiple platforms or even on your own server.\nBackup Git Repository Locally There\u0026rsquo;re two ways to backup your git repository locally on your local machine, that is clone everything (basically the normal git clone command) and clone git bare.\nClone Everything in Git Repository You can backup every file in your git repository by doing the normal git clone \u0026lt;git-url.git\u0026gt; command. By doing that, you download every file in your repository with their actual size and you can do git workflow in it.\nIt\u0026rsquo;s basically the usual way to clone a git repository.\nClone Git Bare You can backup only the git bare from a git repository. To make it simpler, git bare is a .git directory when you initialize a directory as a git repository using git init command.\nSo if you clone the git bare instead of the full git repository, you can get all commit history and all the branch on your git repository without actually downloading all your file so the git bare size is quite small.\nYou might ask \u0026ldquo;if you didn\u0026rsquo;t download all your file then how you called that a backup?\u0026rdquo;. Ok so here\u0026rsquo;s the thing, by cloning a git bare you backup all your commit history and if you decide to upload your git repository to another git services or even your own server, you just need to push those git bare backup and git gonna create all your file with all the commit history on the new git repository. So basically you have your git repository backup without taking too much space on your local storage.\nSo, how can you backup using git bare clone? git have a backup mechanism but it\u0026rsquo;s not obvious, git has a --mirror flag that you can use to backup or push a git bare repository.\nFirst, you need to clone a git bare from git repository by doing\ngit clone --bare \u0026lt;git-repo-url.git\u0026gt; or\ngit clone --mirror \u0026lt;git-repo-url.git\u0026gt;  Both of the commands are basically the same, you can push using --mirror flag or add git remote repository and then push using the usual command. How to push with git bare repository is later in this post.\n Here\u0026rsquo;s an example, if i want to clone my instasaver git bare then the command i need is\ngit clone --bare https://github.com/bruhtus/instasaver.git or\ngit clone --mirror https://github.com/bruhtus/instasaver.git and then it\u0026rsquo;s gonna make instasaver.git directory where it stores all the commit history and branch.\nIf your git repository contains Git Large File Storage (LFS) objects then you need to download those git LFS objects too. First, you clone git bare with one of the command above (--mirror flag or --bare flag) and then download git LFS object with this command\ngit lfs fetch --all  Don\u0026rsquo;t forget to change directory (cd) into your git bare directory before applying those command.\n Backup Git Repository on Multiple Platforms There\u0026rsquo;re two ways you can backup your git repository on multiple platforms. First, you can use git remote add command, and second, you can push git bare repository into another git services. Uploading into your own server is beyond the scope of this post, so i\u0026rsquo;m not gonna explain about that here.\nAdd Git Remote Repository With git remote add you can add another platform so that you can also push into that platform.\nFor example, your main git repository service is github so you usually do\ngit remote add origin \u0026lt;github-repo-url.git\u0026gt; in your local git repository. That command is basically assign origin as your github repository, so if you want to push into your github repository you need to do\ngit push origin \u0026lt;branch\u0026gt; if you didn\u0026rsquo;t like origin as your github repository alias, you can change it to whatever you want, you can change it into github or even anu but you also need to change your git push command like this\ngit push github \u0026lt;branch\u0026gt; or\ngit push anu \u0026lt;branch\u0026gt; So if you want to add another platform and you have your full git working directory (not git bare version), you just need too assign another git repository service url.\nFor example, i want to add instasaver to gitlab. I just need to do\ngit remote add gitlab https://gitlab.com/bruhtus/instasaver.git and then if i want to push the changes to master branch, i just need to do\ngit push gitlab master other git workflow such as git add and git commit is the same.\n Don\u0026rsquo;t forget to change directory (cd) into your local git repo directory.\n Push Git Bare If you alreaady backup your git bare repository on your local machine, then all you gotta do is change directory (cd) into your git-bare-repo.git directory and then do\ngit push --mirror \u0026lt;new-git-repo-url.git\u0026gt; or you could also do\ngit remote add \u0026lt;platform\u0026gt; \u0026lt;new-git-repo-url.git\u0026gt; git push \u0026lt;platform\u0026gt; \u0026lt;branch\u0026gt;  You can change \u0026lt;platform\u0026gt; name to whatever you want such as anu or nganu, you do you.\n The difference between both command is that you can get all the branch if you use git push --mirror meanwhile you only get main branch if you use the usual git push command (without --mirror flag). So if you want to backup all the branch then you should probably use git push --mirror.\nThe disadvantage of using git bare is that you cannot use git workflow such as git add or git commit, you can only use git push command. If you want to update your git bare repository, you can do\ngit remote update inside your git bare directory.\nIf you have Git Large File Storage (LFS) objects, you need to push git LFS object separately using this command\ngit lfs push --all \u0026lt;new-git-repo-url.git\u0026gt; after you download all git LFS objects using git lfs fetch --all command.\nThe Conclusion  Do this post really need a conclusion? well, whatever, here we go.\n There\u0026rsquo;s no harm in backing up your git repository whether locally or on multiple platform, we can\u0026rsquo;t be so sure that some platform gonna stay the same. Maybe the platform we use gonna become worse and mindlessly banned a lot of account, who knows. So it\u0026rsquo;s better to start backup your git repository from now on before something like that happen.\nReferences  How to backup a git repository . How to mirror an entire existing git repository . Duplicating a repository . The difference between git clone \u0026ndash;mirror and git clone \u0026ndash;bare .  "},{"ref":"https://bruhtus.github.io/posts/python-vs-shell/","title":"Python Script vs. Shell Script: Command Line Use Case","section":"posts","tags":["Python","Shell"],"date":"2021.01.14","body":" In this post i\u0026rsquo;m comparing performance python script and shell script that i\u0026rsquo;ve made. The script objective is to check git status on every git repo directory.\n Skip-able Part  Just a background story why i made this post\n Have you wondered what performance between python script and shell script (or some people might say bash script) to run command on terminal? no? same, just kidding. I\u0026rsquo;m curious about shell script because i can do command line stuff with python script too which for my past self is easier to read.\nFor around a month, i\u0026rsquo;ve learned shell scripting from youtube (mostly luke smith videos ) and i gotta say that for command line use case, shell script is more efficient than python script. Please keep in mind that i\u0026rsquo;m not an expert when it comes to python scripting or shell scripting, so there might be some performance enhancement that you can make to my script.\nA Brief Intro to Shell Script You might be wondering \u0026ldquo;what is shell script?\u0026rdquo; well, to put it simpler, it\u0026rsquo;s a script that you can run with shell. Shell, like python, is a programming language and that\u0026rsquo;s why you can run for loop in a terminal like a psychopath, just kidding i did that sometimes (maybe i\u0026rsquo;m a pyschopath? 👀).\nThere are quite a few ways to run shell script, here\u0026rsquo;s some that i know:\n  Make script executable by doing chmod +x \u0026lt;script-name\u0026gt; and then run the script by doing ./\u0026lt;script-name\u0026gt; if you\u0026rsquo;re already in the same directory as the script or chmod +x \u0026lt;path-to-script\u0026gt;/\u0026lt;script-name\u0026gt; and then run the script by doing \u0026lt;path-to-script\u0026gt;/\u0026lt;script-name\u0026gt; if you\u0026rsquo;re not in the same directory as the script. For example: If i want to run my git-status-checker script, then what i would do is type chmod +x git-status-checker if i already in the same directory as git-status-checker script and then run the script by typing ./git-status-checker. But, if i\u0026rsquo;m not in the same directory as git-status-checker, let\u0026rsquo;s say i\u0026rsquo;m in Download directory meanwhile the script is in Script directory, then what i would do is type chmod +x ~/script/git-status-checker and then run the script by typing ~/script/git-status-checker.\n  Using sh or bash command. For example: If i want to run my git-status-checker script, then i run the script by typing sh git-status-checker or bash git-status-checker, both command basically do the same thing.\n  The Test In this post, i\u0026rsquo;m gonna test the runtime both python script and shell script that i\u0026rsquo;ve made. The script objective is to check git status in each git repo directory on my machine.\nThe Environment of The Test All my git repo directory is located in all_git directory on home directory, so i only need to focus on that directory which is gonna make the task of the script a little bit easier and faster than check all other directory too.\nThe amount of not staged for commit is the same when running both script, which is around 13 in total. So the performance may decrease as the amount of unstaged increase.\nI\u0026rsquo;m gonna run the in two way, the first test is including git status for git submodules and the second test is excluding git status for git submodules.\nThe Results First Test In the first test, the runtime of python script is around 0.08 - 0.12 seconds and the runtime of shell script is around 0.07 - 0.08 seconds. Here\u0026rsquo;s the example of runtime of both script (which at this time coincidentally the same):\nSecond Test In the second test, the runtime of python script is around 0.05 - 0.07 seconds and the runtime of shell script is around 0.01 - 0.02 seconds. Here\u0026rsquo;s the example of runtime of both script:\nThe Explanation The effect of checking git status in git submodules is quite a lot but still less than a second which gonna make people think \u0026ldquo;it\u0026rsquo;s not that much of a difference\u0026rdquo;, well, in case you forgot that this script objective is just to check git status on each git repo directory, it\u0026rsquo;s just a simple task. If you\u0026rsquo;re going to make a more complicated task with a lot of command, that\u0026rsquo;s where you\u0026rsquo;re gonna see the gap.\nI\u0026rsquo;m gonna explain the first test source code first, below is the python script for the first test:\nimport os home = \u0026#39;/home/bruhtus/\u0026#39; dir_list = [dirname for dirname in os.listdir(f\u0026#39;{home}/all_git\u0026#39;) if os.path.isdir(f\u0026#39;{home}/all_git/{dirname}\u0026#39;) == True] for dirname in dir_list: path = f\u0026#39;{home}/all_git/{dirname}\u0026#39; os.system(f\u0026#39;echo {path}\u0026#39;) if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: os.system(f\u0026#39;git -C {path}status -s\u0026#39;) os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) else: os.system(f\u0026#39;git -C {path}status -s\u0026#39;) And below is the shell script for the first test:\n#!/bin/sh  for d in $(ls -d ~/all_git/*/); do echo $d \u0026amp;\u0026amp; git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s done Let\u0026rsquo;s take step by step of the processes. The first process is to list all directory in all_git directory, because we already specify that all git repo directory gonna be in all_git directory so we don\u0026rsquo;t need to check if it\u0026rsquo;s a git repo or not (one less task). So, here\u0026rsquo;s a comparison of the python script and shell script:\nThe python part below:\nhome = \u0026#39;/home/bruhtus/\u0026#39; dir_list = [dirname for dirname in os.listdir(f\u0026#39;{home}/all_git\u0026#39;) if os.path.isdir(f\u0026#39;{home}/all_git/{dirname}\u0026#39;) == True] for dirname in dir_list: is equivalent to this part in shell script:\nfor d in $(ls -d ~/all_git/*/); do Both of the script take list of directory in all_git directory and do a for loop to check each folder. You might be able to optimize my python script but i think that the shell script is much simpler than python script. What i mean simpler is less lines of code, not easier to understand. If you need an easier to understand code then python is the way, but it\u0026rsquo;s not really the topic of this post (everyone knows that python script is human readable, right?).\nLet\u0026rsquo;s continue, in the python part below:\npath = f\u0026#39;{home}/all_git/{dirname}\u0026#39; os.system(f\u0026#39;echo {path}\u0026#39;) if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: os.system(f\u0026#39;git -C {path}status -s\u0026#39;) os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) else: os.system(f\u0026#39;git -C {path}status -s\u0026#39;) is equivalent to this part in shell script:\necho $d \u0026amp;\u0026amp; git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s done That second process is to check the git status and git submodules status of all directories. The part where there\u0026rsquo;s echo is basically print out anything that we give, in this case it\u0026rsquo;s gonna print directory name so i\u0026rsquo;m not gonna go into detail for that part.\nNow i\u0026rsquo;m gonna explain a little bit about \u0026amp;\u0026amp; command in shell, it\u0026rsquo;s basically let you run the second command (on the right side) if the first command (on the left side) success. So, it\u0026rsquo;s basically an equivalent of if-else statement in most programming language. The shell script part below:\ngit -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s is equivalent to this part of python script:\nif os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: os.system(f\u0026#39;git -C {path}status -s\u0026#39;) os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) else: os.system(f\u0026#39;git -C {path}status -s\u0026#39;) The ls -a $d | grep -q .gitmodules is equivalent to if os.path.exists(f'{path}/.gitmodules') == True, both command check if there\u0026rsquo;s a git submodules in the git repo directory or not. So in the second test, i\u0026rsquo;m removing those if-else statement and then run only git status which makes shell script faster than python script.\nThe Conclusion For command line interface stuff, you should probably use shell script instead of python script. I\u0026rsquo;m not saying that python is bad, it\u0026rsquo;s just not the right tool for command line interface stuff. Yeah python can do almost everything but that doesn\u0026rsquo;t mean python is the best at everything, at least not at command line interface use case which is shell script clearly better here.\nReferences  Luke smith youtube channel . Distrotube youtube channel . Git command without change directory . Git submodule documentation .  "},{"ref":"https://bruhtus.github.io/posts/mac-os-linux/","title":"The Difference Between GNU/Linux and Mac OS","section":"posts","tags":["Linux"],"date":"2020.12.29","body":" To make things clear that GNU/Linux and Mac OS is different and don\u0026rsquo;t expect both of them do the same thing.\n Skip-able Part What motivates me to make this post is a tweet from someone named David Bombal that trying to install ubuntu on macbook with M1 chip, here is the tweet .\nThat tweet makes me wonder why would someone install ubuntu on mac, isn\u0026rsquo;t mac os and linux similar? well it turn out they\u0026rsquo;re not similar. In this post i\u0026rsquo;ll try to explain what the difference between mac os and linux.\nA Brief Intro to Kernel and Operating System First thing first, we need to know what kernel and operating system is because we\u0026rsquo;re gonna talk a lot about both of them.\nWhat is kernel? to make things simpler, kernel is a bridge between software and hardware. Kernel allocate the system resources (such as CPU, RAM, and other stuff, basically the hardware) to the program/software that we open.\nWhat is operating system? according to debian post : An operating system consists of various fundamental programs which are needed by your computer so that it can communicate and receive instructions from users; read and write data to hard disks, tapes, and printers; control the use of memory; and run other software. The most important part of an operating system is the kernel.\nThen what\u0026rsquo;s the difference? well, kernel is part of the operating system and kernel alone doesn\u0026rsquo;t form a working operating system. You need another component other than kernel to make a working operating system which is beyond the scope of this post.\nA Brief Intro to GNU/Linux What most people knew as \u0026ldquo;linux\u0026rdquo; operating system is usually GNU/Linux. GNU consist the component for operating system except the kernel and linux fill the gap on GNU system. So linux alone doesn\u0026rsquo;t form a working operating system and GNU system with linux kernel make the whole operating system. You can use something other than GNU system if you want, like alpine linux which use musl libc and busybox instead of GNU. GNU/Linux is a unix-like operating system, what it means is that the operating system behaves in a similar manner to a unix system while not necessarily conforming to or being certified to any unix trademark.\nA Brief Intro to Mac OS Mac OS is Apple Mac line up (macbook, mac pro, etc) operating system and it\u0026rsquo;s based on darwin operating system which is based on BSD operating system. Here\u0026rsquo;s the schema for better understanding what i mean:\nMac OS also use XNU as a kernel. XNU stands for X is Not Unix (from apple\u0026rsquo;s github repo ). So Mac OS based on Darwin and XNU which is already different from GNU/Linux.\nThe Difference So basically mac os was based on BSD system while GNU/Linux was based on GNU system. Other than that, mac os doesn\u0026rsquo;t use linux as it\u0026rsquo;s kernel and use XNU instead. I\u0026rsquo;m not sure the difference between XNU and linux at this point, but i can give one example that is KVM (Kernel-based Virtual Machine) which is built into linux kernel. Basically KVM let you interact with kernel directly instead of using emulation for virtual machine.\nOther than the difference between kernel feature such as KVM, there\u0026rsquo;s a different feature or command from BSD system and GNU system. From this article , if you want to use GNU command, you need to install it first while in GNU/Linux you can use it immediately.\nAlso Mac OS doesn\u0026rsquo;t come with the package manager, as far as i know, you need to install the package manager yourself (i might be wrong tho, i have no mac to test it out), whereas the package manager already installed in GNU/Linux.\nTo make it simpler, the package manager is for installing and uninstalling a program from terminal.\nThe Conclusion Mac OS and GNU/Linux are two different system and don\u0026rsquo;t expect them to behaves the same. If you want Mac OS app then you should buy a mac, but if you want to have access to a lot of terminal or command line interface stuff then you should install GNU/Linux on PC or buy a machine that already came with it.\nInstalling GNU/Linux on mac is not really efficient because you could get a lot more computing power with the same price and mac hardware usually optimized with Mac OS so GNU/Linux might not run as smooth as Mac OS on mac (and mac user gonna judge that GNU/Linux not as good as Mac OS bla bla bla, which is stupid).\nHopefully you didn\u0026rsquo;t confuse Mac OS with GNU/Linux again and choose what you need rather than expecting Mac OS and GNU/Linux to behaves the same (and it seems like a lot of people did that).\nReferences  Why GNU/Linux . Linux kernel vs mac kernel . The difference between linux and bsd . Darwin os (wikipedia) . Unix-like term (wikipedia) . Quora about mac os, darwin os, and gnu/linux . Apple\u0026rsquo;s darwin-xnu github repo . Wikimedia unix timeline . Is mac os UNIX .  "},{"ref":"https://bruhtus.github.io/posts/manjaro-i3/","title":"Manjaro i3 Edition","section":"posts","tags":["Linux"],"date":"2020.12.22","body":" A minimalist approach to new arch linux user or new linux user in general. No more, maybe less.\n Skip-able Part  Just a background story why i use manjaro i3 edition.\n Have you ever bored with your current desktop environment? because that\u0026rsquo;s what made me wanted to try manjaro i3 edition. Why manjaro i3 edition and not something else like regolith linux? Here\u0026rsquo;s the reason:  I want a rolling release distro because it\u0026rsquo;s kind of a pain to reinstall the whole system just to upgrade to the new version in fixed-point release distro just like regolith linux (because it\u0026rsquo;s based on ubuntu). I want to try new things and fixed-point release is kind of boring for me. There\u0026rsquo;s nothing wrong with fixed-point release distro, it\u0026rsquo;s just not for me.  Now you wondering why manjaro specifically, Ok let me explain. I choose manjaro because a lot of people said that manjaro is more stable than other arch-based linux because the manjaro team hold on the update for a week or two after the update on arch repo. And now arch linux user gonna get mad at me like \u0026ldquo;Arch linux is also stable if you choose the stable mirror bla bla bla\u0026rdquo;, and honestly it\u0026rsquo;s kind of true. The reason i didn\u0026rsquo;t using arch linux is more for long term stuff. What i mean long term is that when i need to reinstall the operating system on my machine or when i got a new device, i want to install the operating system quickly. Especially with new device which have different hardware and configuration and i need to figure it out one by one if i use arch linux, uh that\u0026rsquo;s kind of hard for me at the moment. I might try installing arch linux in the future because i like the philosophy of arch linux that is \u0026ldquo;choose only what you want\u0026rdquo; so you need to know what you want first and you decide what you want. That\u0026rsquo;s like the most \u0026ldquo;freedom\u0026rdquo; i\u0026rsquo;ve ever seen and i\u0026rsquo;m kind of excited to try it out, but for now i\u0026rsquo;m still trying to figure out what i want to install on my machine and manjaro i3 edition is here to help me do that.\nMy Experience In this part i\u0026rsquo;m gonna type it into two part, the good part and the not good part of manjaro i3 edition.\nThe Good Part What makes manjaro i3 edition special is that this installation come with only minimum stuff which is good because it doesn\u0026rsquo;t take up too much space in the fresh installation.\nWhat do you mean by minimum stuff? well, for example in the full fledged desktop environment such as gnome, kde plasma, and xfce, you got system settings with the function to set up wallpaper, lock screen and stuff that you don\u0026rsquo;t even know. In manjaro i3 edition or in standalone window manager desktop environment, you didn\u0026rsquo;t have something like that. If you want something similar then you need to install the program that has the same function to set up wallpaper, lock screen, and other stuff.\nThat\u0026rsquo;s what i mean by minimal, it doesn\u0026rsquo;t come with a lot of stuff that you probably didn\u0026rsquo;t know that it\u0026rsquo;s installed on you system and take up space.\nThe Not Good Part In case you\u0026rsquo;re happy that the fresh installation doesn\u0026rsquo;t take up a lot of space, you might want to hold that for a moment because it has the downside.\nThe downside with minimum stuff installed is that a lot of things doesn\u0026rsquo;t work from the start so you need to install a package or even tweak a bit on the terminal. For example, you need to install pulseaudio before you could use your headphone jack and you need to install pulseaudio-bluetooth if you want to use your bluetooth audio stuff. The installation doesn\u0026rsquo;t come with pulseaudio and it kind of hard to work with only alsa for me, so you need to keep that in mind.\nBut, it\u0026rsquo;s not really a bad thing (that\u0026rsquo;s why i called it \u0026ldquo;not good\u0026rdquo; instead of \u0026ldquo;bad\u0026rdquo;). Why it\u0026rsquo;s not a bad thing? because you learn how to fix stuff and slowly you understand what works and what doesn\u0026rsquo;t work rather than only using what\u0026rsquo;s already there. So there\u0026rsquo;s a learning process there, not as intense as arch linux but it\u0026rsquo;s the first step towards that.\nYou might complain \u0026ldquo;but it\u0026rsquo;s not specific to standalone window manager, we learn something too in full fledged desktop environment\u0026rdquo;, and yeah you might learn something in full fledged desktop environment but what you learn usually tied down to that specific desktop environment rather than the general stuff. I have no problem if i want to change distro from manjaro to arch linux or debian, but if you use a specific desktop environment that come with the linux distro then you might have a hard time because every linux distro has different configuration for their desktop environment. Yeah it\u0026rsquo;s gnome but the configuration and the key bindings doesn\u0026rsquo;t necessary the same with each linux distro, meanwhile when you\u0026rsquo;re using standalone window manager, you can backup you configuration file to github or other git repositories and use it in any linux distro you want (i\u0026rsquo;m not sure if you can do that with full fledged desktop environment).\nConclusion Manjaro i3 edition is a nice warm up for someone who are trying to learn about standalone window manager desktop environment. Learn what package and configuration you want and learn how to fix the problem that doesn\u0026rsquo;t come with the installation package first, and then finally try arch linux.\nIn case you want something more minimal than manjaro i3 edition, you might want to try arcolinux , especially the arcolinuxd iso (it\u0026rsquo;s a combination of arch linux freedom with calamares installer).\nDecember 2020 Setup  Rick Astley ASCII version anyone?\n "},{"ref":"https://bruhtus.github.io/posts/zsh-aliases/","title":"Zsh Aliases: Be Lazy!","section":"posts","tags":["Shell"],"date":"2020.12.13","body":" Have you ever thought that typing a long command is such a pain? well, here\u0026rsquo;s one of the solution for you.\n Skip-able Part  Just a background story why i write this in the first place.\n At some random time back then i thought \u0026ldquo;can i increase my speed while using the command line interface?\u0026rdquo; and there\u0026rsquo;s two option i could think of back then, the first one is to increase my typing speed and the second one is to make everything i type shorter.\nHonestly i like to take it easy when typing, i\u0026rsquo;m not really those rush type who are trying to type fast. I don\u0026rsquo;t really care about the speed of my typing as long as i\u0026rsquo;m comfortable typing at that speed. It\u0026rsquo;s more like durability rather than speed and that\u0026rsquo;s why i take it easy. So, increasing my typing speed is a no (at least for me).\nThe second option is where piques my interest. From that point i did some research how i can shorten the command i use on internet (google, youtube, and even pornhub but unfortunately no one uploaded on pornhub yet (maybe i should be the one to upload on pornhub? hmm)).\nA Brief Intro to Shell Alias Basically shell alias is a key shortcut for the command that you want to use. Here\u0026rsquo;s an example: If you want to make a new directory, let\u0026rsquo;s say the name of the directory is \u0026ldquo;anu\u0026rdquo;, then if the directory already exist, you want the command doesn\u0026rsquo;t give an error message as a feedback. So here\u0026rsquo;s the command: mkdir -p anu. Everytime you want to make a new directory without an error message if the directory already exist, you need to type mkdir -p \u0026lt;directory-name\u0026gt; and further more if there\u0026rsquo;s a typo while you type it, uh that\u0026rsquo;s such a pain.\nSo, how would you make that simpler? well by using shell alias of course, and in this post i\u0026rsquo;m gonna use Z shell or zsh so the syntax might be different from bash and fish shell. Please keep that in mind.\nType of Zsh Aliases Before we start to how to make a shell aliases or in particular zsh aliases, we need to type the type of aliases that zsh has.\nThere\u0026rsquo;re four type of zsh aliases:\n Simple aliases Suffix aliases Function aliases Global aliases  Simple Aliases Like the name suggest, it\u0026rsquo;s just a simple stuff (or you could say the default?). Here\u0026rsquo;s an example of simple alias: alias md='mkdir -p' From above example, if you want to make a new directory without error message if the directory already exist then you only need to type md rather than mkdir -p. So, it\u0026rsquo;s basically telling the terminal that md equivalent to command mkdir -p and execute mkdir -p command immediately.\nSuffix Aliases Suffix alias is for opening a specific extension in a specific program, it defined using -s flag. Here\u0026rsquo;s an example of suffix alias: alias -s py=vim From above example, it\u0026rsquo;s basically telling the terminal to open every extension .py in vim. So if you have a python file, let\u0026rsquo;s say anu.py, then you just need to type anu.py and it\u0026rsquo;s gonna translate to vim anu.py, opening anu.py file in vim.\nPersonally i don\u0026rsquo;t really use this alias because i like to type vim and even if you\u0026rsquo;re too lazy to type vim you could use simple alias instead, like alias v='vim'.\nFunction Aliases Function alias is when you want an input in your command. For example, you want to open a python file and then preview the output using less (you use try it yourself, i won\u0026rsquo;t explain it). Here\u0026rsquo;s the function alias for that scenario: function anu(){ python $@ | less} #the @ symbol is basically saying to take everything after the command as input. So everytime you run a python script with anu then it\u0026rsquo;s output gonna be displayed in less. Let\u0026rsquo;s say you want to open itu.py, then you use anu itu.py and you\u0026rsquo;re gonna see the output inside of less rather than on terminal.\nHere\u0026rsquo;s another example: function nganu(){ python -c $1} #the 1 number is saying to take the first argument as an input, you could add another input by using $2, $3, and so on. If you type nganu 'import numpy as np' in your terminal then you\u0026rsquo;re gonna execute python -c 'import numpy as np'. So it\u0026rsquo;s basically take the first argument which is 'import numpy as np' as an input (please keep in mind that using '' considered as one argument).\nGlobal Aliases Global alias is the command that you could place anywhere in the sequences. For example, you want to grep a specific file in your directory and you probably gonna type ls | grep filename (honestly i don\u0026rsquo;t care what you type, you do you), if you type ls | grep filename every time you want to search for something than it\u0026rsquo;s gonna become a hassle.\nSo, what should you do? well, you could give a global alias to grep like this: alias -g G='| grep'. After that you could place G in the middle of your command, like this: ls G filename | less (it\u0026rsquo;s gonna direct the grep result to less) or in the back of your command, like this: ls G filename (not using less). The G alias is only for | grep command so you still need to type the pattern to search or in this case a filename.\nTips To make some Aliases First, before you make aliases, please check if the command already exist or not using which command. For example: i want to name my alias ls, before i make alias ls, i\u0026rsquo;m need to type which ls to show me if ls already assigned to other program or command. If after you type which \u0026lt;alias-you-want\u0026gt; and there\u0026rsquo;s an error like \u0026lt;alias-you-want\u0026gt; not found then you\u0026rsquo;re ready to go.\nSecond, don\u0026rsquo;t make to much specific simple aliases. For example, you want to make alias to install packages from your package manager, let\u0026rsquo;s say debian package manager (apt), you need to type sudo apt install \u0026lt;package-name\u0026gt;, and you want to make it simple by using alias like alias sai='sudo apt install'. So everytime you want to install something you just need to type sai \u0026lt;package-name\u0026gt;, simple right? well, honestly it\u0026rsquo;s not really a good practice because over time you\u0026rsquo;re gonna forget what the command behind those alias. If you\u0026rsquo;re really forgot what the command then there\u0026rsquo;s which command as a livesaver, but do you really want that? Imagine you\u0026rsquo;re using another machine without your aliases, then you forgot a simple command because you\u0026rsquo;re using to much alias, isn\u0026rsquo;t that kind of sad?\nConclusion Shell alias is like a double edge sword, use it carefully and don\u0026rsquo;t be dependent on it. If possible, only make aliases if you\u0026rsquo;re using it a lot and already know the basic or maybe make aliases that represent the executed command.\nBonus Useful Alias function sd(){cd \u0026quot;$(du ~ | awk '{print $2}' | fzf)\u0026quot;}  That\u0026rsquo;s my first attempt using fzf, it basically change directory using fzf output as an input.\n function cs(){find ~ -type f | fzf | xargs -o -r vim}  That\u0026rsquo;s basically find every files in home directory using find and fzf and then open the file in vim directly (the parameter -o for not broke my terminal after the command (for some reason) and the -r for if there\u0026rsquo;s no input then it\u0026rsquo;s gonna back to terminal instead opening vim. For more detail please check on man page).\n References  Types of zsh aliases . Luke smith youtube channel . Distrotube youtube channel .  "},{"ref":"https://bruhtus.github.io/posts/manjaro-backup/","title":"Backup: The Options","section":"posts","tags":["Linux"],"date":"2020.11.22","body":" A few options about backing up your system, especially on arch-based distro. It\u0026rsquo;s mostly for linux, i\u0026rsquo;m not sure if you can use it for mac os or windows too.\n Why Should You Backup Your System? In case you\u0026rsquo;re wondering why you should backup your system (for some reason), let me briefly explain. There\u0026rsquo;s always a risk of failure when you\u0026rsquo;re updating or installing a new package (to make things easier to understand, it\u0026rsquo;s basically a program), when that happens, do you want to restore your system before the error occur or you want to re-install the whole system from scratch? your choice. For me personally, i want to restore my system before the error occur rather than re-install everything from scratch. Ideally, before you install something, you should backup your system just in case there\u0026rsquo;s some trouble. If you\u0026rsquo;re insane enough to backup your system everytime you want to install new stuff or update your system then go ahead, you do you.\nIn this post, i\u0026rsquo;m gonna explain two most popular backup solution, that is clonezilla and timeshift , and also a bonus tip for arch-based linux user.\nClonezilla For the record, i\u0026rsquo;ve never tried this. I only do the research about backup on linux and this tool came up quite often. For more info on how to use this tool, you can check youtube video here .\nClonezilla is a tool to backup your entire system. That\u0026rsquo;s right, it backup your whole system. You might think \u0026ldquo;wow that\u0026rsquo;s great, i don\u0026rsquo;t need other backup tool!\u0026rdquo;, you might want to hold that thought. Yeah it backup your entire system, even your empty storage. Let me explain, for example your system has 200 GB allocated storage and let\u0026rsquo;s say you used only 100 GB so you have another 100 GB storage left. When i said clonezilla backup your entire system, it means clonezilla gonna backup the 200 GB of allocated storage for your system and not only 100 GB of used storage. So if you\u0026rsquo;re gonna install the whole system again (which is possible with clonezilla apparently), you need minimum the allocated storage space from previous machine for your system. If you didn\u0026rsquo;t provide the minimum storage space, you gonna get the error. Unless you strive for that error then it\u0026rsquo;s not gonna be a problem but if you didn\u0026rsquo;t expect the error then you might get confused. That\u0026rsquo;s the downside of clonezilla as far as i know it. I personally didn\u0026rsquo;t use it because of that downside but if it\u0026rsquo;s an advatage for you then you should try it.\nTimeshift This is a backup tool that i\u0026rsquo;ve always used since i started using linux on daily basis. The concept is simple, it made checkpoint for your system and then you can revert back to that checkpoint if something wrong after the update or after installing new stuff.\nFor arch-based linux user, this tool is a must. Arch-based linux is a rolling release linux, that means you get an update regularly and those update not always gonna work so there\u0026rsquo;s a high chance your system gonna break from the update. This tool is a livesaver.\nThe difference between this and clonezilla is that this tool only backup the used storage and also exclude the home directory (you can include backup home directory in the setting). In case you\u0026rsquo;re wandering why exclude the home directory is a default, here\u0026rsquo;s an example: If you have 3 file on your home directory, let\u0026rsquo;s just say \u0026lsquo;anu.py\u0026rsquo;, \u0026lsquo;nganu.py\u0026rsquo;, \u0026lsquo;anumu.py\u0026rsquo; then you backup your system. After backing up your system you add another file, let\u0026rsquo;s say \u0026lsquo;itumu.py\u0026rsquo;, then you revert back to the checkpoint you created previously. After you revert back, the file \u0026lsquo;itumu.py\u0026rsquo; is gone because it\u0026rsquo;s re-write everything according to checkpoint list and file \u0026lsquo;itumu.py\u0026rsquo; not in the checkpoint list yet. That\u0026rsquo;s why excluding home directory is a default, you can change it in the setting if you want.\n I\u0026rsquo;ve never used this tool on newly installed system like clonezilla did, so i\u0026rsquo;m not sure if you can using that checkpoint on newly installed system.\n Bonus Tip Here\u0026rsquo;s a bonus tip for arch-based linux user, just in case those two tools didn\u0026rsquo;t save your system (it\u0026rsquo;s not gonna hurt if you have another backup option you know).\nType this two command in your terminal:  For official arch repository: pacman -Qqen \u0026gt; pkglist-repo.txt For arch user repository (AUR): pacman -Qqem \u0026gt; pkglist-aur.txt  What it did is make a package list installed on your system from official arch repository and AUR and save it to .txt file. After that you can install all your package with those file using the command below:  For official arch repository: sudo pacman -S --needed - \u0026lt; pkglist-repo.txt For arch user repository (AUR): for x in $(cat pkglist-aur.txt); do pamac build $x; done (you can also use other alternative such as yay, you don\u0026rsquo;t need to use pamac if you don\u0026rsquo;t want to).  For more info you can check my repo here , i write a few references there.\n"},{"ref":"https://bruhtus.github.io/posts/instasaver/","title":"Instasaver: Save Your Chosen Instagram Posts","section":"posts","tags":["Python"],"date":"2020.11.09","body":" A brief explanation about instasaver, a tool to save instagram post build with instaloader python module and streamlit.\n Background Story  Nothing fancy, you should probably skip this.\n To make things short, basically i was inspired by Kevin Hazy\u0026rsquo;s project here . After i see Kevin\u0026rsquo;s project, i was like \u0026ldquo;can i make almost the same thing with python?\u0026rdquo; and that was the trigger.\nIf you ask me why i want a tool (kind of) to save instagram post, that\u0026rsquo;s because i have a hard time to save video memes on instagram and i don\u0026rsquo;t really want to use a screen recorder (said the guy who made something that took longer than learning how to use screen recorder and edit the result 👀). So anyway, that\u0026rsquo;s my motivation to made this. For the memes!\nInstaloader Python Module  In this part, i\u0026rsquo;m only gonna explain the part that i\u0026rsquo;ve used and tried along with the problem that i\u0026rsquo;ve encountered. So for the full documentations, you can check here .\n Instaloader Feature That I Used There\u0026rsquo;re a lot of things you can do with instaloader like saving instagram stories, follower list, following list, and so on, but for this project i only use saving post feature. Why i only use that feature? because other features require login to instagram account, i\u0026rsquo;ll explain on that later.\nSo, even only saving post feature has a lot going on but i\u0026rsquo;m not quite sure how to implement that, the only thing i\u0026rsquo;m sure is how to implement saving post using a url (well, also other features that require login too). If you want to learn other features you can check their documentations which is quite confusing. Well, maybe i\u0026rsquo;m just a complete idiot but you know what, you\u0026rsquo;re gonna have other complete idiots that didn\u0026rsquo;t understand their documentations. I don\u0026rsquo;t really want to be hard on them but they could have done better, especially on the examples (not only advance examples but basic examples too).\nInstaloader Feature That I\u0026rsquo;ve Tried Apart from saving post feature, i\u0026rsquo;ve also implemented saving stories, following list, and follower list. As i\u0026rsquo;ve mentioned above, those three features require login to instagram account. Well, you can made a fake account to login to instagram but it\u0026rsquo;s not ideal to deploy it. There\u0026rsquo;s this problem when you login quite often so you need to wait around 15 minute to use it again, i mean if i want everyone else to access it then that problem gonna be a hassle. I\u0026rsquo;ve never tried to make a lot of fake account and rotate through all of them to login but i might do it later, who knows. If you want to see all the features that i didn\u0026rsquo;t implement in the public version, you can check my github repo here .\nImplementation With Streamlit  In this part, i\u0026rsquo;m gonna explain the implementation with streamlit and how to deploy on streamlit sharing.\n Instaloader (Main Class) For starter, in instaloader main class (Instaloader ) the parameters that used in this project was dirname_pattern, download_comments, download_geotags, download_video_thumbnails, and save_metadata.\ndirname_pattern was to make the default folder to save the file which in this case i use temporary folder because i don\u0026rsquo;t want to save the images or videos on my github repo but download it to my device (whether smartphone or pc).\ndownload_comments, download_geotags, download_video_thumbnails, and save_metadata is set to False just to make it simple, i just want to download the memes (whether images or videos) and i don\u0026rsquo;t want anything else. You could turn that on to get all that stuff tho.\nShort code To extract the short code, i used regular expression from this source and then using instaloader Post.from_shortcode and download_post to finally download the post (saved in temporary folder first and then generate download button to save to device).\nPublic Version What i mean by public version is the version that i deploy so that everyone can use it. For the public version, i only use saving post with url input. The diagram process is below:\nAs you see above, you can download one item or multiple items in one post. Implementation for download instagram post with streamlit is first you download the post from url using instaloader download_post and saved it to temporary folder and then define exception handling to detect whether it\u0026rsquo;s an image or a video. The implementation is below:\nimport instaloader import streamlit as st import os from zipfile import ZipFile post = instaloader.Post.from_shortcode(instaloader.Instaloader.context, shortcode) instaloader.Instaloader.download_post(post, target=temp) #temp is temporary folder file_list = [filename for filename in os.listdir] if len(file_list) == 1: #if only one item in one post try: st.image(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, use_column_width=True) #use_column_width is to resize the width of the image displayed st.markdown(download_button(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, temp), unsafe_allow_html=True) #download_button is to generate link to download the file except: st.video(f\u0026#39;{temp}/{file_list[0]}\u0026#39;) st.markdown(download_button(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, temp), unsafe_allow_html=True) #download_button is to generate link to download the file else: #if more than one item in one post with ZipFile(f\u0026#39;{temp}/{shortcode}_posts.zip\u0026#39;, \u0026#39;w\u0026#39;) as zip: #put all the posts into zip file for filename in file_list: try: st.image(f\u0026#39;{temp}/{filename}\u0026#39;, use_column_width=True) st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) except: st.video(f\u0026#39;{temp}/{filename}\u0026#39;) st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) please keep in mind that the implementation above is only a partial from all the code, it\u0026rsquo;s just a glimpse of what\u0026rsquo;s going on inside the process. For the full code you can check here .\nLocal Version What i mean by local version is the version that has a more features than the public version that requires login to instagram account. For the local version, i use download the post from url feature (like the public version), download stories (download all the stories of the user, public profile only or you already followed the private account), and download following and follower list.\nDownload stories For download stories, i used download_stories module with user id as input and zip all the stories. Below is a diagram process of the download stories:\nglimpse of the process:\nimport instaloader from zipfile import ZipFile load = instaloader.Instaloader() profile = instaloader.Profile.from_username(load.context, username) profile_id = instaloader.Instaloader.check_profile_id(username) load.download_stories(userids=[profile_id.userid]) with ZipFile(f\u0026#39;{temp}/{profile.username}_stories.zip\u0026#39;, \u0026#39;w\u0026#39;_) as zip: for filename in file_list: zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) st.markdown(download_button(f\u0026#39;{temp}/{profile.username}_stories.zip\u0026#39;, temp), unsafe_allow_html=True) Download following and follower list For download following and follower list, i used get_followees and get_followers module with username as input and write all user\u0026rsquo;s following and follower username in .txt file.\nglimpse of the process (download following list):\nimport instaloader load = instaloader.Instaloader() profile = instaloader.Profile.from_username(load.context, username) with open(f\u0026#39;{temp}/{profile.username}_following.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: for following in profile.get_followees(): f.write(f\u0026#39;{following.username}\\n\u0026#39;) st.markdown(download_button(f\u0026#39;{temp}/{profile.username}_follower.txt\u0026#39;, temp), unsafe_allow_html=True) Deployment I use streamlit sharing and heroku to deploy this project, why i use two services to deploy this project? that\u0026rsquo;s because i want to try web app deployement on heroku and streamlit sharing (which is quite new).\nStreamlit Sharing To deploy on streamlit sharing you need to request an invite in their website and then after that you can deploy your streamlit app. It takes a few minutes to deploy the first time but after that it\u0026rsquo;s deploy in an instant. You can check the live demo on streamlit sharing here .\nHeroku To deploy on heroku, you need a few thing and here\u0026rsquo;s the list:\n runtime.txt -\u0026gt; to specify python version. Procfile -\u0026gt; to specify type of our application and run command, check here for more info. create_config.sh (or setup.sh, whatever you want) -\u0026gt; to make config.toml for streamlit to run.  I don\u0026rsquo;t really need to explain about runtime.txt, do i? you just need to type your python version, for example python-3.7.5, that\u0026rsquo;s all (i explain it anyway, dammit).\nFor .sh file (in this case i\u0026rsquo;m gonna name it create_config.sh because i\u0026rsquo;m not creative, sorry), type this:\nmkdir -p ~/.streamlit echo \u0026#34;[server] headless = true port = $PORTenableCORS = false \u0026#34; \u0026gt; ~/.streamlit/config.toml For Procfile, you don\u0026rsquo;t need to add an extention there. Just Procfile is enough. In the Procfile, type this:\nweb: sh create_config.sh \u0026amp;\u0026amp; streamlit run instasaver.py You don\u0026rsquo;t need to name it create_config.sh, be creative, don\u0026rsquo;t be like me.\nAfter all that, you can deploy it using git or from github or something else (i don\u0026rsquo;t remember all the choices). If you want to deploy it using git workflow, you can check here .\nFor live demo on heroku, you can check here .\nReferences  The one who trigger me to made this . Instaloader documentations . Streamlit . Download instagram stories . View public profile anonymous (more advance version) . Streamlit multiselect nested in if . Streamlit download file . Deploy streamlit app on heroku example .  "},{"ref":"https://bruhtus.github.io/posts/tiling-window-manager/","title":"Tiling Window Manager For Efficiency","section":"posts","tags":["Linux"],"date":"2020.10.15","body":" A brief explanation about tiling window manager and configuration tiling window manager that i\u0026rsquo;ve tried.\n A Brief Explanation About Window Manager  A window manager is a system software that controls the placement and appearance of windows within a windowing system in a graphical user interface (GUI). It can be part of desktop enviroment (DE) or be used standalone1.\n So basically window manager is how to place a window. There\u0026rsquo;re three types of window manager (according to arch wiki1): stacking (aka floating) window manager, tiling window manager, and dynamic window manager.\nHere\u0026rsquo;s a brief explanation about those three window manager:\n Stacking (aka floating) Window Manager: All window manager that allow the overlapping of windows are considered stacking window manager, although it is possible that not all stacking window manager use the same method. You can check a few list of stacking window manager here . Tiling Window Manager: Tiling window manager manage the window so that no window are overlapping with each other. You can check a few list of tiling window manager here . Dynamic Window Manager: Dynamic window manager is a tiling window manager that positioned based on the preset layouts which user can switch. You can check a few list of dynamic window manager here .  For more detailed comparision of tiling window manager, you can check here .\nThe Window Manager I\u0026rsquo;ve Tried For now (at the times of writing this post), i only have tried 2 window manager. My first window manager is qtile and my second window manager is i3. You can check below for more (not really) detailed explanation(?).\ni3 Window Manager i3 is one of tiling type window manager, for more info you can check at their website . I\u0026rsquo;m just gonna explain configuration that i\u0026rsquo;ve made at my github repo . For the record, this is my current window manager (at the time writing this post). I\u0026rsquo;m using the manjaro i3 edition.\nFirst of all, i changed the mod keybinding from super key (or some people call it windows key) to alt key. You can change the mod keybinding by change set $mod Mod4 (super key) to set $mod Mod1 (alt key) in config file (you can find config file in folder .i3). For the most part i used the default config from manjaro i3 edition but i add a few program and even changed the i3 status bar with polybar. Here\u0026rsquo;s what i used in this config:\n pywal : I use this for color scheme around my i3 environment (such as terminal, border color around window, etc) polybar : I use this to replace i3 status bar because i can place the clock and date in the middle (i have no ide how to do that in i3 status bar or even py3status) and it has quite a lot of customization. You can also use py3status if you want. rofi : I use this as application manager instead of dmenu, because it\u0026rsquo;s more convenient for me (rofi appear in the middle meanwhile dmenu appear at the top). conky : I use this to take a glance what process currently taking up resources (for more detailed info i use htop). flameshot : I use this to take screenshot.  Setting Up Keybinding For setting up keybinding you can use bindsym, for example: bindsym $mod+q kill for close focused or currenly active window. Other than setting up keybindings, you can also set a program to do a certain thing, for example: set $myTerm alacritty, every thing that used $myTerm gonna access the command via terminal alacritty. Alacritty is my current (at the time of typing this post) terminal emulator, i also have xterm as a backup terminal emulator. Example of using $myTerm: bindsym $mod+e exec $myTerm -e ranger to open ranger file manager.\nSetting Up Polybar For setting up polybar, you need to move the default polybar config. In my case, the default config is in /usr/share/doc/polybar/ but if it\u0026rsquo;s not there, you can use locate polybar | grep config.\nFirst of all, make a polybar folder in .config folder. After that move the default polybar config into those folder (the path should be like this .config/polybar/config). You can change the default config to anything you want, but remember the bar name because we\u0026rsquo;re gonna use the bar name to launch the polybar. The default bar name should be like this [bar/example], you can change it to the name you want and please specify the monitor for the polybar. You can check you monitor name by typing xrandr on terminal. Here\u0026rsquo;s an example how to set a monitor in polybar config:  After i typed xrandr on my terminal, i got my laptop screen name eDP-1 so i\u0026rsquo;m gonna use my laptop screen to display the polybar.\n [bar/mainbar-i3] monitor = {env:MONITOR:eDP-1} After you\u0026rsquo;re done with your polybar config, the next step is to add launch.sh. What is launch.sh? well, it\u0026rsquo;s basically to launch all of our polybar bar config (that has this naming scheme [bar/example]). Here\u0026rsquo;s an example of launch.sh in two monitor (each bar config has different monitor assigned to it):\n#!/usr/bin/bash #Terminate already running bar instances killall -q polybar #Wait until the process have been shut down while pgrep -u $UID -x polybar \u0026gt;/dev/null; do sleep i; done #launch bar polybar mainbar-i3 \u0026amp; polybar secondbar-i3 \u0026amp; echo \u0026quot;bars launched...\u0026quot; Who Should Try or Use i3? i3 is a \u0026lsquo;manual\u0026rsquo; tiling window manager so it doesn\u0026rsquo;t really have default layout which is different from dynamic \u0026lsquo;tiling\u0026rsquo; window manager, you need to specify where the window opened (whether the window opened on the right or below). If you want to use i3 you might want to consider that. So, who should try or use i3? everyone who wants a tiling window manager and doesn\u0026rsquo;t really mind to have manually control where the window appear, that\u0026rsquo;s all.\nQtile Window Manager Qtile is one of dynamic window manager that use python as basis configuration, for more info you can check at their website .\nIn my config repo i usually use MonadTall layout or Max layout. MonadTall layout basically split the first two window into half vertically and then for the third and so on gonna split the right window horisontally. Max layout basically have the application automatically take up the whole screen, that\u0026rsquo;s all. For more build-in layouts you can check their documentation here .\nI used qtile window manager first before switch to i3 because the configuration is in python but what i don\u0026rsquo;t really like is how qtile treat multiple screen. When i want to switch to second monitor, it swapped the application on second monitor to first monitor (currently active monitor) and that\u0026rsquo;s not what i want, i just want to switch to different screen and not have application on that screen swapped with application on my currently active screen. It was confusing and then i tried i3wm after that.\nThe config in my repo is a basic config i\u0026rsquo;ve done because i don\u0026rsquo;t really like the workflow of qtile. Sorry about that.\nWho Should Try or Use Qtile? If you\u0026rsquo;re fine with the workflow of qtile, wants to try or use dynamic window manager, like to have or doesn\u0026rsquo;t really mind preconfigured layout, and have experience with python then you might want to try qtile. Don\u0026rsquo;t get me wrong, qtile is good as a window manager but unfortunately it doesn\u0026rsquo;t meet my needs.\nThe Pros and Cons of Tiling Window Manager Pros:\n You don\u0026rsquo;t have to worry about placement of your window because it\u0026rsquo;s automatically split the workplace for you. You use mouse less. All your application is right there without any overlap window.   Surprise, you haven\u0026rsquo;t close me yet so i\u0026rsquo;m gonna take up your computer resources.\n Cons:\n If you open too much application then the application size gonna become smaller. (that\u0026rsquo;s why use virtual desktop or workspace 1 to 8). It\u0026rsquo;s kind of hard to setting for first time. After that hard time, you can backup your previous config and use it again in other computer. Nice.  To Wrap Things Up If you want to use keyboard-oriented navigation or you don\u0026rsquo;t want to use mouse often, then you probably should try using tiling window manager. But, as i\u0026rsquo;ve mentioned above, it took time to learn how to configure it (and i think it\u0026rsquo;s worth your time).\nIf you\u0026rsquo;re still not sure, you can try it first in virtual machine (such as virtualbox or virt-manager) and then you could copy those config file to your new installation.\nExample of i3 Window Manager  Moderate use:   Too much window in one workspace can be bad:    Arch linux wiki (window manager) .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"ref":"https://bruhtus.github.io/posts/pavement-distress-detector/","title":"Pavement Distress Detector Using Single Shot Detector (SSD)","section":"posts","tags":["Python"],"date":"2020.10.11","body":" This was my graduation project, in this article i\u0026rsquo;m gonna explain what i did in my graduation project. This project was based on Congcong Li\u0026rsquo;s Project .\n Diagram Process of This Project A Brief Explanation About Single Shot Detector (SSD) Single shot detector is a deep learning method presented by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed4, Cheng-Yang Fu, Alexander C. Berg in their research paper SSD: Single Shot Multibox Detector . There are 2 commonly used SSD model, that is, SSD300 and SSD512. Here\u0026rsquo;s a brief explanation about SSD300 and SSD512:\n SSD300: More fast. SSD512: More accurate.  Long story short, SSD300 is about speed. If you need speed than you should probably use SSD300 (i haven\u0026rsquo;t tried the mobilenet as base network at the time to type this, so at this time knowledge SSD300 is faster than SSD512). Meanwhile, SSD512 is about accuracy. It doesn\u0026rsquo;t really show up in image processing but in video processing, i notice that there\u0026rsquo;s a frame rate drop while doing live object detection. To be fair, SSD300 has frame rate drop as well but it\u0026rsquo;s still usable (around 7-10 frame per second) but SSD512 has frame rate around 3-5 frame per second. Who want to watch a video with 3 fps?? If you\u0026rsquo;re that kind of person then, go ahead. You do you mate.\nFor the record, at that time when I try live detection, i use opencv to display live detection video. i\u0026rsquo;m not sure whether it is opencv fault or the model fault because if I save the video result, the video itself has no frame rate drop. It\u0026rsquo;s weird but it happens, so let\u0026rsquo;s go on with saving the video and forget about live detection (for now, until i find some way to do live detection).\nSo, in this project i\u0026rsquo;m not gonna make it live detection. Rather than live detections, we\u0026rsquo;re gonna save the video result first and then display it later. That way it could also reduce some computational cost.\nFor those who still confused about live detection, to make things simpler, live detection is when you process the video, detect the object, and play the video at the same time. After you detect the object, you immediately display the frame that just recently processed and then processed the next frame. Repeat.\nSingle Shot Detector (SSD) Architecture That\u0026rsquo;s Used in This Project As explained above, in this project we\u0026rsquo;re gonna use SSD512. SSD512 is basically SSD with input image 512x512. The basic architecture of SSD contains 2 part, base network and extra feature layers. The base network layers are based on standard architecture used for high quality image classification (truncated before classification layers). The extra feature layers used for multi-scale feature maps for detection and convolutional predictors for detection.\nHere is an architecture single shot detector that used in this project (made this with NN architecture maker ):\nInformation:\n Input image. Base Network (truncated before classification layers). Layer 6 and layer 7 of base network (from fully-connected layer turned into classification layer). Extra feature layers. Collection of boxes and scores.  Base Network The base network used in this project is Visual Geometry Group (VGG). I chose VGG because of transfer learning capability so that i could have a good result with small dataset. To be more specific, in this project i used VGG16, here is a brief explanation of each layers:\n In the first layer, there\u0026rsquo;s a convolutional process with kernel filter 3x3 and stride (total shift filter per pixel) 1 pixel. That process repeat 2 times and then did some max pooling with kernel filter 2x2 and stride 2 pixel. In the second layer until fourth layer, the model did the same thing as in first layer. The difference was in fifth layer. In fifth layer, the convolution process still the same as the other four layers but the max pooling process was different from the other four layers. The max pooling process used kernel filter 3x3 with stride 1 pixel with padding (adding zero value around pixel image) 1. You can check the illustration below to understand the process of max pooling with kernel filter 3x3, stride 1, and padding 1.  And here\u0026rsquo;s a VGG16 after truncated from classification layers:\nIf you want to calculate the result from max polling, you can use this equation 1:\nInformation:\n kernel_size, stride, padding, and dilation can be 1 integer (in this case, the value for height and width are the same) or 2 integer (in this case, the first integer is height and the second integer is width). For more info you can see pytorch page .  Here\u0026rsquo;s some example of max pooling calculation with input 32x32, kernel filter 3x3, stride 1, padding 1, and dilation 1:\nLayer 6 and Layer 7 After feature extraction process in base network, the next layers is to change layer 6 and 7 of base network from fully-connected into convolutional layer with subsample parameters from fully-connected 6 (fc6) and fully-connected 7 (fc7). The convolution operation used in layer 6 and layer 7 is atrous convolution, you can see atrous convolution shift below:\nWith atrous convolution we can expand area of observation for feature extraction while maintaning the amount of parameters fewer than traditional convolution operation.\nExtra Feature Layers Extra feature layers is a prediction layers. In this layer, the model predict the object using default box. Default box is a box with various aspect ratio in every location of feature maps with different size. You can see an example of default box below 2\nIn the last layer is a collection of default boxes which closer to ground truth box with confidence score from that default boxes.\nTake A Video (Training Video and Testing Video) In this part, i\u0026rsquo;m gonna explain about the video used in this project. The camera configuration, the place where the video taken, the camera angle and height from the road.\nThe place where the video taken was in Surabaya, at Kertajaya Indah Timur IX, Kertajaya Indah Timur X, and Kertajaya Indah Timur XI. The camera angle was perpendicular(?) with the road (90 degrees) and the camera position from the road was 200 cm.\nThere\u0026rsquo;re 7 video taken, 3 for training and 4 for testing. The format of the video was *.mp4. You can check the location partition of the video taken below:\nThe black block is for testing and the white block is for training. You can check the position of the camera below:\nSetting Up The Config File For more detailed configuration please check develop guide by Congcong Li . In this post i\u0026rsquo;m gonna explain it the easiest way.\nBasic Configuration To make things easier, copy the format dataset you want. For example, in this project i want to use COCO dataset format. Then, i copied the coco.py in the path ssd/data/datasets/ and rename it to my_dataset.py. After that, edit the class names for your classification class. In this project, the class i\u0026rsquo;m gonna use is alligator crack, longitudinal crack, transverse crack, and pothole. Also, don\u0026rsquo;t forget to change the class COCODataset to MyDataset.\nThe next step is to add those configuration to __init__.py in ssd/data/datasets/. For example:\nfrom .my_dataset import MyDataset _DATASETS = { \u0026#39;VOCDataset\u0026#39;: VOCDataset, \u0026#39;COCODataset\u0026#39;: COCODataset, \u0026#39;MyDataset\u0026#39;: MyDataset, } Another next step is to add the path of your datasets and anotations to the path_catlog.py in ssd/config/. For example:\nimport os class DatasetCatalog: DATA_DIR = \u0026#39;datasets\u0026#39; DATASETS = { \u0026#39;my_custom_train_dataset\u0026#39;: { \u0026#34;data_dir\u0026#34;: \u0026#34;train\u0026#34;, \u0026#34;ann_file\u0026#34;: \u0026#34;annotations/train.json\u0026#34; }, \u0026#39;my_custom_validation_dataset\u0026#39;: { \u0026#34;data_dir\u0026#34;: \u0026#34;validation\u0026#34;, \u0026#34;ann_file\u0026#34;: \u0026#34;annotations/validation.json\u0026#34; }, } @staticmethod def get(name): if \u0026#34;my_custom_train_dataset\u0026#34; in name: my_custom_train_dataset = DatasetCatalog.DATA_DIR attrs = DatasetCatalog.DATASETS[name] args = dict( data_dir = os.path.join(my_custom_train_dataset, attrs[\u0026#39;data_dir\u0026#39;]), ann_file = os.path.join(my_custom_train_dataset, attrs[\u0026#39;ann_file\u0026#39;]), ) return dict(factory=\u0026#34;MyDataset\u0026#34;, args=args) elif \u0026#34;my_custom_test_dataset\u0026#34; in name: my_custom_train_dataset = DatasetCatalog.DATA_DIR attrs = DatasetCatalog.DATASETS[name] args = dict( data_dir = os.path.join(my_custom_train_dataset, attrs[\u0026#39;data_dir\u0026#39;]), ann_file = os.path.join(my_custom_train_dataset, attrs[\u0026#39;ann_file\u0026#39;]), ) return dict(factory=\u0026#34;MyDataset\u0026#34;, args=args) And finally, for the *.yaml file for configuration i copied vgg_ssd512_coco_trainval35k.yaml in configs folder and rename it to config.yaml. What i changed from that file was the train and test (or more like validation) like in path_catlog.py, the batch size, and num_classes. I changed batch size because my laptop gpu only capable of 4 batch size. Here\u0026rsquo;s an example:\nModel: num_classes: 5 #the __background__ counted ... DATASETS: TRAIN: (\u0026#34;my_custom_train_dataset\u0026#34;, ) TEST: (\u0026#34;my_custom_test_dataset\u0026#34;, ) SOLVER: ... BATCH_SIZE: 4 ... OUTPUT_DIR: \u0026#39;outputs/ssd_custom_coco_format\u0026#39; You don\u0026rsquo;t need to create folder ssd_custom_coco_format, when the training begin the folder gonna created automatically (if the folder didn\u0026rsquo;t exist).\nValidation Configuration First of all, copy coco folder in ssd/data/datasets/evaluation/ and rename it to my_dataset. Rename the def coco_evaluation to def my_dataset_evaluation in file __init__.py. After that, add folder my_dataset to file __init__.py in ssd/data/datasets/evaluation/. For example:\nfrom ssd.data.datasets import VOCDataset, COCODataset, MyDataset ... from .my_dataset import my_dataset_evaluation def evaluate(dataset, predictions, output_dir, **kwargs): ... elif isinstance(dataset, MyDataset); return my_dataset_evaluation(**args) else: raise NotImplementError The Interface For the interface, i\u0026rsquo;m using python library streamlit. Streamlit is more like web interface rather than common graphical user interface (GUI). Here\u0026rsquo;s how the interface looks like:\nThere\u0026rsquo;s a problem with file uploader at the time (streamlit version 0.59.0), i can\u0026rsquo;t upload file larger than 100 mb meanwhile the limit of file uploader should be 200 mb at that time. You can check the issue here , it seems like they\u0026rsquo;re already fixed it but i haven\u0026rsquo;t try it yet. So, when making this project i\u0026rsquo;m using the dropdown menu bar.\nTraining Preparation Before training the model, we need to do some preparation. There\u0026rsquo;re two steps in this process, frame extraction and labeling. Without further ado, let\u0026rsquo;s get started.\nFrame Extraction In this process, i used python library opencv to extract some frame. Here\u0026rsquo;s the script:\nimport cv2 import time from fire import Fire from tqdm import tqdm def main(video_file, path_save, speed): # the lower the speed the fastest the frame_rates, speed = 0 (pause) vidcap = cv2.VideoCapture(video_file) current_frame = 0 speed_frame = speed while (vidcap.isOpened()): success, frame = vidcap.read() # success = retrival value for frame length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) print(f\u0026#39;Current Frame: {current_frame}/{length}\u0026#39;) current_frame += 1 if success == True: cv2.imshow(\u0026#39;Video\u0026#39;, frame) if cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;s\u0026#39;): # press s to save the frame cv2.imwrite(f\u0026#34;{path_save}/frame_{current_frame}.jpg\u0026#34;, frame) elif cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): # press q to quit break elif cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;w\u0026#39;): # play/pause if speed != 0: speed = 0 elif speed == 0: speed = speed_frame else: vidcap.release() cv2.destroyAllWindows() if __name__ == \u0026#39;__main__\u0026#39;: Fire(main) Every time we press s, it\u0026rsquo;s gonna take the current frame at that time. For the speed, i usually go for 25 but if you want slower you could change it to 10 or lower (as long as it\u0026rsquo;s not 0, please).\nAfter the extraction process, i have 652 images/frames for training process. The 652 images/frames have this proportion (There\u0026rsquo;re a few object in one frame):\n   Pavement Distress Object     Alligator Crack 367   Longitudinal Crack 951   Transverse Crack 243   Potholes 161    Labeling For the labeling i use labelme, you could check the tutorial here and to change labelme format to coco dataset format here . There\u0026rsquo;s nothing much to explain about labeling, you just give box to an object and save with the label you want. So, let\u0026rsquo;s move on.\nHere We Go, It\u0026rsquo;s Training Time! For the training process i use google colaboratory (how to use google colaboratory is beyond this post, sorry) but you could also use other services such as paperspace . Here\u0026rsquo;s an example of command line if you use you local machine or cloud services: Local: python train.py --config-file configs/config.yaml Cloud: !python train.py --config-file configs/config.yaml Basically there\u0026rsquo;s no difference so i think it\u0026rsquo;s not that difficult, good luck.\nLoss Function Graph As the training begin, please don\u0026rsquo;t forget to check the loss function. The closer the loss function to zero the better but be carefull so that it doesn\u0026rsquo;t overfitting (a model memorized the training data and have difficulty predicting the testing data). Here\u0026rsquo;s the unscientific tips from me, stop the training process if you don\u0026rsquo;t see any improvement in loss function. For example, if the loss function stuck at 0.9 - 0.5 for quite some time then you should stop the process. Here\u0026rsquo;s my loss function graph:\nTesting Preparation Before testing the model, there\u0026rsquo;re a few things we need to do:\n Copy or move video you want to use into folder input. Copy or move configuration file (*.yaml) into folder configs. Copy or move folder that has training result into folder outputs (in this project the folder name is ssd_custom_coco_format). The folder name must be the same as in configuration file OUTPUT_DIR. If every file and folder in the right places, then let\u0026rsquo;s move on.  It\u0026rsquo;s Testing Time! For this project, there\u0026rsquo;s a problem with the counting. Because i have no idea how to implement tracking so i made the counting in the iteration frame (detection at every frame, which is insane) and that\u0026rsquo;s makes the total counting more than the actual object. To fix this problem (kind of), i do the counting for every 20 frames. The reason was because at every 20 frames, the object detected was closer to the total of actual object than every 10, 15, 25, and 30 frames. So, for the evaluation i\u0026rsquo;m gonna evaluate the detection result every 20 frames. Thanks.\nA Brief Showcase and Explanation of The Results Below is the result:\nVideo Testing 1    Class Name Counting Results Actual Objects     Alligator Crack 2 3   Longitudinal Crack 4 29   Transverse Crack 8 11   Potholes 1 2       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 2 11 0 1   Longitudinal Crack 4 9 1 25   Transverse Crack 6 7 1 5   Potholes 1 12 0 1    Video Testing 2    Class Name Counting Results Actual Objects     Alligator Crack 14 8   Longitudinal Crack 5 6   Transverse Crack 1 4   Potholes 0 2       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 7 3 7 1   Longitudinal Crack 2 8 2 4   Transverse Crack 1 9 0 3   Potholes 0 10 0 2    Video Testing 3    Class Name Counting Results Actual Objects     Alligator Crack 21 8   Longitudinal Crack 7 15   Transverse Crack 1 1   Potholes 2 4       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 8 7 10 0   Longitudinal Crack 4 11 2 11   Transverse Crack 1 14 0 0   Potholes 2 13 0 2    Video Testing 4    Class Name Counting Results Actual Objects     Alligator Crack 23 22   Longitudinal Crack 13 46   Transverse Crack 4 12   Potholes 5 22       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 10 14 8 12   Longitudinal Crack 8 16 3 38   Transverse Crack 2 22 2 10   Potholes 4 20 0 18    From the counting results above we can see that the model struggle to detect longitudinal crack and have a lot of alligator crack detections (detected two times or more). There\u0026rsquo;re 2 reasons for that, the first is that there\u0026rsquo;s not enough small-sized longitudinal crack in training dataset and the second is the frame field-of-view too narrow so that a lot of alligator crack devided into different frames and detected multiple times. Here\u0026rsquo;s an example of that problem:\n Undetected Small Longitudinal Crack:   Multiple Detection of Alligator Crack in Different Frames (there\u0026rsquo;re 2 frames below):  And then, we have the precision and recall of the model as below:\n   Video Precision Recall Accuracy     Video Testing 1 91.43% 46.25% 60.69%   Video Testing 2 50% 36.45% 69.58%   Video Testing 3 77.78% 69.17% 75.45%   Video Testing 4 69.57% 24.42% 53.81%     At the time of making this post, i\u0026rsquo;m still not sure whether to use accuracy or f1-score so for now i\u0026rsquo;m gonna use accuracy.\n The difference between video testing 1 to 4 is the total of small-sized pavement distress and video testing 3 has the least total of small-sized pavement distress of all video. So that means, for this trained weight, we obtain the best accuracy when we have the least small-sized pavement distress.\nConclusion The perfomance of single shot detector is not bad or maybe you could say it\u0026rsquo;s good. Considering the lack of training data, it still can produce above 50% accuracy so you might say that this project has the worst result possible. There\u0026rsquo;re a lot of things you could improve, especially in the amount of training dataset. Good luck!\nFuture Suggestion By no means this is not the best implementation of SSD for pavement distress detection. I have only a few training dataset and a few testing dataset. So you could say what i did here is a minimum requirement that result in minimum performance. You can improve this project quite a lot.\nIf you want to improve this project, you can start from these things:\n Use a lot of training data and testing data. Use a camera that has wide angle lens (because 50mm lens is not wide enough).  Side Notes This is not really important, it\u0026rsquo;s more like a momento for me. In the undergraduate thesis defence(?) there\u0026rsquo;s this examiner who has a misconception about testing process and validation process. That examiner switch the possition of testing process as validation process and validation process as testing process, so that really confusing and we have quite an argument there. I even ask in stackexchange if i\u0026rsquo;m wrong or not and it turns out that examiner has switch the term for testing and validation. Now i feel stupid for having an argument with that examiner.\n  Pytorch maxpool2d .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n SSD: Single Shot Multibox Detector .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "}]