[{"ref":"https://bruhtus.github.io/posts/replacement-for-info-command/","title":"Replacement For Info Command","section":"posts","tags":["Notes","Linux"],"date":"2022.06.11","body":"A Brief Intro Sometime ago, someone posted a tweet about a replacement for rm -rf $HOME/directory that is mv $HOME/directory /dev/null. When i took a look of it, using the ls -l /dev/null command, i found something like this:\n1crw-rw-rw- 1 root root 1, 3 Jun 11 04:21 /dev/null Now, what is c in the crw below\n1crw-rw-rw- 1 root root 1, 3 Jun 11 04:21 /dev/null After going around on internet, i found an answer in stackexchange1. Which explain that c stands for character special file. Please keep in mind that everything in unix-like system is a file, even a directory (or some people called it folder) is a file with type directory.\n The file we usually use has a type of regular file.\n In the stackexchange, someone mentioned about info ls to show the file type with their respective character symbol. That\u0026rsquo;s what pique my interest, the info command.\nReplacement For info Command When i use the command info ls, i noticed that i can\u0026rsquo;t use j and k for navigation, which is disgusting.\nSo, the first thing that i need to figure out is, how to change the pager used in info command. According to this stackexchange answer2, \u0026ldquo;info doesn\u0026rsquo;t use separate pager because it handles navigation\u0026rdquo;. So, basically there\u0026rsquo;s no hope with info command? Probably.\nAnd then, the person who answer on the stackexchange also give a suggestion about pinfo which, at least use j and k as down or up movement. Now i need to read the manpage about pinfo to configure it.\nWhat\u0026rsquo;s Next? Other than trying to configure pinfo, i might need to figure out what is the info documents is all about. This might be a new kind of documentation other than manpage that i can use (if the developer support it). Also, figure out about the character special file type.\n  Stackexchange: The meaning of c in crw-rw-rw- \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Stackexchange: Replacement for info command \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"ref":"https://bruhtus.github.io/posts/vim-statusline/","title":"Guide to Make Your Own Vim/Neovim Statusline","section":"posts","tags":["No one asked","Vim"],"date":"2021.08.20","body":" We don\u0026rsquo;t have to install vim plugin to get a statusline if we don\u0026rsquo;t want to.\n Introduction Hi everyone! In this post I will talk about making your custom statusline in vim. There are a lot of plugins out there that makes vim statusline way better and works out of the box.\n\u0026ldquo;Why would someone going through all the trouble while there\u0026rsquo;s a plugin for that?\u0026rdquo;, you might ask. Well, for me personally, having one less plugin is a good thing. I don\u0026rsquo;t really want to depend on the plugin for something simple such as statusline. If I can build it myself, then I will build it myself rather than using a plugin. Also, it save me a lot of time to figure out which plugin causes the issue if I have less plugin! Oh, another thing, some vim statusline plugin can slow down your startup time (I\u0026rsquo;m looking at you vim-airline) so be careful about that. Alright then, let\u0026rsquo;s get into it!\nRequirements Before we start, we need to prepare a few things:\n Vim/Neovim (we will use vim script or VimL in this post) set laststatus=2 (always display the statusline) Patience (don\u0026rsquo;t be scared when you see an error, calm down!)  If all is set, then let\u0026rsquo;s get started!\nDisable Old Statusline Before we move further, you need to disable or remove your old statusline plugin and the config. If you don\u0026rsquo;t want to remove your old statusline plugin config, you can commented out those line/config in your vimrc or init.vim.\nDifferent statusline for active and inactive window You can have different statusline for active and inactive window by using autocmd event. \u0026ldquo;What is autocmd?\u0026rdquo; you might ask, go take a look at :help autocmd for more info. To make it simple, autocmd is to automatically execute a command on certain event which you can check on :help autocmd-events for more info.\nSo, how can we have different statusline for active and inactive window? First, we need to make a function to define our active or inactive statusline component. You can create the function similar to this:\n1\u0026#34; component for active window2function! StatuslineActive()3\u0026#34; the component goes here4endfunction56\u0026#34; component for inactive window7function! StatuslineInactive()8\u0026#34; the component goes here9endfunction1011\u0026#34; load statusline using `autocmd` event with this function12function! StatuslineLoad(mode)13 if a:mode ==# \u0026#39;active\u0026#39;14\u0026#34; to make it simple, %! is to evaluate the current changes in the window15\u0026#34; it can be useful for evaluate current mode in statusline. For more info:16\u0026#34; :help statusline.17 setlocal statusline=%!StatuslineActive()18 else19 setlocal statusline=%!StatuslineInactive()20 endif21endfunctionand also the autocmd similar to this:\n1\u0026#34; so that autocmd didn\u0026#39;t stack up and slow down vim2augroup statusline_startup3 autocmd!4\u0026#34; for more info :help WinEnter and :help BufWinEnter5 autocmd WinEnter,BufWinEnter * call StatuslineLoad(\u0026#39;active\u0026#39;)6 autocmd WinLeave * call StatuslineLoad(\u0026#39;inactive\u0026#39;)7augroup ENDNow we can compose our statusline component. We can take a look at :help statusline for supported items like for example f for relative path to the file in the buffer. You can choose whatever item you like in your statusline, and put it in the previous function similar to this:\n1function! StatuslineActive()2\u0026#34; if we want to add `f` items in our statusline3 let l:filename = \u0026#39;%f\u0026#39;4\u0026#34; if we want to add \u0026#39;m\u0026#39; items in our statusline5 let l:mod = \u0026#39;%m\u0026#39;6\u0026#34; the `.` is basically to ignore whitespace before and put it right after the previous component7 return l:filename.l:mod8endfunctionWhy we need to do that? well, I\u0026rsquo;ll explain it in next section\nCurrent Mode in Statusline If you want to put your current mode in your statusline, you can do it with a function similar to this:\n1function! StatuslineMode() abort23 let l:currentmode={4 \\ \u0026#39;n\u0026#39;: \u0026#39;N\u0026#39;,5 \\ \u0026#39;v\u0026#39;: \u0026#39;V\u0026#39;,6 \\ \u0026#39;V\u0026#39;: \u0026#39;VL\u0026#39;,7 \\ \u0026#39;^V\u0026#39;: \u0026#39;VB\u0026#39;,8 \\ \u0026#39;s\u0026#39;: \u0026#39;S\u0026#39;,9 \\ \u0026#39;S\u0026#39;: \u0026#39;SL\u0026#39;,10 \\ \u0026#39;^S\u0026#39;: \u0026#39;SB\u0026#39;,11 \\ \u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;,12 \\ \u0026#39;R\u0026#39;: \u0026#39;R\u0026#39;,13 \\ \u0026#39;c\u0026#39;: \u0026#39;C\u0026#39;,14 \\ \u0026#39;t\u0026#39;: \u0026#39;T\u0026#39;}1516 let l:modecurrent = mode()17\u0026#34; use get() -\u0026gt; fails safely, since ^V doesn\u0026#39;t seem to register18\u0026#34; 3rd arg is used when return of mode() == 0, which is case with ^V19\u0026#34; thus, ^V fails -\u0026gt; returns 0 -\u0026gt; replaced with \u0026#39;VB\u0026#39;20 let l:modelist = toupper(get(l:currentmode, l:modecurrent, \u0026#39;VB\u0026#39;))21 let l:current_status_mode = l:modelist22 return l:current_status_mode23endfunctionand put it inside of your statusline function like this:\n1function! StatuslineActive()2 let l:filename = \u0026#39;%f\u0026#39;3 let l:mod = \u0026#39;%m\u0026#39;4\u0026#34; `w:` is basically local variable to current window5\u0026#34; and `l:` is basically local variable to function. For more info :help E1216 let w:mode = \u0026#39;%{StatuslineMode()}\u0026#39;7 return w:mode.l:filename.l:mod8endfunctionNow, if we want to change the current mode background based on the current mode, we can do something like this:\n1\u0026#34; define Normal mode color, Insert mode color, and so on2hi NormalModeColor ctermbg=... ctermfg=... guifg=#... guibg=#...3hi InsertModeColor ctermbg=... ctermfg=... guifg=#... guibg=#...45function! StatuslineActive()6 let l:filename = \u0026#39;%f\u0026#39;7 let l:mod = \u0026#39;%m\u0026#39;8 if mode() ==# \u0026#39;n\u0026#39;9 let w:mode = \u0026#39;%#NormalModeColor#%{StatuslineMode()}\u0026#39;10 elseif mode() ==# v:insertmode11 let w:mode = \u0026#39;%#InsertModeColor#%{StatuslineMode()}\u0026#39;12 endif13\u0026#34; %* is basically to restore highlight to StatusLine highlight group14 return w:mode.\u0026#39;%* \u0026#39;.l:filename.l:mod15endfunctionGit Branch in Statusline If you install vim-fugitive plugin, then you can use fugitive#head() in your statusline like this:\n1function! StatuslineActive()2 let l:filename = \u0026#39;%f\u0026#39;3 let l:mod = \u0026#39;%m\u0026#39;4 if mode() ==# \u0026#39;n\u0026#39;5 let w:mode = \u0026#39;%#NormalModeColor#%{StatuslineMode()}\u0026#39;6 elseif mode() ==# v:insertmode7 let w:mode = \u0026#39;%#InsertModeColor#%{StatuslineMode()}\u0026#39;8 endif9\u0026#34; make sure it doesn\u0026#39;t throw an error if `vim-fugitive` is not installed10 let l:git = \u0026#34;%{exists(\u0026#39;*FugitiveHead\u0026#39;) ? fugitive#head() : \u0026#39;\u0026#39;}\u0026#34;11\u0026#34; to separate left and right side12 let l:sep = \u0026#39;%=\u0026#39;13 return w:mode.\u0026#39;%* \u0026#39;.l:filename.l:mod.l:sep.l:git14endfunctionalternatively, you can use system() command to get the current git branch (for more info :help system()) like this:\n1function! StatuslineActive()2 let l:filename = \u0026#39;%f\u0026#39;3 let l:mod = \u0026#39;%m\u0026#39;4 if mode() ==# \u0026#39;n\u0026#39;5 let w:mode = \u0026#39;%#NormalModeColor#%{StatuslineMode()}\u0026#39;6 elseif mode() ==# v:insertmode7 let w:mode = \u0026#39;%#InsertModeColor#%{StatuslineMode()}\u0026#39;8 endif9\u0026#34; for more info :help E12110 let g:gitbranchcmd = \u0026#34;git branch --show-current 2\u0026gt;/dev/null | tr -d \u0026#39;\\n\u0026#39;\u0026#34;11\u0026#34; use system() if vim-fugitive not installed12 let l:git = \u0026#34;%{exists(\u0026#39;*FugitiveHead\u0026#39;) ? fugitive#head() : system(g:gitbranchcmd)}\u0026#34;13 let l:sep = \u0026#39;%=\u0026#39;14 return w:mode.\u0026#39;%* \u0026#39;.l:filename.l:mod.l:sep.l:git15endfunctionSame Statusline for active and inactive window Now, if we want our statusline to be the same whether in active or inactive window. We can make simplify it by only make one function and not using autocmd. It will look something like this:\n1function! StatuslineComponent()2\u0026#34; your component goes here3endfunction45set statusline=%!StatuslineComponent()and you can use some tips from previous section too!\nConclusion This guide is for do-it-yourself kind of people, so it\u0026rsquo;s only giving some pointer you can use to make your own statusline. I don\u0026rsquo;t want to tell you what to put in your statusline, it is your OWN statusline after all, so you need to know what you want in it. Also, I\u0026rsquo;m not a vim script expert so please forgive me if I miss something. If you have any question regarding this post, feel free to hit me up on twitter (@diawanchris)! See you later!\nReferences  Kade Killary blog post . Junegunn\u0026rsquo;s statusline .  "},{"ref":"https://bruhtus.github.io/posts/ssh-google-colab/","title":"Access Google Colab Through SSH and SSHFS","section":"posts","tags":["Linux","No one asked"],"date":"2021.06.22","body":" Have you ever want a terminal emulator in google colab instead of jupyter notebook? Well, here it is bois!\n Skip-able Part First thing first, i prefer to use vim/neovim instead of jupyter notebook and that\u0026rsquo;s why i prefer to use terminal or terminal emulator to do things as much as possible. I know jupyter notebook has it\u0026rsquo;s own vim emulation, but it\u0026rsquo;s not as good as the OG vim/neovim. Also, i know you can have neovim inside your browser using firenvim plugin , but i\u0026rsquo;m not sure how useful it is and do i really want that? i mean, my IDE is a terminal or terminal emulator. So, unless it\u0026rsquo;s a terminal or terminal emulator then i won\u0026rsquo;t feel comfortable using it.\nThat\u0026rsquo;s why i\u0026rsquo;m trying to figure out how to access google colab through terminal emulator, and it turns out i can do that!\n As far as i know, google colab support terminal emulator but only for pro user. I haven\u0026rsquo;t try it yet, so i might be wrong.\n Requirements To make this work, please make sure that SSH and SSHFS installed on your system. You can install both of them using your distro package manager or something similar (there are a lot of tutorial on how to install SSH or SSHFS).\nA Brief Intro to SSH According to wikipedia , Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network. Typical applications include remote command-line login and remote command execution, but any network service can be secured with SSH.\nTo make it simple, SSH let you access another computer/server and you can use command line interface (CLI) to interact with those computer/server.\nA Brief Intro to SSHFS According to the project github repo , SSHFS allows you to mount a remote filesystem using SFTP.\nTo make it simple, SSHFS let you mount the computer/server storage to your local system and you can treat it like the usual storage on your local system, like using mv and cp command.\nAccess Google Colab Using SSH First thing first, you need to have ngrok account. If you doesn\u0026rsquo;t have ngrok account, then you can register here .\nAfter that, you need to make a new google colab notebook. And then, paste the code below:\n1#CODE 2 3#Generate root password 4import random, string 5password = \u0026#39;\u0026#39;.join(random.choice(string.ascii_letters + string.digits) for i in range(20)) 6 7#Download ngrok 8! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip 9! unzip -qq -n ngrok-stable-linux-amd64.zip 10#Setup sshd 11! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen \u0026gt; /dev/null 12#Set root password 13! echo root:$password | chpasswd 14! mkdir -p /var/run/sshd 15! echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 16! echo \u0026#34;PasswordAuthentication yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config 17! echo \u0026#34;LD_LIBRARY_PATH=/usr/lib64-nvidia\u0026#34; \u0026gt;\u0026gt; /root/.bashrc 18! echo \u0026#34;export LD_LIBRARY_PATH\u0026#34; \u0026gt;\u0026gt; /root/.bashrc 19 20#Run sshd 21get_ipython().system_raw(\u0026#39;/usr/sbin/sshd -D \u0026amp;\u0026#39;) 22 23#Ask token 24print(\u0026#34;Copy authtoken from https://dashboard.ngrok.com/auth\u0026#34;) 25import getpass 26authtoken = getpass.getpass() 27 28#Create tunnel 29get_ipython().system_raw(\u0026#39;./ngrok authtoken $authtoken \u0026amp;\u0026amp; ./ngrok tcp 22 \u0026amp;\u0026#39;) 30#Print root password 31print(\u0026#34;Root password: {}\u0026#34;.format(password)) 32#Get public address 33! curl -s http://localhost:4040/api/tunnels | python3 -c \\ 34\t\u0026#34;import sys, json; print(json.load(sys.stdin)[\u0026#39;tunnels\u0026#39;][0][\u0026#39;public_url\u0026#39;])\u0026#34; What that code does is basically create a connection from google colab back-end with ngrok so that we can access the google colab back-end using SSH and SSHFS. You can check the github gist for more detail.\nAfter that, you can decide whether to mount your google drive to google colab to or not. If you decide to mount your google drive to google colab, then later on you can mount your google drive using SSHFS.\nAfter you setup all of that, run the code. After you run the code, it gonna request you to copy your ngrok authtoken from ngrok dashboard . If there\u0026rsquo;s an error after you input ngrok authtoken, you can just re-run the code.\nAfter that, you can see the Root password and something similar to tcp://n.tcp.ngrok.io:xxxxx (which n and x represent any number from 0-9)\nNext, you can access the google colab using SSH with the following command:\n1ssh -p xxxxx root@n.tcp.ngrok.io If there\u0026rsquo;s a question \u0026ldquo;Are you sure you want to continue connecting?\u0026rdquo;, answer yes. That was to save the SSH address so that you can connect to it again in the future. But, if you use google google colab then you need to make new SSH address because each SSH session restart when google colab restart. That is one of the downside for this method.\nAccess Google Drive in Google Colab If you mount google drive before you activate google colab, then you can access google drive too. You can access google drive in google colab using SSHFS.\nIf you can already access google colab back-end using SSH, then all you need too do is to run this command:\n1sshfs -p xxxxx root@n.tcp.ngrok.io:/content/drive/MyDrive /mnt/colab The command is almost the same as SSH command from before, the difference is that you need to specify which directory you want to mount on the google colab (in above example is google drive which has path /content/drive/MyDrive) and which directory you want to mount it on the local system (in above example is on /mnt/colab).\nYou can choose to mount google drive on any other directory you want, even on your home directory. I usually mount it on /mnt because it\u0026rsquo;s already a habit of mine to mount an external drive to either /mnt or /run/media. And also, make sure the directory you want to mount your drive into is already exist. I already create colab directory beforehand, so i can mount the google drive into /mnt/colab.\nUpside and Downside There\u0026rsquo;s always an upside and downside of things, and this method is also the same. Below is a few upside and downside that i notice when using this method.\nThe Upside  You can use it like the usual terminal or terminal emulator, with familiar command and vim/neovim in it. If you use SSHFS, you can update your script directly from your terminal emulator to google colab through google drive. Basically you can update your script in google drive directly from your terminal emulator.  The Downside  SSH connection with ngrok only last around 40 minutes, so if you need a long time to run your code then you should use the google colab front-end instead. Use SSH and SSHFS to update script instead of running the script. If you have slow internet connection, then it\u0026rsquo;s gonna be a bit of lag when you interact with google colab back-end CLI. SSH session restart when the google colab restart, and SSH session only last for around 40 minutes so you might need to start over again.  Conclusion When you see that the method has a lot more downside rather than upside, you might be thinking \u0026ldquo;why should i use this method?\u0026rdquo;. Well, only use this method if you feel more comfortable using terminal emulator rather than jupyter notebook, other than that, it\u0026rsquo;s not really worth your time.\nUse this method while it lasts, as far as i know, there was a time when this method doesn\u0026rsquo;t work. At the time of writing this post, this method still works but i\u0026rsquo;m not sure if google gonna let this happen for a long time. Anyway, updating the script through SSHFS is really convenient for me.\nReferences  Google colab SSH github gist . How to SSH into google colab medium post . Access your google drive using ssh medium post . Distrotube youtube video on SSHFS .  "},{"ref":"https://bruhtus.github.io/posts/pyv/","title":"Pyv: Minimalist Python Venv Management Tool","section":"posts","tags":["No one asked","Shell","Python"],"date":"2021.04.16","body":" This is a continuation from my previous post (you can check it here ). Long story short, this is a minimalist way to manage your python virtual environment. All you need is python and git.\n What Is Pyv? Here\u0026rsquo;s a brief intro to what is pyv:\n Pyv is a simple shell function that let you manage python virtual environment that decoupled from project directory.\n To put it simply, pyv move the virtual environment to $PYV_dir which by default is $HOME/.cache/pyv\nHow Pyv Manage Virtual Environment Pyv can create, remove, activate, deactivate, and list python virtual environment. Below is the explanation for each action that pyv can do.\nCreate Virtual Environment Pyv create python virtual environment using python default command python -m venv to $PYV_DIR/{given-name}. The given-name can be from git repo name or user input.\nIf the user give pyv command an argument then pyv gonna create python virtual environment with that argument name. For example:\n1pce something-big With the command above, pyv gonna create python virtual environment with the name something-big. pce is pyv command to to create python virtual environment (why would i made something with long command?).\nPyv also can create python virtual environment with git repo name. If you give no argument and just enter pce in a working git repo then pyv gonna create python virtual environment using the git repo name.\n\u0026ldquo;How does pyv do that?\u0026rdquo; you might ask, well, pyv use the command git rev-parse --show-toplevel to get the git root directory name (with a lot of trimming of course). For those who don\u0026rsquo;t know what git root directory, to make simple, git root directory is the directory where you first use git init command or the directory that have .git directory.\nPlease do remember tho, you can only use pce without any argument in git working directory and not in git bare directory. What i mean by git working directory is the normal git repo that has .git directory in it.\nHere\u0026rsquo;s an example of pce without an argument (in case you\u0026rsquo;re still confused):\n1pce Yup, only that. Simple right?\nRemove Virtual Environment After creating virtual environment, how do you delete the virtual environment?\nYou can remove the virtual environment using pre command with or without argument similar to creating virtual environment.\nFor example, we already create something-big python virtual environment. And now we want to remove those virtual environment. All we need to do is something like this:\n1pre something-big Those command gonna invoke the rm command to remove something-big virtual environment directory in $PYV_DIR. So the pre command depends on how you setup your rm command in your shell.\nIf you create virtual environment using the git repo name, and you want to remove those environment, you can use pre without an argument inside those git repo directory (assumming you haven\u0026rsquo;t changed the directory name). How to use the command is the same as when you create the virtual environment, just type\n1pre and you\u0026rsquo;re done.\nActivate Virtual Environment Ok, now you know how to create and remove the virtual environment using pyv. Now, how do you activate those virtual environment?\nBecause we use default python python -m venv command, we need to know how the activate the virtual environment created using those command.\nAccording to python venv documentation , we need to source the activate file in \u0026lt;venv-dir\u0026gt;/bin/activate.\n Did you know that the activate file is also a shell function? Now you know.\n So, the pyv command to activate the virtual environment is pae. Similar to previous command, you can use pae with or without an argument. pae command basically to source activate file in virtual environment directory that located in $PYV_DIR.\nLike previous command, let\u0026rsquo;s say we have something-big virtual env and we want to activate those command. All we need to do is\n1pae something-big or if you create a virtual environment using git repo name (assuming you didn\u0026rsquo;t change the git repo directory name), then you can just use\n1pae without any argument.\nDeactivate Virtual Environment After you know how to create, remove, and activate virtual environment with pyv. Now it\u0026rsquo;s time for you to know how to deactivate virtual environment.\nYou can deactivate the virtual environment either using default deactivate command or using pde. pde is just an alias for deactivate command provided by python venv. You do you.\nPlease keep in mind that you don\u0026rsquo;t need any argument to deactivate virtual environment.\nList Virtual Environment The last thing is how to list all the virtual environment that available?\nTo list all the virtual environment that ever created, you can use pve command without an argument or ls -l $PYV_DIR. pve is just an alias for ls -l $PYV_DIR.\nConclusions There\u0026rsquo;s always an upside/downside to a project, and this project is no exception.\nThe upside is that, it\u0026rsquo;s minimal. If you didn\u0026rsquo;t do anything fancy with your python virtual environment then pyv probably gonna fit your need.\nThe downside is also \u0026ldquo;it\u0026rsquo;s minimal\u0026rdquo;. If you use default options that come with default python venv command, then you can\u0026rsquo;t do that with pyv. Pyv only able to handle python -m venv \u0026lt;env-directory\u0026gt;. That\u0026rsquo;s all.\n At the time of writing this article, i\u0026rsquo;ve only tested pyv in bash and zsh. I might provide support for fish, csh/tcsh in the future.\n References  A few alternative to manage python virtual environment . Python venv documentation .  "},{"ref":"https://bruhtus.github.io/posts/split-up-vimrc/","title":"Split Up Vimrc","section":"posts","tags":["Vim"],"date":"2021.03.22","body":" If you\u0026rsquo;re the type of person who like to place all your source code in one file, then this article is not for you. But, if you\u0026rsquo;re the type of person who like to split up your source code into a few sub-modules, then this article is for you.\n Skip-able Part When i look at my vimrc (vimrc is a vim config file for those who don\u0026rsquo;t know), i always feel confused where should i add new configuration for new plugin. I want to organize my vimrc so that it is easier to maintain and adding stuff, and that\u0026rsquo;s where the problem comes in.\nOvertime, when you keep adding configuration to vim, slowly your vimrc gonna become a huge mess and at some point it gonna feels cumbersome to access your vimrc. And that\u0026rsquo;s why i thought \u0026ldquo;can i split up my vimrc so that i don\u0026rsquo;t feel overwhelm every time i want to add new configuration or mapping to vim?\u0026rdquo; and it turns out i can.\nUpside and Downside Before we move on, you need to know the upside and downside for split up your vimrc that i\u0026rsquo;ve found. Here we go.\nUpside  Easier to manage, you can have a dedicated config file for every plugin you have Doesn\u0026rsquo;t overwhelm you with the piled up line of text, because it\u0026rsquo;s on separate file You can take a part of your config such as defaults setting and mappings, rather than a whole vim config (useful for accessing server with ssh)  Downside  You need to use grep or something similar if you want to check if the mapping already exist or not. If you didn\u0026rsquo;t split up your vimrc, you can just use vim built-in search function. You need to download a lot of file if you want to have full experience of your vim.  Split Up Vimrc If you still want to go on despite the downside, then it\u0026rsquo;s time to split up your vimrc.\nFirst thing first, you need to know that vim has runtime path which gonna be loaded everytime you start vim. And we can use the default runtime path to our advantage. For the full list, you can check here , but to make it simple, we\u0026rsquo;re only gonna use ~/.vim/plugin (for vanilla vim) or ~/.config/nvim/plugin (for neovim) directory. Every file in those directory get loaded every time you open vim, so you can add your config in those directory.\nFor example, you can move your defaults config such as set number relativenumber into file defaults.vim and place it in ~/.vim/plugin (for vanilla vim) or ~/.config/nvim/plugin (for neovim) and defaults config such as number and relativenumber gonna get loaded everytime you open vim.\nYou can also make config file for your plugin and give it the plugin name such as fugitive.vim and place it into ~/.vim/plugin (for vanilla vim) or ~/.config/nvim/plugin.\nThe Conclusion It\u0026rsquo;s quite easy to manage vim if you split up your vimrc, but it comes back to your personal preference. This article is just to remind vim user that they can split up their vimrc if they want to.\nReferences  Manage plugin in dark ages .  "},{"ref":"https://bruhtus.github.io/posts/python-venv/","title":"Managing Python Virtual Environment","section":"posts","tags":["No one asked","Python"],"date":"2021.03.02","body":" Have you ever want python virtual environment that decoupled from the project directory like conda but not actually conda (dejavu)? That\u0026rsquo;s what this post is about, a simple way to manage a python virtual environment similar to how conda manage virtual environment without python version management.\n A Brief Intro to Python Virtual Environment To makes thing simpler, python virtual environment is a self-contained directory tree that contains a python installation for a particular version of python plus a number of additional packages.\nWith virtual environment you can minimize the conflicting requirements for each python script you made. For example, application A can have its own virtual environment with python package at version 1.0 installed and application B can have its own virtual environment with python package at version 2.0. If applicatin B need to upgrade the python package to version 3.0 then this will not affect application A\u0026rsquo;s environment with python package at version 1.0.\nLet\u0026rsquo;s Get Started In this post we\u0026rsquo;re only going to use the default python venv to create a virtual environment, the command is something like this:\n1python -m venv \u0026lt;directory-name\u0026gt; In case you\u0026rsquo;re wondering, \u0026ldquo;if we only gonna use the default python command, then what\u0026rsquo;s so special about it?\u0026rdquo;. Let me tell you this, what special about the default python venv command is that we can specify the path of the directory and we can enhance that with a shell script. Please keep in mind that this is a minimalist approach to manage python virtual environment without installing other tools except python and git (we\u0026rsquo;ll get to that later).\nCreate Python Virtual Environment Here\u0026rsquo;s how we make a simple shell script to manage our python environment. First thing first, you should decide where you want all your virtual environment located. If you use miniconda, usually it\u0026rsquo;s in miniconda3/envs/ directory or something along those path, i forgot. I personally want to place my python virtual environment in .cache/python-venv directory because i rarely check my .cache directory and the virtual environment not gonna disturb other shell scripts i have.\nAfter that, we decide whether we give the name to virtual environment ourself or just use git repo root name. Like i told you before, we can use git repo to decide the name of our virtual environment. To make things simpler, what i mean git repo root is the directory where you use command git init to initialize git repo.\nFor example, if you use git init command in nganu directory then you can have your virtual environment named nganu without you enter any name, but if you didn\u0026rsquo;t want to use your git repo root name then you can also insert the name you want, similar to conda create -n \u0026lt;name-env\u0026gt;. With that brief intro, here\u0026rsquo;s the code:\n1#!/bin/sh 2 3gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; 4trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; 5projectdir=\u0026#34;${trim##*/}\u0026#34; 6venvdir=$HOME/.cache/python-venv 7 8mkdir -pv $venvdir 9 10if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then 11\tpython -m venv $venvdir/$1 2\u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Created $1python venv\u0026#34; 12 13elif [ \u0026#34;$projectdir\u0026#34; != \u0026#34;\u0026#34; ]; then 14\tpython -m venv $venvdir/$projectdir 2\u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Created $projectdirpython venv\u0026#34; 15 16else 17\techo \u0026#34;Not git repo, please insert a name for virtual env (for example: pce nganu)\u0026#34; 18 19fi We can get git repo root path using the command git rev-parse --show-toplevel but the problem is, those command give the full path to git repo but what i want is only the name of the git repo root so we need to trim the full path and only give the directory name we want. That\u0026rsquo;s what trim and projectdir in those code did. So the name of the git repo dir is in projectdir variable.\nAfter that, venvdir is for the path you want to save all your virtual environment and the command mkdir -pv $venvdir is to make sure if the directory doesn\u0026rsquo;t exist then it\u0026rsquo;s gonna create the directory.\nThe \u0026quot;$1\u0026quot; != \u0026quot;\u0026quot; to make sure if no argument is given then don\u0026rsquo;t create any virtual environment and skip to next statement, but if there\u0026rsquo;s an argument then make the virtual environment with the argument as the name.\nThe \u0026quot;$projectdir\u0026quot; != \u0026quot;\u0026quot; to make sure if it\u0026rsquo;s not a git repo then don\u0026rsquo;t create any virtual environment and skip to next statement, but if it\u0026rsquo;s a git repo then make the virtual environment with the git repo root as the name.\nLet\u0026rsquo;s say we save those script with the name pce (python create env), you can choose different name, you do you. And don\u0026rsquo;t forget to place those script into your PATH so you can use that without typing the full path. With that in mind, here\u0026rsquo;s a few scenario we can do:\n Without any argument, if it\u0026rsquo;s git repo then it\u0026rsquo;s create virtual env with git repo root name and if it\u0026rsquo;s not git repo then it\u0026rsquo;s not gonna create any virtual env.  1pce With an argument, it\u0026rsquo;s gonna create virtual env with the argument as virtual env name.  1pce nganu #create nganu virtual env Remove Python Virtual Environment After creating virtual environment, we can also delete virtual environment using a shell script. The concept is similar to create virtual environment, the difference is if there\u0026rsquo;s no virtual environment with the same name as user input or git repo root, then the script not gonna delete anything. With that in mind, here\u0026rsquo;s the code:\n1#!/bin/sh 2 3gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; 4trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; 5projectdir=\u0026#34;${trim##*/}\u0026#34; 6venvdir=$HOME/.cache/python-venv 7 8if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then 9\trm -r $venvdir/$1 2\u0026gt; /dev/null || \\ 10\techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; 11 12elif [ \u0026#34;$projectdir\u0026#34; != \u0026#34;\u0026#34; ]; then 13\trm -r $venvdir/$projectdir 2\u0026gt; /dev/null || \\ 14\techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; 15 16else 17\techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; 18 19fi Activate and Deactivate Python Virtual Environment For activating the virtual environment, i use shell function and then source that function with your shell config, like .bashrc or .zshrc or something else.\nI did that because i can\u0026rsquo;t really source inside a shell script (it\u0026rsquo;s just gonna create a subprocess and didn\u0026rsquo;t effect the current shell, so you can\u0026rsquo;t activate the virtual environment).\nThe concept is similar to create and remove script above, if the virtual environment name not found then give message \u0026ldquo;Python virtual environment not found, create new one\u0026rdquo;, the difference is that we\u0026rsquo;re sourcing the virtual environment to activate it. With that in mind, here\u0026rsquo;s the code:\n1function pae(){ 2\tlocal gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; 3\tlocal trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; 4\tlocal projectdir=\u0026#34;${trim##*/}\u0026#34; 5\tlocal venvdir=$HOME/.cache/python-venv 6 7\tif [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then 8\t. $venvdir/$1/bin/activate 2\u0026gt; /dev/null || echo \u0026#34;Python virtual environment not found, create new one\u0026#34; 9 10\telse 11\t. $venvdir/$projectdir/bin/activate 2\u0026gt; /dev/null || echo \u0026#34;Python virtual environment not found, create new one\u0026#34; 12 13\tfi 14} To deactivate the virtual environment, you just need to use deactivate command. And to list all the virtual environment you made, you can use ls command. For example, ls ~/.cache/python-venv (or the path you choose to hold all your virtual environment).\nThe Conclusion What i mean \u0026ldquo;simple\u0026rdquo; earlier is not \u0026ldquo;easier to setup\u0026rdquo;, what i mean is you only need the tools that you probably already have and make a workflow with only that tools to be similar to how conda handles virtual environment.\nThis post is only an alternative way to manage python virtual environment because most python virtual environment tools usually make the virtual environment inside the project directory and i don\u0026rsquo;t really like that, i prefer how conda manage virtual environment but i think conda is overkill to only manage virtual environment and that\u0026rsquo;s why i make those shell script and function to behave the same as conda (without python version control yet).\nI may make this a standalone project if i ever know how to make the installer. And also please keep in mind that this was based on my needs so maybe there\u0026rsquo;s some glitch that i haven\u0026rsquo;t found yet. Ok that\u0026rsquo;s all, hopefully you gain something from this (probably not, like always).\nReferences  A few python virtual environment tools . Python virtual environments documentatton .  "},{"ref":"https://bruhtus.github.io/posts/arch-installation-guide/","title":"Arch Linux Installation Guide (Feb 2021)","section":"posts","tags":["Linux"],"date":"2021.02.23","body":" Don\u0026rsquo;t install arch linux, install TempleOS instead. All hail HolyC! (R.I.P Terry). Jokes aside, please keep in mind that this guide might become obsolete because the nature of rolling release so you should always check the arch wiki installation guide .\n A Brief Intro I\u0026rsquo;m trying to install an OS as minimal as possible but not from scratch like Linux From Scratch or compiling everything like Gentoo . I might try to install both of them but i don\u0026rsquo;t think i\u0026rsquo;m gonna use one of them as my main OS.\nPre-Installation First thing to do is download the iso from here and make bootable usb drive. After that check if your machine using legacy BIOS or UEFI by doing ls /sys/firmware/efi/efivars. If it shows directory without error then the system is booted in UEFI mode, and if the directory doesn\u0026rsquo;t exist then the system may be booted in legacy BIOS (or CSM). Most modern computer usually using UEFI but please check first because it could save you a lot of time during the installation.\nFor this guide, i\u0026rsquo;m installing arch linux on machine with UEFI so i won\u0026rsquo;t cover the legacy BIOS installation here.\nInstallation Here\u0026rsquo;s step-by-step arch linux installation\nLive Environment After booting into live environment, there\u0026rsquo;re a few things you need to do. Below is the things you need to do inside live environment.\nSet Keyboard Layout The default keyboard layout is US so if you want to use US keyboard layout then you can skip this step. You can check available layouts using this command\n1ls /usr/share/kbd/keymaps/**/*.map.gz and to modify the layout you can use loadkeys. For example, to set a german keyboard layout you can do\n1loadkeys de-latin1 Verify The Boot Mode If you already know which one your machine use (legacy BIOS or UEFI), then boot into the right boot mode. You can verify the boot mode by using the same command\n1ls /sys/firmware/efi/efivars Connect To The Internet At the time of writing this guide, the installation ISO using IWD (Internet Wireless Daemon) to connect to wifi. You can check if you have internet connection or not by doing this\n1ping google.com #or any website you like such as pornhub.com If there\u0026rsquo;s no internet connection, you can do:\n If you\u0026rsquo;re using ethernet, plug in the cable. If you\u0026rsquo;re using wifi, you can use iwctl (IWD command).  Here\u0026rsquo;s the basic command after you use iwctl (don\u0026rsquo;t forget to press enter to execute the command):\n1device list #to check the device name 2station \u0026lt;device-name\u0026gt; scan #to scan the wifi 3station \u0026lt;device-name\u0026gt; get-networks #to check the available networks 4station \u0026lt;device-name\u0026gt; connect \u0026lt;wifi-name\u0026gt; #and then enter the password if there\u0026#39;s any 5exit #exit iwctl After that check again using the ping command.\nUpdate The System Clock After there\u0026rsquo;s an internet connection, you can update the system clock by using\n1timedatectcl set-ntp true to check the service status, use timedatectl status\nPartition The Disks  This is for UEFI, for legacy BIOS please check arch wiki .\n To check the disks that available, you can use lsblk command and pick which disk you want to install arch linux on. The disk start with sd and without a number at the end like sda\nFor the disk partitions, i use cfdisk command. For example, cfdisk /dev/sda (replace sda with the disk you want to use).\n I didn\u0026rsquo;t make swap partition so if you want to make one please refer to arch wiki .\n The first partition is bootloader, usually around 120M and 512M (M means megabytes) depends on what bootloader you want to use. And then change the partition type to EFI System.\nThe second partition is root directory. I use everything that\u0026rsquo;s left but you can spare some for another partition. And then change the partition type to Linux Filesystem.\nAfter you\u0026rsquo;re sure about your disk partitions, then write the changes and then quit.\nFormat The Partitions The next step is format the disk partition. For bootloader partition, format it to FAT32 by using this command\n1mkfs.fat -F32 /dev/\u0026lt;efi-system-partition\u0026gt; For root directory, format it to ext4 by using this command\n1mkfs.ext4 /dev/\u0026lt;root-partition\u0026gt; For example:\n1mkfs.fat -F32 /dev/sda1 2mkfs.ext4 /dev/sda2 Mount The Filesystems  More info please refer to EFI System Partition Arch Wiki . The next step is mount the partition that we\u0026rsquo;ve made. First, mount root partition to /mnt using this command\n 1mount /dev/\u0026lt;root-partition\u0026gt; /mnt after that create two directories, /mnt/boot and /mnt/boot/efi (basically create efi directory inside boot directory which both of them doesn\u0026rsquo;t exist so you need to create one by one). You can create both directories using this command\n1mkdir -p /mnt/boot 2mkdir -p /mnt/boot/efi after that mount your bootloader to /mnt/boot/efi using this command\n1mount /dev/\u0026lt;efi-system-partition\u0026gt; /mnt/boot/efi For example:\n1mount /dev/sda2 /mnt 2mount /dev/sda1 /mnt/boot/efi Install Essential Packages You can install essential package by using this command\n1pacstrap /mnt base base-devel linux linux-firmware vim  /mnt is where you mount your root partition. base is base packages. base-devel is development tool such as sudo and grep. linux is the kernel. linux-firmware is the necessary part of linux kernel. vim is the text editor (i don\u0026rsquo;t even install nano, sorry).  Generate Fstab  Fstab is for telling the kernel what to load in the booting process.\n To generate fstab use this command\n1genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab -U means using unique ID instead of human readable ID such as sda. You can check the content of fstab by opening /mnt/etc/fstab file using your text editor (in my case is vim, so i use vim /mnt/etc/fstab).\nChange Root Into New System To take a look inside our new system, we can change root to our new system by using this command\n1arch-chroot /mnt Configure New System After setup our new system in live environment, now we need to configure our system inside the system (not in live environment anymore) after change root into our new system. There\u0026rsquo;re a few things you need to configure, below is the things you need configure inside new system.\nSet The Time Zone To set the time zone, we can use this command\n1ln -sf /usr/share/zoneinfo/Region/City /etc/localtime For example:\n1ln -sf /usr/share/zoneinfo/Asia/Jakarta /etc/localtime You can check the region or the city by doing this\n1ls /usr/share/zoneinfo/Region and\n1ls /usr/share/zoneinfo/Region/City or just double \u0026lt;tab\u0026gt; while typing ln -sf /usr/share/zoneinfo/\nAfter that, generate the clock using hwclock with this command\n1hwclock --systohc Select Your OS Language First, you need to edit etc/locale.gen using your favorite text editor (which is not nano for me) and uncomment the language you want.\nFor example, you want to use US English as your OS language then what you need to uncomment is en_US.UTF-8 UTF-8 and en_US ISO-8859-1.\nAfter that, use this command to generate locales\n1locale-gen and then create the locale.conf file. For example:\n1vim /etc/locale.conf #you can replace vim with your favorite text editor the content of locale.conf is\n1LANG=en_US.UTF-8 #if you choose US English If you set the keyboard layout (at the beginning of installation), then you make the changes persistent in vconsole.conf. For example:\n1vim /etc/vconsole.conf with the content\n1KEYMAP=de-latin1 Network Configuration For the network configuration, create the hostname file with the name you want (it\u0026rsquo;s not the user name, it\u0026rsquo;s more like your machine name). For example:\n1vim /etc/hostname with the content\n1bruhtus After that, add this content to /etc/hosts\n1127.0.0.1\tlocalhost 2::1\tlocalhost 3127.0.1.1\t\u0026lt;yourhostname\u0026gt;.localdomain\t\u0026lt;yourhostname\u0026gt; For example: The previous step i define my hostname as bruhtus, so what i need to do is add this content to /etc/hosts\n1127.0.0.1\tlocalhost 2::1\tlocalhost 3127.0.1.1\tbruhtus.localdomain\tbruhtus  If the system has a permanent IP address, it should be used instead of 127.0.1.1.\n After that, install network manager using this command pacman -S networkmanager wpa_supplicant and then activate on startup using systemctl enable NetworkManager and systemctl enable wpa_supplicant.\nSet The Root Password To set the root (or admin) password, use this command\n1passwd and then enter your password.\nAdd New User  The default user is only root (or admin).\n To create new user, use this command\n1useradd -m \u0026lt;username\u0026gt; after that create the password for that username by using this command\n1passwd \u0026lt;username\u0026gt; and then enter your password.\nFor example:\n1useradd -m bruhtus #i like my hostname and my username to be the same 2passwd bruhtus and then enter the password.\nAdd New User To Group  It\u0026rsquo;s so that the new user can have root access using sude\n To add new user to group, we can use this command\n1usermod -aG wheel,audio,video,optical,storage \u0026lt;username\u0026gt; Edit Sudoers File  Add the wheel group to have root access with sudo\n Use visudo and look for wheel and then uncomment those line.\nThe line would be like this\n1%wheel ALL=(ALL) ALL By default, visudo using vi to edit the file. If you didn\u0026rsquo;t want to use vi then you can do\n1EDITOR=vim visudo Install Bootloader  In my case, i use grub as the bootloader. So, i\u0026rsquo;m just gonna cover grub installation in this section.\n First, install the bootloader and efibootmgr using this command\n1pacman -S grub efibootmgr After that, use this command to install grub on system partition\n1grub-install /dev/\u0026lt;disk-partition\u0026gt; For example:\n1grub-install /dev/sda #without any number at the end If there\u0026rsquo;s no error, then generate grub config using this command\n1grub-mkconfig -o /boot/grub/grub.cfg  If there\u0026rsquo;s an error, please check here or use search engine. I can\u0026rsquo;t cover every single error.\n Reboot If there\u0026rsquo;s no error, exit from chroot using exit command. After that, unmount the system using umount -R /mnt and then reboot (please make sure the disk partition is priority in bios).\nGraphical User Interface If you only use command line interface, then you don\u0026rsquo;t need to continue. But, if you still need graphical user interface (GUI) to open browser or something like that, then keep going.\nLogin To Your Arch Installation After reboot, login to your user you just created (in my case, it\u0026rsquo;s bruhtus). After that check the internet connection by using ping command (for example: ping google.com). If there\u0026rsquo;s an error then connect to your network using nmtui command. After that check the connection using ping command again. If there\u0026rsquo;s still an error, please check arch wiki or use your search engine.\nInstall And Configure Graphical Environment Install a few package using this command\n1sudo pacman -S xorg xorg-init alacritty lightdm lightdm-gtk-greeter lightdm-gtk-greeter-settings i3-gaps  xorg and xorg-init is for graphical interface. alacritty is the terminal emulator i choose (you can replace that with your favorite terminal emulator). lightdm, lightdm-gtk-greeter, and lightdmn-gtk-greeter-settings is for login manager (unless you want to start your graphical environment manually like a champ, then you should probably install one). i3-gaps is the window manager i choose (you can replace that with your favorite window manager).   Please keep in mind that i use standalone window manager so i need to install xorg and xorg-init manually, but if you choose desktop environment then i think you don\u0026rsquo;t need to install those two.\n After everything installed, copy the xinitrc using this command\n1cp /etc/X11/xinit/xinitrc ~/.xinitrc What that command do is copy xinitrc to your home directory. What .xinitrc do is start the window manager or desktop environment you choose (we\u0026rsquo;ll get to that later in this post).\nThe next step is to edit .xinitrc, you can do the following step:\n Remove the last 5 row (from twm \u0026amp; until exec xterm -geometry 80x66+0+0 -name login) And then add exec i3 in the last row (basically replace the last 5 row with this exec i3).   Twm is default xorg window manager, in case you\u0026rsquo;re wondering.\n Please keep in mind that i use i3 window manager so i use exec i3. To makes thing simpler, exec is where you start your window manager or desktop environment so the usual syntax would be like this exec \u0026lt;window-manager\u0026gt;.\nAfter that, enable login manager by using this command:\n1sudo sysmtemctl enable lightdm The last step is to check if there\u0026rsquo;s an error or not by using this command startx to start xorg. If everything fine (you can access your window manager or desktop environment) then reboot.\n For intel user, if there\u0026rsquo;s an error while using startx, you can try installing xf86-video-intel first and see if that fix the error or not. If that doesn\u0026rsquo;t fix your error then you know what to do.\n The Conclusion That\u0026rsquo;s all you need to do to install arch linux. After that you can install all the program you want. I suggest you to try it first inside virtual machine such as virtualbox, virt-manager or something else.\nReferences  Arch wiki installation guide . Arch wiki grub installation . Arch wiki mount EFI system partition . Mental outlaw arch installation video part 1 . Mental outlaw arch installation video part 2 . Iwd usage video . Distrotube arch installation video part 1 . Distrotube arch installation video part 2 .  "},{"ref":"https://bruhtus.github.io/posts/linux-ownership-permissions/","title":"Linux Ownership and Permissions","section":"posts","tags":["Linux"],"date":"2021.02.06","body":" How to change ownership and permission on linux (and probably other unix-like system? i\u0026rsquo;m not sure). Plain boring and important.\n A Brief Intro File/directory ownership is basically who owns the file/directory, for example, if we create a file/directory as admin or root in this case and if we want to edit those file/directory as user then we need to change the file/directory ownership because that file/directory owned by the root and we have no access to change that file/directory.\nTo check the ownership and permission you can use ls -l, to make it simpler, -l means long format and the file ownership on column 3 (username) or 4 (group). Below is the example output when you run ls -l command:\nAs you can see from example above, most of the files owned by bruhtus user and only one file owned by root (the admin) so if we want to change that file then we need to change the ownership or access root privilege everytime we want to change that file (which is such a pain). Your choice.\nThere\u0026rsquo;re 10 character on the most left side or the first column of the ls -l result, let\u0026rsquo;s take a look on nganu file as an example. On the most left side of nganu there\u0026rsquo;re this squence -rw-r--r--. The - means that there\u0026rsquo;s no permission for that file or directory, or if it on the most left then it means it\u0026rsquo;s not directory (or folder). So if nganu is directory rather than a file then the squence gonna be like this drwxr-xr-x.\nr means that the file or directory has read permission (we can take a look at it), w means that the file or directory has write permission (we can edit the file), and lastly is x which means that the file or directory is executable (we can run the file). Usually if we make a folder then it\u0026rsquo;s already have executable permission.\nHow To Change Ownership Change ownership is quite simple, the format is like this\n1chown user:group \u0026lt;file\u0026gt; if you want to change ownership from or to root (or admin) then you need to access root privilege. Take the example from the image above, if i want to change ownership of file nganu from root to bruhtus user, then what i need to do is\n1sudo chown bruhtus:bruhtus nganu Please keep in mind that the user is on column three and the group is on column four on ls -l result if you\u0026rsquo;re unsure about the user and the group. chown command is not only for file but also for directory (or folder).\nIf you want to change all the files within the directory (not only the directory itself), then you can do\n1chown -R user:group \u0026lt;directory\u0026gt; For example, let\u0026rsquo;s assume that nganu is a directory with a file anu and owned by root like this\nIf we want to change ownership of the nganu directory and anu file from root to bruhtus user, then what we need to do is\n1sudo chown -R bruhtus:bruhtus nganu Please keep in mind that we need sudo because the previous owner is root (or admin), if the previous owner isn\u0026rsquo;t root then we don\u0026rsquo;t need sudo.\nHow To Change Permission The permission section has 10 character like i explained above, here\u0026rsquo;s the breakdown of those permission:\n   Directory Owner Group Others     d or - rwx or --- rwx or --- rwx or ---     r or read is worth 4 points w or write is worth 2 points x or executable is worth 1 points  So if you have read, write, and execute permission of the owner, group, and others then it\u0026rsquo;s gonna be 777. Here\u0026rsquo;s the example:\nfile anu only has read and write permission for the owner, and only read permission for group and others (or everyone else), in this case it\u0026rsquo;s worth 644. Why is this important? because we can use that to change file or directory permission precisely.\nSo if you want to change anyfile to have the same arrangement as file anu then you can do\n1chmod 644 \u0026lt;file\u0026gt; if we want to change those file to have execute permission on owner, group, and others then we can do this\n1chmod 755 \u0026lt;file\u0026gt; or if you want everyone to have the same permission you can use +, like this one (have execute permission on owner, group, and others)\n1chmod +x \u0026lt;file\u0026gt; or you want to specify one of them (either owner/user, group, or others) you can do one of this\n1chmod u+x \u0026lt;file\u0026gt; #execute permission only for user/owner 2chmod g+x \u0026lt;file\u0026gt; #execute permission only for group 3chmod o+x \u0026lt;file\u0026gt; #execute permission only for others 4chmod ug+x \u0026lt;file\u0026gt; #execute permission only for user/owner and group 5chmod a+x \u0026lt;file\u0026gt; #similar to +x, give user/owner, group, and others execute permission You can also change permission recursively like change ownership, by doing this\n1chmod -R u+x \u0026lt;directory\u0026gt; or\n1chmod u+x -R \u0026lt;directory\u0026gt; The Conclusion That\u0026rsquo;s all about changing ownership and permission on linux. Hopefully that\u0026rsquo;s useful (probably not, optimistic).\nReferences  DistroTube Video .  "},{"ref":"https://bruhtus.github.io/posts/git-repo-backup/","title":"Git Repository Backup","section":"posts","tags":["Git"],"date":"2021.01.18","body":" Have you ever thought that some day your git remote repository service account got banned or even their server went down? It always crossed my mind and if that happen i would lose all my source code. So in this post i\u0026rsquo;m gonna explain how to backup your git repository locally and to other git remote repository services.\n Skip-able Part Git is a version control system for tracking any changes in any set of files. It\u0026rsquo;s basically a tool that makes tracking changes on your code easier, so you would know which line fix a certain bug and so on.\nThere\u0026rsquo;re a lot of git remote repository such as github, gitlab, bitbucket, codeberg and so on. The most popular one is github, but like other platform, github could decide whether to let you continue in their platform or banned you in their platform. You can check an article about github banned developer account on here and it seems like github in the process of restoring access to all developer in iran, you can check the github CEO tweet here .\nYou might be thinking \u0026ldquo;well, i\u0026rsquo;m not on those country so why should i worry about that?\u0026rdquo;. My point is not about the country that got banned, but about what would you do if your account got banned and all access to you code was gone. We can\u0026rsquo;t take anything for granted these days so better start backup your git repository rather than regret it later.\nBackup Git Repository First of all, there\u0026rsquo;re two ways you can backup a git repository that i knew. The first one is to backup locally in your machine, and the second one is to backup on multiple platforms or even on your own server.\nBackup Git Repository Locally There\u0026rsquo;re two ways to backup your git repository locally on your local machine, that is clone everything (basically the normal git clone command) and clone git bare.\nClone Everything in Git Repository You can backup every file in your git repository by doing the normal git clone \u0026lt;git-url.git\u0026gt; command. By doing that, you download every file in your repository with their actual size and you can do git workflow in it.\nIt\u0026rsquo;s basically the usual way to clone a git repository.\nClone Git Bare You can backup only the git bare from a git repository. To make it simpler, git bare is a .git directory when you initialize a directory as a git repository using git init command.\nSo if you clone the git bare instead of the full git repository, you can get all commit history and all the branch on your git repository without actually downloading all your file so the git bare size is quite small.\nYou might ask \u0026ldquo;if you didn\u0026rsquo;t download all your file then how you called that a backup?\u0026rdquo;. Ok so here\u0026rsquo;s the thing, by cloning a git bare you backup all your commit history and if you decide to upload your git repository to another git services or even your own server, you just need to push those git bare backup and git gonna create all your file with all the commit history on the new git repository. So basically you have your git repository backup without taking too much space on your local storage.\nSo, how can you backup using git bare clone? git have a backup mechanism but it\u0026rsquo;s not obvious, git has a --mirror flag that you can use to backup or push a git bare repository.\nFirst, you need to clone a git bare from git repository by doing\n1git clone --bare \u0026lt;git-repo-url.git\u0026gt; or\n1git clone --mirror \u0026lt;git-repo-url.git\u0026gt;  Both of the commands are basically the same, you can push using --mirror flag or add git remote repository and then push using the usual command. How to push with git bare repository is later in this post.\n Here\u0026rsquo;s an example, if i want to clone my instasaver git bare then the command i need is\n1git clone --bare https://github.com/bruhtus/instasaver.git or\n1git clone --mirror https://github.com/bruhtus/instasaver.git and then it\u0026rsquo;s gonna make instasaver.git directory where it stores all the commit history and branch.\nIf your git repository contains Git Large File Storage (LFS) objects then you need to download those git LFS objects too. First, you clone git bare with one of the command above (--mirror flag or --bare flag) and then download git LFS object with this command\n1git lfs fetch --all  Don\u0026rsquo;t forget to change directory (cd) into your git bare directory before applying those command.\n Backup Git Repository on Multiple Platforms There\u0026rsquo;re two ways you can backup your git repository on multiple platforms. First, you can use git remote add command, and second, you can push git bare repository into another git services. Uploading into your own server is beyond the scope of this post, so i\u0026rsquo;m not gonna explain about that here.\nAdd Git Remote Repository With git remote add you can add another platform so that you can also push into that platform.\nFor example, your main git repository service is github so you usually do\n1git remote add origin \u0026lt;github-repo-url.git\u0026gt; in your local git repository. That command is basically assign origin as your github repository, so if you want to push into your github repository you need to do\n1git push origin \u0026lt;branch\u0026gt; if you didn\u0026rsquo;t like origin as your github repository alias, you can change it to whatever you want, you can change it into github or even anu but you also need to change your git push command like this\n1git push github \u0026lt;branch\u0026gt; or\n1git push anu \u0026lt;branch\u0026gt; So if you want to add another platform and you have your full git working directory (not git bare version), you just need too assign another git repository service url.\nFor example, i want to add instasaver to gitlab. I just need to do\n1git remote add gitlab https://gitlab.com/bruhtus/instasaver.git and then if i want to push the changes to master branch, i just need to do\n1git push gitlab master other git workflow such as git add and git commit is the same.\n Don\u0026rsquo;t forget to change directory (cd) into your local git repo directory.\n Push Git Bare If you alreaady backup your git bare repository on your local machine, then all you gotta do is change directory (cd) into your git-bare-repo.git directory and then do\n1git push --mirror \u0026lt;new-git-repo-url.git\u0026gt; or you could also do\n1git remote add \u0026lt;platform\u0026gt; \u0026lt;new-git-repo-url.git\u0026gt; 2git push \u0026lt;platform\u0026gt; \u0026lt;branch\u0026gt;  You can change \u0026lt;platform\u0026gt; name to whatever you want such as anu or nganu, you do you.\n The difference between both command is that you can get all the branch if you use git push --mirror meanwhile you only get main branch if you use the usual git push command (without --mirror flag). So if you want to backup all the branch then you should probably use git push --mirror.\nThe disadvantage of using git bare is that you cannot use git workflow such as git add or git commit, you can only use git push command. If you want to update your git bare repository, you can do\n1git remote update inside your git bare directory.\nIf you have Git Large File Storage (LFS) objects, you need to push git LFS object separately using this command\n1git lfs push --all \u0026lt;new-git-repo-url.git\u0026gt; after you download all git LFS objects using git lfs fetch --all command.\nThe Conclusion  Do this post really need a conclusion? well, whatever, here we go.\n There\u0026rsquo;s no harm in backing up your git repository whether locally or on multiple platform, we can\u0026rsquo;t be so sure that some platform gonna stay the same. Maybe the platform we use gonna become worse and mindlessly banned a lot of account, who knows. So it\u0026rsquo;s better to start backup your git repository from now on before something like that happen.\nReferences  How to backup a git repository . How to mirror an entire existing git repository . Duplicating a repository . The difference between git clone \u0026ndash;mirror and git clone \u0026ndash;bare .  "},{"ref":"https://bruhtus.github.io/posts/python-vs-shell/","title":"Python Script vs. Shell Script: Command Line Use Case","section":"posts","tags":["No one asked","Linux","Python","Shell"],"date":"2021.01.14","body":" In this post i\u0026rsquo;m comparing performance python script and shell script that i\u0026rsquo;ve made. The script objective is to check git status on every git repo directory.\n Skip-able Part  Just a background story why i made this post\n Have you wondered what performance between python script and shell script (or some people might say bash script) to run command on terminal? no? same, just kidding. I\u0026rsquo;m curious about shell script because i can do command line stuff with python script too which for my past self is easier to read.\nFor around a month, i\u0026rsquo;ve learned shell scripting from youtube (mostly luke smith videos ) and i gotta say that for command line use case, shell script is more efficient than python script. Please keep in mind that i\u0026rsquo;m not an expert when it comes to python scripting or shell scripting, so there might be some performance enhancement that you can make to my script.\nA Brief Intro to Shell Script You might be wondering \u0026ldquo;what is shell script?\u0026rdquo; well, to put it simpler, it\u0026rsquo;s a script that you can run with shell. Shell, like python, is a programming language and that\u0026rsquo;s why you can run for loop in a terminal like a psychopath, just kidding i did that sometimes (maybe i\u0026rsquo;m a pyschopath? ).\nThere are quite a few ways to run shell script, here\u0026rsquo;s some that i know:\n  Make script executable by doing chmod +x \u0026lt;script-name\u0026gt; and then run the script by doing ./\u0026lt;script-name\u0026gt; if you\u0026rsquo;re already in the same directory as the script or chmod +x \u0026lt;path-to-script\u0026gt;/\u0026lt;script-name\u0026gt; and then run the script by doing \u0026lt;path-to-script\u0026gt;/\u0026lt;script-name\u0026gt; if you\u0026rsquo;re not in the same directory as the script. For example: If i want to run my git-status-checker script, then what i would do is type chmod +x git-status-checker if i already in the same directory as git-status-checker script and then run the script by typing ./git-status-checker. But, if i\u0026rsquo;m not in the same directory as git-status-checker, let\u0026rsquo;s say i\u0026rsquo;m in Download directory meanwhile the script is in Script directory, then what i would do is type chmod +x ~/script/git-status-checker and then run the script by typing ~/script/git-status-checker.\n  Using sh or bash command. For example: If i want to run my git-status-checker script, then i run the script by typing sh git-status-checker or bash git-status-checker, both command basically do the same thing.\n  The Test In this post, i\u0026rsquo;m gonna test the runtime both python script and shell script that i\u0026rsquo;ve made. The script objective is to check git status in each git repo directory on my machine.\nThe Environment of The Test All my git repo directory is located in all_git directory on home directory, so i only need to focus on that directory which is gonna make the task of the script a little bit easier and faster than check all other directory too.\nThe amount of not staged for commit is the same when running both script, which is around 13 in total. So the performance may decrease as the amount of unstaged increase.\nI\u0026rsquo;m gonna run the in two way, the first test is including git status for git submodules and the second test is excluding git status for git submodules.\nThe Results First Test In the first test, the runtime of python script is around 0.08 - 0.12 seconds and the runtime of shell script is around 0.07 - 0.08 seconds. Here\u0026rsquo;s the example of runtime of both script (which at this time coincidentally the same):\nSecond Test In the second test, the runtime of python script is around 0.05 - 0.07 seconds and the runtime of shell script is around 0.01 - 0.02 seconds. Here\u0026rsquo;s the example of runtime of both script:\nThe Explanation The effect of checking git status in git submodules is quite a lot but still less than a second which gonna make people think \u0026ldquo;it\u0026rsquo;s not that much of a difference\u0026rdquo;, well, in case you forgot that this script objective is just to check git status on each git repo directory, it\u0026rsquo;s just a simple task. If you\u0026rsquo;re going to make a more complicated task with a lot of command, that\u0026rsquo;s where you\u0026rsquo;re gonna see the gap.\nI\u0026rsquo;m gonna explain the first test source code first, below is the python script for the first test:\n1import os 2 3home = \u0026#39;/home/bruhtus/\u0026#39; 4dir_list = [dirname for dirname in os.listdir(f\u0026#39;{home}/all_git\u0026#39;) if os.path.isdir(f\u0026#39;{home}/all_git/{dirname}\u0026#39;) == True] 5 6for dirname in dir_list: 7 path = f\u0026#39;{home}/all_git/{dirname}\u0026#39; 8 os.system(f\u0026#39;echo {path}\u0026#39;) 9 10 if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: 11 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) 12 os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) 13 else: 14 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) And below is the shell script for the first test:\n1#!/bin/sh 2 3for d in $(ls -d ~/all_git/*/); do 4 echo $d \u0026amp;\u0026amp; git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s 5done Let\u0026rsquo;s take step by step of the processes. The first process is to list all directory in all_git directory, because we already specify that all git repo directory gonna be in all_git directory so we don\u0026rsquo;t need to check if it\u0026rsquo;s a git repo or not (one less task). So, here\u0026rsquo;s a comparison of the python script and shell script:\nThe python part below:\n1home = \u0026#39;/home/bruhtus/\u0026#39; 2dir_list = [dirname for dirname in os.listdir(f\u0026#39;{home}/all_git\u0026#39;) if os.path.isdir(f\u0026#39;{home}/all_git/{dirname}\u0026#39;) == True] 3 4for dirname in dir_list: is equivalent to this part in shell script:\n1for d in $(ls -d ~/all_git/*/); do Both of the script take list of directory in all_git directory and do a for loop to check each folder. You might be able to optimize my python script but i think that the shell script is much simpler than python script. What i mean simpler is less lines of code, not easier to understand. If you need an easier to understand code then python is the way, but it\u0026rsquo;s not really the topic of this post (everyone knows that python script is human readable, right?).\nLet\u0026rsquo;s continue, in the python part below:\n1 path = f\u0026#39;{home}/all_git/{dirname}\u0026#39; 2 os.system(f\u0026#39;echo {path}\u0026#39;) 3 4 if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: 5 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) 6 os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) 7 else: 8 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) is equivalent to this part in shell script:\n1 echo $d \u0026amp;\u0026amp; git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s 2done That second process is to check the git status and git submodules status of all directories. The part where there\u0026rsquo;s echo is basically print out anything that we give, in this case it\u0026rsquo;s gonna print directory name so i\u0026rsquo;m not gonna go into detail for that part.\nNow i\u0026rsquo;m gonna explain a little bit about \u0026amp;\u0026amp; command in shell, it\u0026rsquo;s basically let you run the second command (on the right side) if the first command (on the left side) success. So, it\u0026rsquo;s basically an equivalent of if-else statement in most programming language. The shell script part below:\n1git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s is equivalent to this part of python script:\n1 if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: 2 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) 3 os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) 4 else: 5 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) The ls -a $d | grep -q .gitmodules is equivalent to if os.path.exists(f'{path}/.gitmodules') == True, both command check if there\u0026rsquo;s a git submodules in the git repo directory or not. So in the second test, i\u0026rsquo;m removing those if-else statement and then run only git status which makes shell script faster than python script.\nThe Conclusion For command line interface stuff, you should probably use shell script instead of python script. I\u0026rsquo;m not saying that python is bad, it\u0026rsquo;s just not the right tool for command line interface stuff. Yeah python can do almost everything but that doesn\u0026rsquo;t mean python is the best at everything, at least not at command line interface use case which is shell script clearly better here.\nReferences  Luke smith youtube channel . Distrotube youtube channel . Git command without change directory . Git submodule documentation .  "},{"ref":"https://bruhtus.github.io/posts/mac-os-linux/","title":"The Difference Between GNU/Linux and Mac OS","section":"posts","tags":["Linux"],"date":"2020.12.29","body":" To make things clear that GNU/Linux and Mac OS is different and don\u0026rsquo;t expect both of them do the same thing.\n Skip-able Part What motivates me to make this post is a tweet from someone named David Bombal that trying to install ubuntu on macbook with M1 chip, here is the tweet .\nThat tweet makes me wonder why would someone install ubuntu on mac, isn\u0026rsquo;t mac os and linux similar? well it turn out they\u0026rsquo;re not similar. In this post i\u0026rsquo;ll try to explain what the difference between mac os and linux.\nA Brief Intro to Kernel and Operating System First thing first, we need to know what kernel and operating system is because we\u0026rsquo;re gonna talk a lot about both of them.\nWhat is kernel? to make things simpler, kernel is a bridge between software and hardware. Kernel allocate the system resources (such as CPU, RAM, and other stuff, basically the hardware) to the program/software that we open.\nWhat is operating system? according to debian post : An operating system consists of various fundamental programs which are needed by your computer so that it can communicate and receive instructions from users; read and write data to hard disks, tapes, and printers; control the use of memory; and run other software. The most important part of an operating system is the kernel.\nThen what\u0026rsquo;s the difference? well, kernel is part of the operating system and kernel alone doesn\u0026rsquo;t form a working operating system. You need another component other than kernel to make a working operating system which is beyond the scope of this post.\nA Brief Intro to GNU/Linux What most people knew as \u0026ldquo;linux\u0026rdquo; operating system is usually GNU/Linux. GNU consist the component for operating system except the kernel and linux fill the gap on GNU system. So linux alone doesn\u0026rsquo;t form a working operating system and GNU system with linux kernel make the whole operating system. You can use something other than GNU system if you want, like alpine linux which use musl libc and busybox instead of GNU. GNU/Linux is a unix-like operating system, what it means is that the operating system behaves in a similar manner to a unix system while not necessarily conforming to or being certified to any unix trademark.\nA Brief Intro to Mac OS Mac OS is Apple Mac line up (macbook, mac pro, etc) operating system and it\u0026rsquo;s based on darwin operating system which is based on BSD operating system. Here\u0026rsquo;s the schema for better understanding what i mean:\nMac OS also use XNU as a kernel. XNU stands for X is Not Unix (from apple\u0026rsquo;s github repo ). So Mac OS based on Darwin and XNU which is already different from GNU/Linux.\nThe Difference So basically mac os was based on BSD system while GNU/Linux was based on GNU system. Other than that, mac os doesn\u0026rsquo;t use linux as it\u0026rsquo;s kernel and use XNU instead. I\u0026rsquo;m not sure the difference between XNU and linux at this point, but i can give one example that is KVM (Kernel-based Virtual Machine) which is built into linux kernel. Basically KVM let you interact with kernel directly instead of using emulation for virtual machine.\nOther than the difference between kernel feature such as KVM, there\u0026rsquo;s a different feature or command from BSD system and GNU system. From this article , if you want to use GNU command, you need to install it first while in GNU/Linux you can use it immediately.\nAlso Mac OS doesn\u0026rsquo;t come with the package manager, as far as i know, you need to install the package manager yourself (i might be wrong tho, i have no mac to test it out), whereas the package manager already installed in GNU/Linux.\nTo make it simpler, the package manager is for installing and uninstalling a program from terminal.\nThe Conclusion Mac OS and GNU/Linux are two different system and don\u0026rsquo;t expect them to behaves the same. If you want Mac OS app then you should buy a mac, but if you want to have access to a lot of terminal or command line interface stuff then you should install GNU/Linux on PC or buy a machine that already came with it.\nInstalling GNU/Linux on mac is not really efficient because you could get a lot more computing power with the same price and mac hardware usually optimized with Mac OS so GNU/Linux might not run as smooth as Mac OS on mac (and mac user gonna judge that GNU/Linux not as good as Mac OS bla bla bla, which is stupid).\nHopefully you didn\u0026rsquo;t confuse Mac OS with GNU/Linux again and choose what you need rather than expecting Mac OS and GNU/Linux to behaves the same (and it seems like a lot of people did that).\nReferences  Why GNU/Linux . Linux kernel vs mac kernel . The difference between linux and bsd . Darwin os (wikipedia) . Unix-like term (wikipedia) . Quora about mac os, darwin os, and gnu/linux . Apple\u0026rsquo;s darwin-xnu github repo . Wikimedia unix timeline . Is mac os UNIX .  "},{"ref":"https://bruhtus.github.io/posts/manjaro-i3/","title":"Manjaro i3 Edition","section":"posts","tags":["Linux"],"date":"2020.12.22","body":" A minimalist approach to new arch linux user or new linux user in general. No more, maybe less.\n Skip-able Part  Just a background story why i use manjaro i3 edition.\n Have you ever bored with your current desktop environment? because that\u0026rsquo;s what made me wanted to try manjaro i3 edition. Why manjaro i3 edition and not something else like regolith linux? Here\u0026rsquo;s the reason:  I want a rolling release distro because it\u0026rsquo;s kind of a pain to reinstall the whole system just to upgrade to the new version in fixed-point release distro just like regolith linux (because it\u0026rsquo;s based on ubuntu). I want to try new things and fixed-point release is kind of boring for me. There\u0026rsquo;s nothing wrong with fixed-point release distro, it\u0026rsquo;s just not for me.  Now you wondering why manjaro specifically, Ok let me explain. I choose manjaro because a lot of people said that manjaro is more stable than other arch-based linux because the manjaro team hold on the update for a week or two after the update on arch repo. And now arch linux user gonna get mad at me like \u0026ldquo;Arch linux is also stable if you choose the stable mirror bla bla bla\u0026rdquo;, and honestly it\u0026rsquo;s kind of true. The reason i didn\u0026rsquo;t using arch linux is more for long term stuff. What i mean long term is that when i need to reinstall the operating system on my machine or when i got a new device, i want to install the operating system quickly. Especially with new device which have different hardware and configuration and i need to figure it out one by one if i use arch linux, uh that\u0026rsquo;s kind of hard for me at the moment. I might try installing arch linux in the future because i like the philosophy of arch linux that is \u0026ldquo;choose only what you want\u0026rdquo; so you need to know what you want first and you decide what you want. That\u0026rsquo;s like the most \u0026ldquo;freedom\u0026rdquo; i\u0026rsquo;ve ever seen and i\u0026rsquo;m kind of excited to try it out, but for now i\u0026rsquo;m still trying to figure out what i want to install on my machine and manjaro i3 edition is here to help me do that.\nMy Experience In this part i\u0026rsquo;m gonna type it into two part, the good part and the not good part of manjaro i3 edition.\nThe Good Part What makes manjaro i3 edition special is that this installation come with only minimum stuff which is good because it doesn\u0026rsquo;t take up too much space in the fresh installation.\nWhat do you mean by minimum stuff? well, for example in the full fledged desktop environment such as gnome, kde plasma, and xfce, you got system settings with the function to set up wallpaper, lock screen and stuff that you don\u0026rsquo;t even know. In manjaro i3 edition or in standalone window manager desktop environment, you didn\u0026rsquo;t have something like that. If you want something similar then you need to install the program that has the same function to set up wallpaper, lock screen, and other stuff.\nThat\u0026rsquo;s what i mean by minimal, it doesn\u0026rsquo;t come with a lot of stuff that you probably didn\u0026rsquo;t know that it\u0026rsquo;s installed on you system and take up space.\nThe Not Good Part In case you\u0026rsquo;re happy that the fresh installation doesn\u0026rsquo;t take up a lot of space, you might want to hold that for a moment because it has the downside.\nThe downside with minimum stuff installed is that a lot of things doesn\u0026rsquo;t work from the start so you need to install a package or even tweak a bit on the terminal. For example, you need to install pulseaudio before you could use your headphone jack and you need to install pulseaudio-bluetooth if you want to use your bluetooth audio stuff. The installation doesn\u0026rsquo;t come with pulseaudio and it kind of hard to work with only alsa for me, so you need to keep that in mind.\nBut, it\u0026rsquo;s not really a bad thing (that\u0026rsquo;s why i called it \u0026ldquo;not good\u0026rdquo; instead of \u0026ldquo;bad\u0026rdquo;). Why it\u0026rsquo;s not a bad thing? because you learn how to fix stuff and slowly you understand what works and what doesn\u0026rsquo;t work rather than only using what\u0026rsquo;s already there. So there\u0026rsquo;s a learning process there, not as intense as arch linux but it\u0026rsquo;s the first step towards that.\nYou might complain \u0026ldquo;but it\u0026rsquo;s not specific to standalone window manager, we learn something too in full fledged desktop environment\u0026rdquo;, and yeah you might learn something in full fledged desktop environment but what you learn usually tied down to that specific desktop environment rather than the general stuff. I have no problem if i want to change distro from manjaro to arch linux or debian, but if you use a specific desktop environment that come with the linux distro then you might have a hard time because every linux distro has different configuration for their desktop environment. Yeah it\u0026rsquo;s gnome but the configuration and the key bindings doesn\u0026rsquo;t necessary the same with each linux distro, meanwhile when you\u0026rsquo;re using standalone window manager, you can backup you configuration file to github or other git repositories and use it in any linux distro you want (i\u0026rsquo;m not sure if you can do that with full fledged desktop environment).\nConclusion Manjaro i3 edition is a nice warm up for someone who are trying to learn about standalone window manager desktop environment. Learn what package and configuration you want and learn how to fix the problem that doesn\u0026rsquo;t come with the installation package first, and then finally try arch linux.\nIn case you want something more minimal than manjaro i3 edition, you might want to try arcolinux , especially the arcolinuxd iso (it\u0026rsquo;s a combination of arch linux freedom with calamares installer).\nDecember 2020 Setup  Rick Astley ASCII version anyone?\n "},{"ref":"https://bruhtus.github.io/posts/zsh-aliases/","title":"Zsh Aliases: Be Lazy!","section":"posts","tags":["Linux","Shell"],"date":"2020.12.13","body":" Have you ever thought that typing a long command is such a pain? well, here\u0026rsquo;s one of the solution for you.\n Skip-able Part  Just a background story why i write this in the first place.\n At some random time back then i thought \u0026ldquo;can i increase my speed while using the command line interface?\u0026rdquo; and there\u0026rsquo;s two option i could think of back then, the first one is to increase my typing speed and the second one is to make everything i type shorter.\nHonestly i like to take it easy when typing, i\u0026rsquo;m not really those rush type who are trying to type fast. I don\u0026rsquo;t really care about the speed of my typing as long as i\u0026rsquo;m comfortable typing at that speed. It\u0026rsquo;s more like durability rather than speed and that\u0026rsquo;s why i take it easy. So, increasing my typing speed is a no (at least for me).\nThe second option is where piques my interest. From that point i did some research how i can shorten the command i use on internet (google, youtube, and even pornhub but unfortunately no one uploaded on pornhub yet (maybe i should be the one to upload on pornhub? hmm)).\nA Brief Intro to Shell Alias Basically shell alias is a key shortcut for the command that you want to use. Here\u0026rsquo;s an example: If you want to make a new directory, let\u0026rsquo;s say the name of the directory is \u0026ldquo;anu\u0026rdquo;, then if the directory already exist, you want the command doesn\u0026rsquo;t give an error message as a feedback. So here\u0026rsquo;s the command: mkdir -p anu. Everytime you want to make a new directory without an error message if the directory already exist, you need to type mkdir -p \u0026lt;directory-name\u0026gt; and further more if there\u0026rsquo;s a typo while you type it, uh that\u0026rsquo;s such a pain.\nSo, how would you make that simpler? well by using shell alias of course, and in this post i\u0026rsquo;m gonna use Z shell or zsh so the syntax might be different from bash and fish shell. Please keep that in mind.\nType of Zsh Aliases Before we start to how to make a shell aliases or in particular zsh aliases, we need to type the type of aliases that zsh has.\nThere\u0026rsquo;re four type of zsh aliases:\n Simple aliases Suffix aliases Function aliases Global aliases  Simple Aliases Like the name suggest, it\u0026rsquo;s just a simple stuff (or you could say the default?). Here\u0026rsquo;s an example of simple alias: alias md='mkdir -p' From above example, if you want to make a new directory without error message if the directory already exist then you only need to type md rather than mkdir -p. So, it\u0026rsquo;s basically telling the terminal that md equivalent to command mkdir -p and execute mkdir -p command immediately.\nSuffix Aliases Suffix alias is for opening a specific extension in a specific program, it defined using -s flag. Here\u0026rsquo;s an example of suffix alias: alias -s py=vim From above example, it\u0026rsquo;s basically telling the terminal to open every extension .py in vim. So if you have a python file, let\u0026rsquo;s say anu.py, then you just need to type anu.py and it\u0026rsquo;s gonna translate to vim anu.py, opening anu.py file in vim.\nPersonally i don\u0026rsquo;t really use this alias because i like to type vim and even if you\u0026rsquo;re too lazy to type vim you could use simple alias instead, like alias v='vim'.\nFunction Aliases Function alias is when you want an input in your command. For example, you want to open a python file and then preview the output using less (you use try it yourself, i won\u0026rsquo;t explain it). Here\u0026rsquo;s the function alias for that scenario: function anu(){ python $@ | less} #the @ symbol is basically saying to take everything after the command as input. So everytime you run a python script with anu then it\u0026rsquo;s output gonna be displayed in less. Let\u0026rsquo;s say you want to open itu.py, then you use anu itu.py and you\u0026rsquo;re gonna see the output inside of less rather than on terminal.\nHere\u0026rsquo;s another example: function nganu(){ python -c $1} #the 1 number is saying to take the first argument as an input, you could add another input by using $2, $3, and so on. If you type nganu 'import numpy as np' in your terminal then you\u0026rsquo;re gonna execute python -c 'import numpy as np'. So it\u0026rsquo;s basically take the first argument which is 'import numpy as np' as an input (please keep in mind that using '' considered as one argument).\nGlobal Aliases Global alias is the command that you could place anywhere in the sequences. For example, you want to grep a specific file in your directory and you probably gonna type ls | grep filename (honestly i don\u0026rsquo;t care what you type, you do you), if you type ls | grep filename every time you want to search for something than it\u0026rsquo;s gonna become a hassle.\nSo, what should you do? well, you could give a global alias to grep like this: alias -g G='| grep'. After that you could place G in the middle of your command, like this: ls G filename | less (it\u0026rsquo;s gonna direct the grep result to less) or in the back of your command, like this: ls G filename (not using less). The G alias is only for | grep command so you still need to type the pattern to search or in this case a filename.\nTips To make some Aliases First, before you make aliases, please check if the command already exist or not using which command. For example: i want to name my alias ls, before i make alias ls, i\u0026rsquo;m need to type which ls to show me if ls already assigned to other program or command. If after you type which \u0026lt;alias-you-want\u0026gt; and there\u0026rsquo;s an error like \u0026lt;alias-you-want\u0026gt; not found then you\u0026rsquo;re ready to go.\nSecond, don\u0026rsquo;t make to much specific simple aliases. For example, you want to make alias to install packages from your package manager, let\u0026rsquo;s say debian package manager (apt), you need to type sudo apt install \u0026lt;package-name\u0026gt;, and you want to make it simple by using alias like alias sai='sudo apt install'. So everytime you want to install something you just need to type sai \u0026lt;package-name\u0026gt;, simple right? well, honestly it\u0026rsquo;s not really a good practice because over time you\u0026rsquo;re gonna forget what the command behind those alias. If you\u0026rsquo;re really forgot what the command then there\u0026rsquo;s which command as a livesaver, but do you really want that? Imagine you\u0026rsquo;re using another machine without your aliases, then you forgot a simple command because you\u0026rsquo;re using to much alias, isn\u0026rsquo;t that kind of sad?\nConclusion Shell alias is like a double edge sword, use it carefully and don\u0026rsquo;t be dependent on it. If possible, only make aliases if you\u0026rsquo;re using it a lot and already know the basic or maybe make aliases that represent the executed command.\nBonus Useful Alias function sd(){cd \u0026quot;$(du ~ | awk '{print $2}' | fzf)\u0026quot;}  That\u0026rsquo;s my first attempt using fzf, it basically change directory using fzf output as an input.\n function cs(){find ~ -type f | fzf | xargs -o -r vim}  That\u0026rsquo;s basically find every files in home directory using find and fzf and then open the file in vim directly (the parameter -o for not broke my terminal after the command (for some reason) and the -r for if there\u0026rsquo;s no input then it\u0026rsquo;s gonna back to terminal instead opening vim. For more detail please check on man page).\n References  Types of zsh aliases . Luke smith youtube channel . Distrotube youtube channel .  "},{"ref":"https://bruhtus.github.io/posts/manjaro-backup/","title":"Backup: The Options","section":"posts","tags":["Linux"],"date":"2020.11.22","body":" A few options about backing up your system, especially on arch-based distro. It\u0026rsquo;s mostly for linux, i\u0026rsquo;m not sure if you can use it for mac os or windows too.\n Why Should You Backup Your System? In case you\u0026rsquo;re wondering why you should backup your system (for some reason), let me briefly explain. There\u0026rsquo;s always a risk of failure when you\u0026rsquo;re updating or installing a new package (to make things easier to understand, it\u0026rsquo;s basically a program), when that happens, do you want to restore your system before the error occur or you want to re-install the whole system from scratch? your choice. For me personally, i want to restore my system before the error occur rather than re-install everything from scratch. Ideally, before you install something, you should backup your system just in case there\u0026rsquo;s some trouble. If you\u0026rsquo;re insane enough to backup your system everytime you want to install new stuff or update your system then go ahead, you do you.\nIn this post, i\u0026rsquo;m gonna explain two most popular backup solution, that is clonezilla and timeshift , and also a bonus tip for arch-based linux user.\nClonezilla For the record, i\u0026rsquo;ve never tried this. I only do the research about backup on linux and this tool came up quite often. For more info on how to use this tool, you can check youtube video here .\nClonezilla is a tool to backup your entire system. That\u0026rsquo;s right, it backup your whole system. You might think \u0026ldquo;wow that\u0026rsquo;s great, i don\u0026rsquo;t need other backup tool!\u0026rdquo;, you might want to hold that thought. Yeah it backup your entire system, even your empty storage. Let me explain, for example your system has 200 GB allocated storage and let\u0026rsquo;s say you used only 100 GB so you have another 100 GB storage left. When i said clonezilla backup your entire system, it means clonezilla gonna backup the 200 GB of allocated storage for your system and not only 100 GB of used storage. So if you\u0026rsquo;re gonna install the whole system again (which is possible with clonezilla apparently), you need minimum the allocated storage space from previous machine for your system. If you didn\u0026rsquo;t provide the minimum storage space, you gonna get the error. Unless you strive for that error then it\u0026rsquo;s not gonna be a problem but if you didn\u0026rsquo;t expect the error then you might get confused. That\u0026rsquo;s the downside of clonezilla as far as i know it. I personally didn\u0026rsquo;t use it because of that downside but if it\u0026rsquo;s an advatage for you then you should try it.\nTimeshift This is a backup tool that i\u0026rsquo;ve always used since i started using linux on daily basis. The concept is simple, it made checkpoint for your system and then you can revert back to that checkpoint if something wrong after the update or after installing new stuff.\nFor arch-based linux user, this tool is a must. Arch-based linux is a rolling release linux, that means you get an update regularly and those update not always gonna work so there\u0026rsquo;s a high chance your system gonna break from the update. This tool is a livesaver.\nThe difference between this and clonezilla is that this tool only backup the used storage and also exclude the home directory (you can include backup home directory in the setting). In case you\u0026rsquo;re wandering why exclude the home directory is a default, here\u0026rsquo;s an example: If you have 3 file on your home directory, let\u0026rsquo;s just say \u0026lsquo;anu.py\u0026rsquo;, \u0026lsquo;nganu.py\u0026rsquo;, \u0026lsquo;anumu.py\u0026rsquo; then you backup your system. After backing up your system you add another file, let\u0026rsquo;s say \u0026lsquo;itumu.py\u0026rsquo;, then you revert back to the checkpoint you created previously. After you revert back, the file \u0026lsquo;itumu.py\u0026rsquo; is gone because it\u0026rsquo;s re-write everything according to checkpoint list and file \u0026lsquo;itumu.py\u0026rsquo; not in the checkpoint list yet. That\u0026rsquo;s why excluding home directory is a default, you can change it in the setting if you want.\n I\u0026rsquo;ve never used this tool on newly installed system like clonezilla did, so i\u0026rsquo;m not sure if you can using that checkpoint on newly installed system.\n Bonus Tip Here\u0026rsquo;s a bonus tip for arch-based linux user, just in case those two tools didn\u0026rsquo;t save your system (it\u0026rsquo;s not gonna hurt if you have another backup option you know).\nType this two command in your terminal:  For official arch repository: pacman -Qqen \u0026gt; pkglist-repo.txt For arch user repository (AUR): pacman -Qqem \u0026gt; pkglist-aur.txt  What it did is make a package list installed on your system from official arch repository and AUR and save it to .txt file. After that you can install all your package with those file using the command below:  For official arch repository: sudo pacman -S --needed - \u0026lt; pkglist-repo.txt For arch user repository (AUR): for x in $(cat pkglist-aur.txt); do pamac build $x; done (you can also use other alternative such as yay, you don\u0026rsquo;t need to use pamac if you don\u0026rsquo;t want to).  For more info you can check my repo here , i write a few references there.\n"},{"ref":"https://bruhtus.github.io/posts/instasaver/","title":"Instasaver: Save Your Chosen Instagram Posts","section":"posts","tags":["No one asked","Python"],"date":"2020.11.09","body":" A brief explanation about instasaver, a tool to save instagram post build with instaloader python module and streamlit.\n Background Story  Nothing fancy, you should probably skip this.\n To make things short, basically i was inspired by Kevin Hazy\u0026rsquo;s project here . After i see Kevin\u0026rsquo;s project, i was like \u0026ldquo;can i make almost the same thing with python?\u0026rdquo; and that was the trigger.\nIf you ask me why i want a tool (kind of) to save instagram post, that\u0026rsquo;s because i have a hard time to save video memes on instagram and i don\u0026rsquo;t really want to use a screen recorder (said the guy who made something that took longer than learning how to use screen recorder and edit the result ). So anyway, that\u0026rsquo;s my motivation to made this. For the memes!\nInstaloader Python Module  In this part, i\u0026rsquo;m only gonna explain the part that i\u0026rsquo;ve used and tried along with the problem that i\u0026rsquo;ve encountered. So for the full documentations, you can check here .\n Instaloader Feature That I Used There\u0026rsquo;re a lot of things you can do with instaloader like saving instagram stories, follower list, following list, and so on, but for this project i only use saving post feature. Why i only use that feature? because other features require login to instagram account, i\u0026rsquo;ll explain on that later.\nSo, even only saving post feature has a lot going on but i\u0026rsquo;m not quite sure how to implement that, the only thing i\u0026rsquo;m sure is how to implement saving post using a url (well, also other features that require login too). If you want to learn other features you can check their documentations which is quite confusing. Well, maybe i\u0026rsquo;m just a complete idiot but you know what, you\u0026rsquo;re gonna have other complete idiots that didn\u0026rsquo;t understand their documentations. I don\u0026rsquo;t really want to be hard on them but they could have done better, especially on the examples (not only advance examples but basic examples too).\nInstaloader Feature That I\u0026rsquo;ve Tried Apart from saving post feature, i\u0026rsquo;ve also implemented saving stories, following list, and follower list. As i\u0026rsquo;ve mentioned above, those three features require login to instagram account. Well, you can made a fake account to login to instagram but it\u0026rsquo;s not ideal to deploy it. There\u0026rsquo;s this problem when you login quite often so you need to wait around 15 minute to use it again, i mean if i want everyone else to access it then that problem gonna be a hassle. I\u0026rsquo;ve never tried to make a lot of fake account and rotate through all of them to login but i might do it later, who knows. If you want to see all the features that i didn\u0026rsquo;t implement in the public version, you can check my github repo here .\nImplementation With Streamlit  In this part, i\u0026rsquo;m gonna explain the implementation with streamlit and how to deploy on streamlit sharing.\n Instaloader (Main Class) For starter, in instaloader main class (Instaloader ) the parameters that used in this project was dirname_pattern, download_comments, download_geotags, download_video_thumbnails, and save_metadata.\ndirname_pattern was to make the default folder to save the file which in this case i use temporary folder because i don\u0026rsquo;t want to save the images or videos on my github repo but download it to my device (whether smartphone or pc).\ndownload_comments, download_geotags, download_video_thumbnails, and save_metadata is set to False just to make it simple, i just want to download the memes (whether images or videos) and i don\u0026rsquo;t want anything else. You could turn that on to get all that stuff tho.\nShort code To extract the short code, i used regular expression from this source and then using instaloader Post.from_shortcode and download_post to finally download the post (saved in temporary folder first and then generate download button to save to device).\nPublic Version What i mean by public version is the version that i deploy so that everyone can use it. For the public version, i only use saving post with url input. The diagram process is below:\nAs you see above, you can download one item or multiple items in one post. Implementation for download instagram post with streamlit is first you download the post from url using instaloader download_post and saved it to temporary folder and then define exception handling to detect whether it\u0026rsquo;s an image or a video. The implementation is below:\n1import instaloader 2import streamlit as st 3import os 4from zipfile import ZipFile 5 6post = instaloader.Post.from_shortcode(instaloader.Instaloader.context, shortcode) 7instaloader.Instaloader.download_post(post, target=temp) #temp is temporary folder 8file_list = [filename for filename in os.listdir] 9 10if len(file_list) == 1: #if only one item in one post 11 try: 12 st.image(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, use_column_width=True) #use_column_width is to resize the width of the image displayed 13 st.markdown(download_button(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, temp), unsafe_allow_html=True) #download_button is to generate link to download the file 14 except: 15 st.video(f\u0026#39;{temp}/{file_list[0]}\u0026#39;) 16 st.markdown(download_button(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, temp), unsafe_allow_html=True) #download_button is to generate link to download the file 17 18else: #if more than one item in one post 19 with ZipFile(f\u0026#39;{temp}/{shortcode}_posts.zip\u0026#39;, \u0026#39;w\u0026#39;) as zip: #put all the posts into zip file 20 for filename in file_list: 21 try: 22 st.image(f\u0026#39;{temp}/{filename}\u0026#39;, use_column_width=True) 23 st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) 24 zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) 25 except: 26 st.video(f\u0026#39;{temp}/{filename}\u0026#39;) 27 st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) 28 zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) 29 30 st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) please keep in mind that the implementation above is only a partial from all the code, it\u0026rsquo;s just a glimpse of what\u0026rsquo;s going on inside the process. For the full code you can check here .\nLocal Version What i mean by local version is the version that has a more features than the public version that requires login to instagram account. For the local version, i use download the post from url feature (like the public version), download stories (download all the stories of the user, public profile only or you already followed the private account), and download following and follower list.\nDownload stories For download stories, i used download_stories module with user id as input and zip all the stories. Below is a diagram process of the download stories:\nglimpse of the process:\n1import instaloader 2from zipfile import ZipFile 3 4load = instaloader.Instaloader() 5profile = instaloader.Profile.from_username(load.context, username) 6profile_id = instaloader.Instaloader.check_profile_id(username) 7load.download_stories(userids=[profile_id.userid]) 8 9with ZipFile(f\u0026#39;{temp}/{profile.username}_stories.zip\u0026#39;, \u0026#39;w\u0026#39;_) as zip: 10 for filename in file_list: 11 zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) 12 13st.markdown(download_button(f\u0026#39;{temp}/{profile.username}_stories.zip\u0026#39;, temp), unsafe_allow_html=True) Download following and follower list For download following and follower list, i used get_followees and get_followers module with username as input and write all user\u0026rsquo;s following and follower username in .txt file.\nglimpse of the process (download following list):\n1import instaloader 2 3load = instaloader.Instaloader() 4profile = instaloader.Profile.from_username(load.context, username) 5 6with open(f\u0026#39;{temp}/{profile.username}_following.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: 7 for following in profile.get_followees(): 8 f.write(f\u0026#39;{following.username}\\n\u0026#39;) 9 10st.markdown(download_button(f\u0026#39;{temp}/{profile.username}_follower.txt\u0026#39;, temp), unsafe_allow_html=True) Deployment I use streamlit sharing and heroku to deploy this project, why i use two services to deploy this project? that\u0026rsquo;s because i want to try web app deployement on heroku and streamlit sharing (which is quite new).\nStreamlit Sharing To deploy on streamlit sharing you need to request an invite in their website and then after that you can deploy your streamlit app. It takes a few minutes to deploy the first time but after that it\u0026rsquo;s deploy in an instant. You can check the live demo on streamlit sharing here .\nHeroku To deploy on heroku, you need a few thing and here\u0026rsquo;s the list:\n runtime.txt -\u0026gt; to specify python version. Procfile -\u0026gt; to specify type of our application and run command, check here for more info. create_config.sh (or setup.sh, whatever you want) -\u0026gt; to make config.toml for streamlit to run.  I don\u0026rsquo;t really need to explain about runtime.txt, do i? you just need to type your python version, for example python-3.7.5, that\u0026rsquo;s all (i explain it anyway, dammit).\nFor .sh file (in this case i\u0026rsquo;m gonna name it create_config.sh because i\u0026rsquo;m not creative, sorry), type this:\n1mkdir -p ~/.streamlit 2 3echo \u0026#34;[server] 4headless = true 5port = $PORT6enableCORS = false 7\u0026#34; \u0026gt; ~/.streamlit/config.toml For Procfile, you don\u0026rsquo;t need to add an extention there. Just Procfile is enough. In the Procfile, type this:\nweb: sh create_config.sh \u0026amp;\u0026amp; streamlit run instasaver.py You don\u0026rsquo;t need to name it create_config.sh, be creative, don\u0026rsquo;t be like me.\nAfter all that, you can deploy it using git or from github or something else (i don\u0026rsquo;t remember all the choices). If you want to deploy it using git workflow, you can check here .\nFor live demo on heroku, you can check here .\nReferences  The one who trigger me to made this . Instaloader documentations . Streamlit . Download instagram stories . View public profile anonymous (more advance version) . Streamlit multiselect nested in if . Streamlit download file . Deploy streamlit app on heroku example .  "},{"ref":"https://bruhtus.github.io/posts/tiling-window-manager/","title":"Tiling Window Manager For Efficiency","section":"posts","tags":["Linux"],"date":"2020.10.15","body":" A brief explanation about tiling window manager and configuration tiling window manager that i\u0026rsquo;ve tried.\n A Brief Explanation About Window Manager  A window manager is a system software that controls the placement and appearance of windows within a windowing system in a graphical user interface (GUI). It can be part of desktop enviroment (DE) or be used standalone1.\n So basically window manager is how to place a window. There\u0026rsquo;re three types of window manager (according to arch wiki1): stacking (aka floating) window manager, tiling window manager, and dynamic window manager.\nHere\u0026rsquo;s a brief explanation about those three window manager:\n Stacking (aka floating) Window Manager: All window manager that allow the overlapping of windows are considered stacking window manager, although it is possible that not all stacking window manager use the same method. You can check a few list of stacking window manager here . Tiling Window Manager: Tiling window manager manage the window so that no window are overlapping with each other. You can check a few list of tiling window manager here . Dynamic Window Manager: Dynamic window manager is a tiling window manager that positioned based on the preset layouts which user can switch. You can check a few list of dynamic window manager here .  For more detailed comparision of tiling window manager, you can check here .\nThe Window Manager I\u0026rsquo;ve Tried For now (at the times of writing this post), i only have tried 2 window manager. My first window manager is qtile and my second window manager is i3. You can check below for more (not really) detailed explanation(?).\ni3 Window Manager i3 is one of tiling type window manager, for more info you can check at their website . I\u0026rsquo;m just gonna explain configuration that i\u0026rsquo;ve made at my github repo . For the record, this is my current window manager (at the time writing this post). I\u0026rsquo;m using the manjaro i3 edition.\nFirst of all, i changed the mod keybinding from super key (or some people call it windows key) to alt key. You can change the mod keybinding by change set $mod Mod4 (super key) to set $mod Mod1 (alt key) in config file (you can find config file in folder .i3). For the most part i used the default config from manjaro i3 edition but i add a few program and even changed the i3 status bar with polybar. Here\u0026rsquo;s what i used in this config:\n pywal : I use this for color scheme around my i3 environment (such as terminal, border color around window, etc) polybar : I use this to replace i3 status bar because i can place the clock and date in the middle (i have no ide how to do that in i3 status bar or even py3status) and it has quite a lot of customization. You can also use py3status if you want. rofi : I use this as application manager instead of dmenu, because it\u0026rsquo;s more convenient for me (rofi appear in the middle meanwhile dmenu appear at the top). conky : I use this to take a glance what process currently taking up resources (for more detailed info i use htop). flameshot : I use this to take screenshot.  Setting Up Keybinding For setting up keybinding you can use bindsym, for example: bindsym $mod+q kill for close focused or currenly active window. Other than setting up keybindings, you can also set a program to do a certain thing, for example: set $myTerm alacritty, every thing that used $myTerm gonna access the command via terminal alacritty. Alacritty is my current (at the time of typing this post) terminal emulator, i also have xterm as a backup terminal emulator. Example of using $myTerm: bindsym $mod+e exec $myTerm -e ranger to open ranger file manager.\nSetting Up Polybar For setting up polybar, you need to move the default polybar config. In my case, the default config is in /usr/share/doc/polybar/ but if it\u0026rsquo;s not there, you can use locate polybar | grep config.\nFirst of all, make a polybar folder in .config folder. After that move the default polybar config into those folder (the path should be like this .config/polybar/config). You can change the default config to anything you want, but remember the bar name because we\u0026rsquo;re gonna use the bar name to launch the polybar. The default bar name should be like this [bar/example], you can change it to the name you want and please specify the monitor for the polybar. You can check you monitor name by typing xrandr on terminal. Here\u0026rsquo;s an example how to set a monitor in polybar config:  After i typed xrandr on my terminal, i got my laptop screen name eDP-1 so i\u0026rsquo;m gonna use my laptop screen to display the polybar.\n [bar/mainbar-i3] monitor = {env:MONITOR:eDP-1} After you\u0026rsquo;re done with your polybar config, the next step is to add launch.sh. What is launch.sh? well, it\u0026rsquo;s basically to launch all of our polybar bar config (that has this naming scheme [bar/example]). Here\u0026rsquo;s an example of launch.sh in two monitor (each bar config has different monitor assigned to it):\n#!/usr/bin/bash #Terminate already running bar instances killall -q polybar #Wait until the process have been shut down while pgrep -u $UID -x polybar \u0026gt;/dev/null; do sleep i; done #launch bar polybar mainbar-i3 \u0026amp; polybar secondbar-i3 \u0026amp; echo \u0026quot;bars launched...\u0026quot; Who Should Try or Use i3? i3 is a \u0026lsquo;manual\u0026rsquo; tiling window manager so it doesn\u0026rsquo;t really have default layout which is different from dynamic \u0026lsquo;tiling\u0026rsquo; window manager, you need to specify where the window opened (whether the window opened on the right or below). If you want to use i3 you might want to consider that. So, who should try or use i3? everyone who wants a tiling window manager and doesn\u0026rsquo;t really mind to have manually control where the window appear, that\u0026rsquo;s all.\nQtile Window Manager Qtile is one of dynamic window manager that use python as basis configuration, for more info you can check at their website .\nIn my config repo i usually use MonadTall layout or Max layout. MonadTall layout basically split the first two window into half vertically and then for the third and so on gonna split the right window horisontally. Max layout basically have the application automatically take up the whole screen, that\u0026rsquo;s all. For more build-in layouts you can check their documentation here .\nI used qtile window manager first before switch to i3 because the configuration is in python but what i don\u0026rsquo;t really like is how qtile treat multiple screen. When i want to switch to second monitor, it swapped the application on second monitor to first monitor (currently active monitor) and that\u0026rsquo;s not what i want, i just want to switch to different screen and not have application on that screen swapped with application on my currently active screen. It was confusing and then i tried i3wm after that.\nThe config in my repo is a basic config i\u0026rsquo;ve done because i don\u0026rsquo;t really like the workflow of qtile. Sorry about that.\nWho Should Try or Use Qtile? If you\u0026rsquo;re fine with the workflow of qtile, wants to try or use dynamic window manager, like to have or doesn\u0026rsquo;t really mind preconfigured layout, and have experience with python then you might want to try qtile. Don\u0026rsquo;t get me wrong, qtile is good as a window manager but unfortunately it doesn\u0026rsquo;t meet my needs.\nThe Pros and Cons of Tiling Window Manager Pros:\n You don\u0026rsquo;t have to worry about placement of your window because it\u0026rsquo;s automatically split the workplace for you. You use mouse less. All your application is right there without any overlap window.   Surprise, you haven\u0026rsquo;t close me yet so i\u0026rsquo;m gonna take up your computer resources.\n Cons:\n If you open too much application then the application size gonna become smaller. (that\u0026rsquo;s why use virtual desktop or workspace 1 to 8). It\u0026rsquo;s kind of hard to setting for first time. After that hard time, you can backup your previous config and use it again in other computer. Nice.  To Wrap Things Up If you want to use keyboard-oriented navigation or you don\u0026rsquo;t want to use mouse often, then you probably should try using tiling window manager. But, as i\u0026rsquo;ve mentioned above, it took time to learn how to configure it (and i think it\u0026rsquo;s worth your time).\nIf you\u0026rsquo;re still not sure, you can try it first in virtual machine (such as virtualbox or virt-manager) and then you could copy those config file to your new installation.\nExample of i3 Window Manager  Moderate use:   Too much window in one workspace can be bad:    Arch linux wiki (window manager) .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"ref":"https://bruhtus.github.io/posts/pavement-distress-detector/","title":"Pavement Distress Detector Using Single Shot Detector (SSD)","section":"posts","tags":["Deep learning","Python"],"date":"2020.10.11","body":" This was my graduation project, in this article i\u0026rsquo;m gonna explain what i did in my graduation project. This project was based on Congcong Li\u0026rsquo;s Project .\n Diagram Process of This Project A Brief Explanation About Single Shot Detector (SSD) Single shot detector is a deep learning method presented by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed4, Cheng-Yang Fu, Alexander C. Berg in their research paper SSD: Single Shot Multibox Detector . There are 2 commonly used SSD model, that is, SSD300 and SSD512. Here\u0026rsquo;s a brief explanation about SSD300 and SSD512:\n SSD300: More fast. SSD512: More accurate.  Long story short, SSD300 is about speed. If you need speed than you should probably use SSD300 (i haven\u0026rsquo;t tried the mobilenet as base network at the time to type this, so at this time knowledge SSD300 is faster than SSD512). Meanwhile, SSD512 is about accuracy. It doesn\u0026rsquo;t really show up in image processing but in video processing, i notice that there\u0026rsquo;s a frame rate drop while doing live object detection. To be fair, SSD300 has frame rate drop as well but it\u0026rsquo;s still usable (around 7-10 frame per second) but SSD512 has frame rate around 3-5 frame per second. Who want to watch a video with 3 fps?? If you\u0026rsquo;re that kind of person then, go ahead. You do you mate.\nFor the record, at that time when I try live detection, i use opencv to display live detection video. i\u0026rsquo;m not sure whether it is opencv fault or the model fault because if I save the video result, the video itself has no frame rate drop. It\u0026rsquo;s weird but it happens, so let\u0026rsquo;s go on with saving the video and forget about live detection (for now, until i find some way to do live detection).\nSo, in this project i\u0026rsquo;m not gonna make it live detection. Rather than live detections, we\u0026rsquo;re gonna save the video result first and then display it later. That way it could also reduce some computational cost.\nFor those who still confused about live detection, to make things simpler, live detection is when you process the video, detect the object, and play the video at the same time. After you detect the object, you immediately display the frame that just recently processed and then processed the next frame. Repeat.\nSingle Shot Detector (SSD) Architecture That\u0026rsquo;s Used in This Project As explained above, in this project we\u0026rsquo;re gonna use SSD512. SSD512 is basically SSD with input image 512x512. The basic architecture of SSD contains 2 part, base network and extra feature layers. The base network layers are based on standard architecture used for high quality image classification (truncated before classification layers). The extra feature layers used for multi-scale feature maps for detection and convolutional predictors for detection.\nHere is an architecture single shot detector that used in this project (made this with NN architecture maker ):\nInformation:\n Input image. Base Network (truncated before classification layers). Layer 6 and layer 7 of base network (from fully-connected layer turned into classification layer). Extra feature layers. Collection of boxes and scores.  Base Network The base network used in this project is Visual Geometry Group (VGG). I chose VGG because of transfer learning capability so that i could have a good result with small dataset. To be more specific, in this project i used VGG16, here is a brief explanation of each layers:\n In the first layer, there\u0026rsquo;s a convolutional process with kernel filter 3x3 and stride (total shift filter per pixel) 1 pixel. That process repeat 2 times and then did some max pooling with kernel filter 2x2 and stride 2 pixel. In the second layer until fourth layer, the model did the same thing as in first layer. The difference was in fifth layer. In fifth layer, the convolution process still the same as the other four layers but the max pooling process was different from the other four layers. The max pooling process used kernel filter 3x3 with stride 1 pixel with padding (adding zero value around pixel image) 1. You can check the illustration below to understand the process of max pooling with kernel filter 3x3, stride 1, and padding 1.  And here\u0026rsquo;s a VGG16 after truncated from classification layers:\nIf you want to calculate the result from max polling, you can use this equation 1:\nInformation:\n kernel_size, stride, padding, and dilation can be 1 integer (in this case, the value for height and width are the same) or 2 integer (in this case, the first integer is height and the second integer is width). For more info you can see pytorch page .  Here\u0026rsquo;s some example of max pooling calculation with input 32x32, kernel filter 3x3, stride 1, padding 1, and dilation 1:\nLayer 6 and Layer 7 After feature extraction process in base network, the next layers is to change layer 6 and 7 of base network from fully-connected into convolutional layer with subsample parameters from fully-connected 6 (fc6) and fully-connected 7 (fc7). The convolution operation used in layer 6 and layer 7 is atrous convolution, you can see atrous convolution shift below:\nWith atrous convolution we can expand area of observation for feature extraction while maintaning the amount of parameters fewer than traditional convolution operation.\nExtra Feature Layers Extra feature layers is a prediction layers. In this layer, the model predict the object using default box. Default box is a box with various aspect ratio in every location of feature maps with different size. You can see an example of default box below 2\nIn the last layer is a collection of default boxes which closer to ground truth box with confidence score from that default boxes.\nTake A Video (Training Video and Testing Video) In this part, i\u0026rsquo;m gonna explain about the video used in this project. The camera configuration, the place where the video taken, the camera angle and height from the road.\nThe place where the video taken was in Surabaya, at Kertajaya Indah Timur IX, Kertajaya Indah Timur X, and Kertajaya Indah Timur XI. The camera angle was perpendicular(?) with the road (90 degrees) and the camera position from the road was 200 cm.\nThere\u0026rsquo;re 7 video taken, 3 for training and 4 for testing. The format of the video was *.mp4. You can check the location partition of the video taken below:\nThe black block is for testing and the white block is for training. You can check the position of the camera below:\nSetting Up The Config File For more detailed configuration please check develop guide by Congcong Li . In this post i\u0026rsquo;m gonna explain it the easiest way.\nBasic Configuration To make things easier, copy the format dataset you want. For example, in this project i want to use COCO dataset format. Then, i copied the coco.py in the path ssd/data/datasets/ and rename it to my_dataset.py. After that, edit the class names for your classification class. In this project, the class i\u0026rsquo;m gonna use is alligator crack, longitudinal crack, transverse crack, and pothole. Also, don\u0026rsquo;t forget to change the class COCODataset to MyDataset.\nThe next step is to add those configuration to __init__.py in ssd/data/datasets/. For example:\n1from .my_dataset import MyDataset 2 3_DATASETS = { 4 \u0026#39;VOCDataset\u0026#39;: VOCDataset, 5 \u0026#39;COCODataset\u0026#39;: COCODataset, 6 \u0026#39;MyDataset\u0026#39;: MyDataset, 7} Another next step is to add the path of your datasets and anotations to the path_catlog.py in ssd/config/. For example:\n1import os 2 3class DatasetCatalog: 4 DATA_DIR = \u0026#39;datasets\u0026#39; 5 DATASETS = { 6 \u0026#39;my_custom_train_dataset\u0026#39;: { 7 \u0026#34;data_dir\u0026#34;: \u0026#34;train\u0026#34;, 8 \u0026#34;ann_file\u0026#34;: \u0026#34;annotations/train.json\u0026#34; 9 }, 10 11 \u0026#39;my_custom_validation_dataset\u0026#39;: { 12 \u0026#34;data_dir\u0026#34;: \u0026#34;validation\u0026#34;, 13 \u0026#34;ann_file\u0026#34;: \u0026#34;annotations/validation.json\u0026#34; 14 }, 15 } 16 17 @staticmethod 18 def get(name): 19 if \u0026#34;my_custom_train_dataset\u0026#34; in name: 20 my_custom_train_dataset = DatasetCatalog.DATA_DIR 21 attrs = DatasetCatalog.DATASETS[name] 22 args = dict( 23 data_dir = os.path.join(my_custom_train_dataset, attrs[\u0026#39;data_dir\u0026#39;]), 24 ann_file = os.path.join(my_custom_train_dataset, attrs[\u0026#39;ann_file\u0026#39;]), 25 ) 26 return dict(factory=\u0026#34;MyDataset\u0026#34;, args=args) 27 28 elif \u0026#34;my_custom_test_dataset\u0026#34; in name: 29 my_custom_train_dataset = DatasetCatalog.DATA_DIR 30 attrs = DatasetCatalog.DATASETS[name] 31 args = dict( 32 data_dir = os.path.join(my_custom_train_dataset, attrs[\u0026#39;data_dir\u0026#39;]), 33 ann_file = os.path.join(my_custom_train_dataset, attrs[\u0026#39;ann_file\u0026#39;]), 34 ) 35 return dict(factory=\u0026#34;MyDataset\u0026#34;, args=args) And finally, for the *.yaml file for configuration i copied vgg_ssd512_coco_trainval35k.yaml in configs folder and rename it to config.yaml. What i changed from that file was the train and test (or more like validation) like in path_catlog.py, the batch size, and num_classes. I changed batch size because my laptop gpu only capable of 4 batch size. Here\u0026rsquo;s an example:\n1Model: 2 num_classes: 5 #the __background__ counted 3 ... 4 DATASETS: 5 TRAIN: (\u0026#34;my_custom_train_dataset\u0026#34;, ) 6 TEST: (\u0026#34;my_custom_test_dataset\u0026#34;, ) 7 SOLVER: 8 ... 9 BATCH_SIZE: 4 10 ... 11 12 OUTPUT_DIR: \u0026#39;outputs/ssd_custom_coco_format\u0026#39; You don\u0026rsquo;t need to create folder ssd_custom_coco_format, when the training begin the folder gonna created automatically (if the folder didn\u0026rsquo;t exist).\nValidation Configuration First of all, copy coco folder in ssd/data/datasets/evaluation/ and rename it to my_dataset. Rename the def coco_evaluation to def my_dataset_evaluation in file __init__.py. After that, add folder my_dataset to file __init__.py in ssd/data/datasets/evaluation/. For example:\n1from ssd.data.datasets import VOCDataset, COCODataset, MyDataset 2... 3from .my_dataset import my_dataset_evaluation 4 5def evaluate(dataset, predictions, output_dir, **kwargs): 6 ... 7 elif isinstance(dataset, MyDataset); 8 return my_dataset_evaluation(**args) 9 else: 10 raise NotImplementError The Interface For the interface, i\u0026rsquo;m using python library streamlit. Streamlit is more like web interface rather than common graphical user interface (GUI). Here\u0026rsquo;s how the interface looks like:\nThere\u0026rsquo;s a problem with file uploader at the time (streamlit version 0.59.0), i can\u0026rsquo;t upload file larger than 100 mb meanwhile the limit of file uploader should be 200 mb at that time. You can check the issue here , it seems like they\u0026rsquo;re already fixed it but i haven\u0026rsquo;t try it yet. So, when making this project i\u0026rsquo;m using the dropdown menu bar.\nTraining Preparation Before training the model, we need to do some preparation. There\u0026rsquo;re two steps in this process, frame extraction and labeling. Without further ado, let\u0026rsquo;s get started.\nFrame Extraction In this process, i used python library opencv to extract some frame. Here\u0026rsquo;s the script:\n1import cv2 2import time 3from fire import Fire 4from tqdm import tqdm 5 6def main(video_file, path_save, speed): # the lower the speed the fastest the frame_rates, speed = 0 (pause) 7 vidcap = cv2.VideoCapture(video_file) 8 current_frame = 0 9 speed_frame = speed 10 11 while (vidcap.isOpened()): 12 success, frame = vidcap.read() # success = retrival value for frame 13 length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) 14 print(f\u0026#39;Current Frame: {current_frame}/{length}\u0026#39;) 15 current_frame += 1 16 17 if success == True: 18 cv2.imshow(\u0026#39;Video\u0026#39;, frame) 19 if cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;s\u0026#39;): # press s to save the frame 20 cv2.imwrite(f\u0026#34;{path_save}/frame_{current_frame}.jpg\u0026#34;, frame) 21 22 elif cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): # press q to quit 23 break 24 25 elif cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;w\u0026#39;): # play/pause 26 if speed != 0: 27 speed = 0 28 elif speed == 0: 29 speed = speed_frame 30 31 else: 32 vidcap.release() 33 cv2.destroyAllWindows() 34 35if __name__ == \u0026#39;__main__\u0026#39;: 36 Fire(main) Every time we press s, it\u0026rsquo;s gonna take the current frame at that time. For the speed, i usually go for 25 but if you want slower you could change it to 10 or lower (as long as it\u0026rsquo;s not 0, please).\nAfter the extraction process, i have 652 images/frames for training process. The 652 images/frames have this proportion (There\u0026rsquo;re a few object in one frame):\n   Pavement Distress Object     Alligator Crack 367   Longitudinal Crack 951   Transverse Crack 243   Potholes 161    Labeling For the labeling i use labelme, you could check the tutorial here and to change labelme format to coco dataset format here . There\u0026rsquo;s nothing much to explain about labeling, you just give box to an object and save with the label you want. So, let\u0026rsquo;s move on.\nHere We Go, It\u0026rsquo;s Training Time! For the training process i use google colaboratory (how to use google colaboratory is beyond this post, sorry) but you could also use other services such as paperspace . Here\u0026rsquo;s an example of command line if you use you local machine or cloud services: Local: 1python train.py --config-file configs/config.yaml Cloud: !python train.py --config-file configs/config.yaml Basically there\u0026rsquo;s no difference so i think it\u0026rsquo;s not that difficult, good luck.\nLoss Function Graph As the training begin, please don\u0026rsquo;t forget to check the loss function. The closer the loss function to zero the better but be carefull so that it doesn\u0026rsquo;t overfitting (a model memorized the training data and have difficulty predicting the testing data). Here\u0026rsquo;s the unscientific tips from me, stop the training process if you don\u0026rsquo;t see any improvement in loss function. For example, if the loss function stuck at 0.9 - 0.5 for quite some time then you should stop the process. Here\u0026rsquo;s my loss function graph:\nTesting Preparation Before testing the model, there\u0026rsquo;re a few things we need to do:\n Copy or move video you want to use into folder input. Copy or move configuration file (*.yaml) into folder configs. Copy or move folder that has training result into folder outputs (in this project the folder name is ssd_custom_coco_format). The folder name must be the same as in configuration file OUTPUT_DIR. If every file and folder in the right places, then let\u0026rsquo;s move on.  It\u0026rsquo;s Testing Time! For this project, there\u0026rsquo;s a problem with the counting. Because i have no idea how to implement tracking so i made the counting in the iteration frame (detection at every frame, which is insane) and that\u0026rsquo;s makes the total counting more than the actual object. To fix this problem (kind of), i do the counting for every 20 frames. The reason was because at every 20 frames, the object detected was closer to the total of actual object than every 10, 15, 25, and 30 frames. So, for the evaluation i\u0026rsquo;m gonna evaluate the detection result every 20 frames. Thanks.\nA Brief Showcase and Explanation of The Results Below is the result:\nVideo Testing 1    Class Name Counting Results Actual Objects     Alligator Crack 2 3   Longitudinal Crack 4 29   Transverse Crack 8 11   Potholes 1 2       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 2 11 0 1   Longitudinal Crack 4 9 1 25   Transverse Crack 6 7 1 5   Potholes 1 12 0 1    Video Testing 2    Class Name Counting Results Actual Objects     Alligator Crack 14 8   Longitudinal Crack 5 6   Transverse Crack 1 4   Potholes 0 2       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 7 3 7 1   Longitudinal Crack 2 8 2 4   Transverse Crack 1 9 0 3   Potholes 0 10 0 2    Video Testing 3    Class Name Counting Results Actual Objects     Alligator Crack 21 8   Longitudinal Crack 7 15   Transverse Crack 1 1   Potholes 2 4       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 8 7 10 0   Longitudinal Crack 4 11 2 11   Transverse Crack 1 14 0 0   Potholes 2 13 0 2    Video Testing 4    Class Name Counting Results Actual Objects     Alligator Crack 23 22   Longitudinal Crack 13 46   Transverse Crack 4 12   Potholes 5 22       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 10 14 8 12   Longitudinal Crack 8 16 3 38   Transverse Crack 2 22 2 10   Potholes 4 20 0 18    From the counting results above we can see that the model struggle to detect longitudinal crack and have a lot of alligator crack detections (detected two times or more). There\u0026rsquo;re 2 reasons for that, the first is that there\u0026rsquo;s not enough small-sized longitudinal crack in training dataset and the second is the frame field-of-view too narrow so that a lot of alligator crack devided into different frames and detected multiple times. Here\u0026rsquo;s an example of that problem:\n Undetected Small Longitudinal Crack:   Multiple Detection of Alligator Crack in Different Frames (there\u0026rsquo;re 2 frames below):  And then, we have the precision and recall of the model as below:\n   Video Precision Recall Accuracy     Video Testing 1 91.43% 46.25% 60.69%   Video Testing 2 50% 36.45% 69.58%   Video Testing 3 77.78% 69.17% 75.45%   Video Testing 4 69.57% 24.42% 53.81%     At the time of making this post, i\u0026rsquo;m still not sure whether to use accuracy or f1-score so for now i\u0026rsquo;m gonna use accuracy.\n The difference between video testing 1 to 4 is the total of small-sized pavement distress and video testing 3 has the least total of small-sized pavement distress of all video. So that means, for this trained weight, we obtain the best accuracy when we have the least small-sized pavement distress.\nConclusion The perfomance of single shot detector is not bad or maybe you could say it\u0026rsquo;s good. Considering the lack of training data, it still can produce above 50% accuracy so you might say that this project has the worst result possible. There\u0026rsquo;re a lot of things you could improve, especially in the amount of training dataset. Good luck!\nFuture Suggestion By no means this is not the best implementation of SSD for pavement distress detection. I have only a few training dataset and a few testing dataset. So you could say what i did here is a minimum requirement that result in minimum performance. You can improve this project quite a lot.\nIf you want to improve this project, you can start from these things:\n Use a lot of training data and testing data. Use a camera that has wide angle lens (because 50mm lens is not wide enough).  Side Notes This is not really important, it\u0026rsquo;s more like a momento for me. In the undergraduate thesis defence(?) there\u0026rsquo;s this examiner who has a misconception about testing process and validation process. That examiner switch the possition of testing process as validation process and validation process as testing process, so that really confusing and we have quite an argument there. I even ask in stackexchange if i\u0026rsquo;m wrong or not and it turns out that examiner has switch the term for testing and validation. Now i feel stupid for having an argument with that examiner.\n  Pytorch maxpool2d .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n SSD: Single Shot Multibox Detector .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "}]