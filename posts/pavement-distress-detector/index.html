<!doctype html><html>
<head>
<title>
Pavement Distress Detector Using Single Shot Detector (SSD)
</title>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=author content="Robertus Chris">
<link rel="shortcut icon" type=image/x-icon href=https://bruhtus.github.io/img/favicon.ico>
<link rel=stylesheet href=https://bruhtus.github.io/style.min.4f970aa135421035a71bdc9bbd750aa2cfbe0ddb080ab1ec5db4173f45494cad.css integrity="sha256-T5cKoTVCEDWnG9ybvXUKos++DdsICrHsXbQXP0VJTK0=">
<link disabled rel=stylesheet href=https://bruhtus.github.io/style-dark.min.0a647fb6c07e04b77b54fa0515d0a683d39ecdb251dba960fe1f966f7ff36fc2.css media="(prefers-color-scheme: dark)" integrity="sha256-CmR/tsB+BLd7VPoFFdCmg9OezbJR26lg/h+Wb3/zb8I=">
<link rel=stylesheet href=https://use.fontawesome.com/releases/v5.8.1/css/all.css integrity=sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf crossorigin=anonymous>
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="https://fonts.googleapis.com/css2?family=Bitter:wght@300&family=EB+Garamond&family=PT+Sans&family=Sura&family=Zilla+Slab&family=Inconsolata&family=Libre+Baskerville&display=swap" rel=stylesheet>
<link rel=stylesheet href=https://bruhtus.github.io/css/custom.css>
<meta property="og:title" content="Pavement Distress Detector Using Single Shot Detector (SSD)">
<meta property="og:description" content="Detect pavement distress using single shot detector (SSD) model.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://bruhtus.github.io/posts/pavement-distress-detector/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-10-11T00:00:00+00:00">
<meta property="article:modified_time" content="2020-10-11T00:00:00+00:00">
<meta itemprop=name content="Pavement Distress Detector Using Single Shot Detector (SSD)">
<meta itemprop=description content="Detect pavement distress using single shot detector (SSD) model."><meta itemprop=datePublished content="2020-10-11T00:00:00+00:00">
<meta itemprop=dateModified content="2020-10-11T00:00:00+00:00">
<meta itemprop=wordCount content="2905">
<meta itemprop=keywords content="Python,"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<h1>Pavement Distress Detector Using Single Shot Detector (SSD)</h1>
<header>
<div class=cd>
<h2><a style=text-decoration:none class=cd href=https://bruhtus.github.io/posts>cd ..</a> || <a style=text-decoration:none class=cd href=https://bruhtus.github.io/>cd</a></h2>
</div>
</header>
<p class=date>
October 11, 2020
<span>&#183;</span>
14 mins
<span>&#183;</span>
Robertus Chris
</p>
<div id=tags>
<ul>
<li><a style=text-decoration:none href=https://bruhtus.github.io/tags/python/>Python</a></li>
</ul>
</div>
<div class=toc>
<details>
<summary>
<div class=details>Table of Contents</div>
</summary>
<blockquote><ul><li>
<a style=text-decoration:none href=#diagram-process-of-this-project aria-label="Diagram Process of This Project">Diagram Process of This Project</a></li><li>
<a style=text-decoration:none href=#a-brief-explanation-about-single-shot-detector-ssd aria-label="A Brief Explanation About Single Shot Detector (SSD)">A Brief Explanation About Single Shot Detector (SSD)</a><ul>
<li>
<a style=text-decoration:none href=#single-shot-detector-ssd-architecture-thats-used-in-this-project aria-label="Single Shot Detector (SSD) Architecture That&amp;rsquo;s Used in This Project">Single Shot Detector (SSD) Architecture That&rsquo;s Used in This Project</a><ul>
<li>
<a style=text-decoration:none href=#base-network aria-label="Base Network">Base Network</a></li><li>
<a style=text-decoration:none href=#layer-6-and-layer-7 aria-label="Layer 6 and Layer 7">Layer 6 and Layer 7</a></li><li>
<a style=text-decoration:none href=#extra-feature-layers aria-label="Extra Feature Layers">Extra Feature Layers</a></li></ul>
</li></ul>
</li><li>
<a style=text-decoration:none href=#take-a-video-training-video-and-testing-video aria-label="Take A Video (Training Video and Testing Video)">Take A Video (Training Video and Testing Video)</a></li><li>
<a style=text-decoration:none href=#setting-up-the-config-file aria-label="Setting Up The Config File">Setting Up The Config File</a><ul>
<li>
<a style=text-decoration:none href=#basic-configuration aria-label="Basic Configuration">Basic Configuration</a></li><li>
<a style=text-decoration:none href=#validation-configuration aria-label="Validation Configuration">Validation Configuration</a></li></ul>
</li><li>
<a style=text-decoration:none href=#the-interface aria-label="The Interface">The Interface</a></li><li>
<a style=text-decoration:none href=#training-preparation aria-label="Training Preparation">Training Preparation</a><ul>
<li>
<a style=text-decoration:none href=#frame-extraction aria-label="Frame Extraction">Frame Extraction</a></li><li>
<a style=text-decoration:none href=#labeling aria-label=Labeling>Labeling</a></li></ul>
</li><li>
<a style=text-decoration:none href=#here-we-go-its-training-time aria-label="Here We Go, It&amp;rsquo;s Training Time!">Here We Go, It&rsquo;s Training Time!</a><ul>
<li>
<a style=text-decoration:none href=#loss-function-graph aria-label="Loss Function Graph">Loss Function Graph</a></li></ul>
</li><li>
<a style=text-decoration:none href=#testing-preparation aria-label="Testing Preparation">Testing Preparation</a></li><li>
<a style=text-decoration:none href=#its-testing-time aria-label="It&amp;rsquo;s Testing Time!">It&rsquo;s Testing Time!</a><ul>
<li>
<a style=text-decoration:none href=#a-brief-showcase-and-explanation-of-the-results aria-label="A Brief Showcase and Explanation of The Results">A Brief Showcase and Explanation of The Results</a><ul>
<li>
<a style=text-decoration:none href=#video-testing-1 aria-label="Video Testing 1">Video Testing 1</a></li><li>
<a style=text-decoration:none href=#video-testing-2 aria-label="Video Testing 2">Video Testing 2</a></li><li>
<a style=text-decoration:none href=#video-testing-3 aria-label="Video Testing 3">Video Testing 3</a></li><li>
<a style=text-decoration:none href=#video-testing-4 aria-label="Video Testing 4">Video Testing 4</a></li></ul>
</li></ul>
</li><li>
<a style=text-decoration:none href=#conclusion aria-label=Conclusion>Conclusion</a></li><li>
<a style=text-decoration:none href=#future-suggestion aria-label="Future Suggestion">Future Suggestion</a></li><li>
<a style=text-decoration:none href=#side-notes aria-label="Side Notes">Side Notes</a></li></ul>
</blockquote>
</details>
</div>
<div id=contentBody>
<blockquote>
<p>This was my graduation project, in this article i&rsquo;m gonna explain what i did in my graduation project. This project was based on <a href=https://github.com/lufficc/SSD target=_blank rel="noreferrer noopener">Congcong Li&rsquo;s Project</a>
.</p>
</blockquote>
<h2 id=diagram-process-of-this-project>Diagram Process of This Project</h2>
<p><img src=diagram-process.png alt="Diagram process"><br></p>
<h2 id=a-brief-explanation-about-single-shot-detector-ssd>A Brief Explanation About Single Shot Detector (SSD)</h2>
<p>Single shot detector is a deep learning method presented by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed4, Cheng-Yang Fu, Alexander C. Berg in their research paper <a href=https://arxiv.org/abs/1512.02325 target=_blank rel="noreferrer noopener">SSD: Single Shot Multibox Detector</a>
. There are 2 commonly used SSD model, that is, SSD300 and SSD512.
<br>
Here&rsquo;s a brief explanation about SSD300 and SSD512:</p>
<ul>
<li>SSD300: More fast.</li>
<li>SSD512: More accurate.</li>
</ul>
<p>Long story short, SSD300 is about speed. If you need speed than you should probably use SSD300 (i haven&rsquo;t tried the mobilenet as base network at the time to type this, so at this time knowledge SSD300 is faster than SSD512). Meanwhile, SSD512 is about accuracy. It doesn&rsquo;t really show up in image processing but in video processing, i notice that there&rsquo;s a frame rate drop while doing live object detection. To be fair, SSD300 has frame rate drop as well but it&rsquo;s still usable (around 7-10 frame per second) but SSD512 has frame rate around 3-5 frame per second.
Who want to watch a video with 3 fps?? If you&rsquo;re that kind of person then, go ahead. You do you mate.</p>
<p>For the record, at that time when I try live detection, i use opencv to display live detection video. i&rsquo;m not sure whether it is opencv fault or the model fault because if I save the video result, the video itself has no frame rate drop. It&rsquo;s weird but it happens, so let&rsquo;s go on with saving the video and forget about live detection (for now, until i find some way to do live detection).</p>
<p>So, in this project i&rsquo;m not gonna make it live detection. Rather than live detections, we&rsquo;re gonna save the video result first and then display it later. That way it could also reduce some computational cost.</p>
<p>For those who still confused about live detection, to make things simpler, live detection is when you process the video, detect the object, and play the video at the same time. After you detect the object, you immediately display the frame that just recently processed and then processed the next frame. Repeat.</p>
<h3 id=single-shot-detector-ssd-architecture-thats-used-in-this-project>Single Shot Detector (SSD) Architecture That&rsquo;s Used in This Project</h3>
<p>As explained above, in this project we&rsquo;re gonna use SSD512. SSD512 is basically SSD with input image 512x512. The basic architecture of SSD contains 2 part, base network and extra feature layers. The base network layers are based on standard architecture used for high quality image classification (truncated before classification layers). The extra feature layers used for multi-scale feature maps for detection and convolutional predictors for detection.</p>
<p>Here is an architecture single shot detector that used in this project (made this with <a href=http://alexlenail.me/NN-SVG/AlexNet.html target=_blank rel="noreferrer noopener">NN architecture maker</a>
):</p>
<p><img src=ssd-architecture.png alt="Architecture SSD"><br></p>
<p>Information:</p>
<ol>
<li>Input image.</li>
<li>Base Network (truncated before classification layers).</li>
<li>Layer 6 and layer 7 of base network (from fully-connected layer turned into classification layer).</li>
<li>Extra feature layers.</li>
<li>Collection of boxes and scores.</li>
</ol>
<h4 id=base-network>Base Network</h4>
<p>The base network used in this project is Visual Geometry Group (VGG). I chose VGG because of transfer learning capability so that i could have a good result with small dataset. To be more specific, in this project i used VGG16, here is a brief explanation of each layers:</p>
<ol>
<li>In the first layer, there&rsquo;s a convolutional process with kernel filter 3x3 and stride (total shift filter per pixel) 1 pixel. That process repeat 2 times and then did some max pooling with kernel filter 2x2 and stride 2 pixel.</li>
<li>In the second layer until fourth layer, the model did the same thing as in first layer.</li>
<li>The difference was in fifth layer. In fifth layer, the convolution process still the same as the other four layers but the max pooling process was different from the other four layers. The max pooling process used kernel filter 3x3 with stride 1 pixel with padding (adding zero value around pixel image) 1. You can check the illustration below to understand the process of max pooling with kernel filter 3x3, stride 1, and padding 1.</li>
</ol>
<p><img src=max-pooling-illustration.png alt="Ilustration max-pooling"></p>
<p>And here&rsquo;s a VGG16 after truncated from classification layers:</p>
<p><img src=truncated-vgg16.jpg alt="Truncated vgg16"></p>
<p>If you want to calculate the result from max polling, you can use this equation <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p>
<p><img src=max-pooling-equation.png alt="Max-pooling equation"><br></p>
<p>Information:</p>
<ol>
<li>kernel_size, stride, padding, and dilation can be 1 integer (in this case, the value for height and width are the same) or 2 integer (in this case, the first integer is height and the second integer is width).</li>
<li>For more info you can see <a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html target=_blank rel="noreferrer noopener">pytorch page</a>
.</li>
</ol>
<p>Here&rsquo;s some example of max pooling calculation with input 32x32, kernel filter 3x3, stride 1, padding 1, and dilation 1:</p>
<p><img src=max-pooling-calculation.png alt="Max-pooling calculation"></p>
<h4 id=layer-6-and-layer-7>Layer 6 and Layer 7</h4>
<p>After feature extraction process in base network, the next layers is to change layer 6 and 7 of base network from fully-connected into convolutional layer with subsample parameters from fully-connected 6 (fc6) and fully-connected 7 (fc7). The convolution operation used in layer 6 and layer 7 is atrous convolution, you can see atrous convolution shift below:</p>
<p><img src=atrous-convolution.png alt="Atrous convolution"></p>
<p>With atrous convolution we can expand area of observation for feature extraction while maintaning the amount of parameters fewer than traditional convolution operation.</p>
<h4 id=extra-feature-layers>Extra Feature Layers</h4>
<p>Extra feature layers is a prediction layers. In this layer, the model predict the object using default box. Default box is a box with various aspect ratio in every location of feature maps with different size. You can see an example of default box below <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p>
<p><img src=default-box.png alt="Default box"></p>
<p>In the last layer is a collection of default boxes which closer to ground truth box with confidence score from that default boxes.</p>
<h2 id=take-a-video-training-video-and-testing-video>Take A Video (Training Video and Testing Video)</h2>
<p>In this part, i&rsquo;m gonna explain about the video used in this project. The camera configuration, the place where the video taken, the camera angle and height from the road.</p>
<p>The place where the video taken was in Surabaya, at Kertajaya Indah Timur IX, Kertajaya Indah Timur X, and Kertajaya Indah Timur XI. The camera angle was perpendicular(?) with the road (90 degrees) and the camera position from the road was 200 cm.</p>
<p>There&rsquo;re 7 video taken, 3 for training and 4 for testing. The format of the video was <code>*.mp4</code>. You can check the location partition of the video taken below:</p>
<p><img src=video-taken.jpg alt="Video taken"><br></p>
<p>The black block is for testing and the white block is for training. You can check the position of the camera below:</p>
<p><img src=camera-position.png alt="Camera position"></p>
<h2 id=setting-up-the-config-file>Setting Up The Config File</h2>
<p>For more detailed configuration please check <a href=https://github.com/lufficc/SSD/blob/master/DEVELOP_GUIDE.md target=_blank rel="noreferrer noopener">develop guide by Congcong Li</a>
. In this post i&rsquo;m gonna explain it the easiest way.</p>
<h3 id=basic-configuration>Basic Configuration</h3>
<p>To make things easier, copy the format dataset you want. For example, in this project i want to use COCO dataset format. Then, i copied the <code>coco.py</code> in the path <code>ssd/data/datasets/</code> and rename it to <code>my_dataset.py</code>. After that, edit the class names for your classification class. In this project, the class i&rsquo;m gonna use is alligator crack, longitudinal crack, transverse crack, and pothole. Also, don&rsquo;t forget to change the class <code>COCODataset</code> to <code>MyDataset</code>.</p>
<p>The next step is to add those configuration to <code>__init__.py</code> in ssd/data/datasets/. For example:</p>
<div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>.my_dataset</span> <span style=color:#6ab825;font-weight:700>import</span> MyDataset

_DATASETS = {
    <span style=color:#ed9d13>&#39;VOCDataset&#39;</span>: VOCDataset,
    <span style=color:#ed9d13>&#39;COCODataset&#39;</span>: COCODataset,
    <span style=color:#ed9d13>&#39;MyDataset&#39;</span>: MyDataset,
}
</code></pre></div><p>Another next step is to add the path of your datasets and anotations to the <code>path_catlog.py</code> in <code>ssd/config/</code>. For example:</p>
<div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>os</span>

<span style=color:#6ab825;font-weight:700>class</span> <span style=color:#447fcf;text-decoration:underline>DatasetCatalog</span>:
    DATA_DIR = <span style=color:#ed9d13>&#39;datasets&#39;</span>
    DATASETS = {
        <span style=color:#ed9d13>&#39;my_custom_train_dataset&#39;</span>: {
            <span style=color:#ed9d13>&#34;data_dir&#34;</span>: <span style=color:#ed9d13>&#34;train&#34;</span>,
            <span style=color:#ed9d13>&#34;ann_file&#34;</span>: <span style=color:#ed9d13>&#34;annotations/train.json&#34;</span>
        },

        <span style=color:#ed9d13>&#39;my_custom_validation_dataset&#39;</span>: {
            <span style=color:#ed9d13>&#34;data_dir&#34;</span>: <span style=color:#ed9d13>&#34;validation&#34;</span>,
            <span style=color:#ed9d13>&#34;ann_file&#34;</span>: <span style=color:#ed9d13>&#34;annotations/validation.json&#34;</span>
        },
    }

    <span style=color:orange>@staticmethod</span>
    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>get</span>(name):
        <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#ed9d13>&#34;my_custom_train_dataset&#34;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
            my_custom_train_dataset = DatasetCatalog.DATA_DIR
            attrs = DatasetCatalog.DATASETS[name]
            args = <span style=color:#24909d>dict</span>(
                    data_dir = os.path.join(my_custom_train_dataset, attrs[<span style=color:#ed9d13>&#39;data_dir&#39;</span>]),
                    ann_file = os.path.join(my_custom_train_dataset, attrs[<span style=color:#ed9d13>&#39;ann_file&#39;</span>]),
            )
            <span style=color:#6ab825;font-weight:700>return</span> <span style=color:#24909d>dict</span>(factory=<span style=color:#ed9d13>&#34;MyDataset&#34;</span>, args=args)

         <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#ed9d13>&#34;my_custom_test_dataset&#34;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
            my_custom_train_dataset = DatasetCatalog.DATA_DIR
            attrs = DatasetCatalog.DATASETS[name]
            args = <span style=color:#24909d>dict</span>(
                    data_dir = os.path.join(my_custom_train_dataset, attrs[<span style=color:#ed9d13>&#39;data_dir&#39;</span>]),
                    ann_file = os.path.join(my_custom_train_dataset, attrs[<span style=color:#ed9d13>&#39;ann_file&#39;</span>]),
            )
            <span style=color:#6ab825;font-weight:700>return</span> <span style=color:#24909d>dict</span>(factory=<span style=color:#ed9d13>&#34;MyDataset&#34;</span>, args=args)
</code></pre></div><p>And finally, for the <code>*.yaml</code> file for configuration i copied <code>vgg_ssd512_coco_trainval35k.yaml</code> in <code>configs</code> folder and rename it to <code>config.yaml</code>. What i changed from that file was the train and test (or more like validation) like in <code>path_catlog.py</code>, the batch size, and num_classes. I changed batch size because my laptop gpu only capable of 4 batch size. Here&rsquo;s an example:</p>
<div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python>Model:
    num_classes: <span style=color:#3677a9>5</span> <span style=color:#999;font-style:italic>#the __background__ counted</span>
    ...
    DATASETS:
        TRAIN: (<span style=color:#ed9d13>&#34;my_custom_train_dataset&#34;</span>, )
        TEST: (<span style=color:#ed9d13>&#34;my_custom_test_dataset&#34;</span>, )
    SOLVER:
        ...
        BATCH_SIZE: <span style=color:#3677a9>4</span>
        ...

    OUTPUT_DIR: <span style=color:#ed9d13>&#39;outputs/ssd_custom_coco_format&#39;</span>
</code></pre></div><p>You don&rsquo;t need to create folder <code>ssd_custom_coco_format</code>, when the training begin the folder gonna created automatically (if the folder didn&rsquo;t exist).</p>
<h3 id=validation-configuration>Validation Configuration</h3>
<p>First of all, copy <code>coco</code> folder in <code>ssd/data/datasets/evaluation/</code> and rename it to <code>my_dataset</code>. Rename the <code>def coco_evaluation</code> to <code>def my_dataset_evaluation</code> in file <code>__init__.py</code>. After that, add folder <code>my_dataset</code> to file <code>__init__.py</code> in <code>ssd/data/datasets/evaluation/</code>. For example:</p>
<div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>ssd.data.datasets</span> <span style=color:#6ab825;font-weight:700>import</span> VOCDataset, COCODataset, MyDataset
...
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>.my_dataset</span> <span style=color:#6ab825;font-weight:700>import</span> my_dataset_evaluation

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>evaluate</span>(dataset, predictions, output_dir, **kwargs):
    ...
    <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#24909d>isinstance</span>(dataset, MyDataset);
        <span style=color:#6ab825;font-weight:700>return</span> my_dataset_evaluation(**args)
    <span style=color:#6ab825;font-weight:700>else</span>:
        <span style=color:#6ab825;font-weight:700>raise</span> NotImplementError
</code></pre></div><h2 id=the-interface>The Interface</h2>
<p>For the interface, i&rsquo;m using python library <code>streamlit</code>. Streamlit is more like web interface rather than common graphical user interface (GUI). Here&rsquo;s how the interface looks like:</p>
<p><img src=streamlit-interface.png alt="The interface"></p>
<p>There&rsquo;s a problem with file uploader at the time (streamlit version 0.59.0), i can&rsquo;t upload file larger than 100 mb meanwhile the limit of file uploader should be 200 mb at that time. You can check the issue <a href=https://discuss.streamlit.io/t/network-error-when-uploading-files-larger-than-100mb/2646 target=_blank rel="noreferrer noopener">here</a>
, it seems like they&rsquo;re already fixed it but i haven&rsquo;t try it yet. So, when making this project i&rsquo;m using the dropdown menu bar.</p>
<h2 id=training-preparation>Training Preparation</h2>
<p>Before training the model, we need to do some preparation. There&rsquo;re two steps in this process, frame extraction and labeling. Without further ado, let&rsquo;s get started.</p>
<h3 id=frame-extraction>Frame Extraction</h3>
<p>In this process, i used python library <code>opencv</code> to extract some frame. Here&rsquo;s the script:</p>
<div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>cv2</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>time</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>fire</span> <span style=color:#6ab825;font-weight:700>import</span> Fire
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>tqdm</span> <span style=color:#6ab825;font-weight:700>import</span> tqdm

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>main</span>(video_file, path_save, speed): <span style=color:#999;font-style:italic># the lower the speed the fastest the frame_rates, speed = 0 (pause)</span>
    vidcap = cv2.VideoCapture(video_file)
    current_frame = <span style=color:#3677a9>0</span>
    speed_frame = speed

    <span style=color:#6ab825;font-weight:700>while</span> (vidcap.isOpened()):
        success, frame = vidcap.read() <span style=color:#999;font-style:italic># success = retrival value for frame</span>
        length = <span style=color:#24909d>int</span>(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
        <span style=color:#24909d>print</span>(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#39;Current Frame: </span><span style=color:#ed9d13>{</span>current_frame<span style=color:#ed9d13>}</span><span style=color:#ed9d13>/</span><span style=color:#ed9d13>{</span>length<span style=color:#ed9d13>}</span><span style=color:#ed9d13>&#39;</span>)
        current_frame += <span style=color:#3677a9>1</span>

        <span style=color:#6ab825;font-weight:700>if</span> success == <span style=color:#6ab825;font-weight:700>True</span>:
            cv2.imshow(<span style=color:#ed9d13>&#39;Video&#39;</span>, frame)
            <span style=color:#6ab825;font-weight:700>if</span> cv2.waitKey(speed) &amp; <span style=color:#3677a9>0xFF</span> == <span style=color:#24909d>ord</span>(<span style=color:#ed9d13>&#39;s&#39;</span>): <span style=color:#999;font-style:italic># press s to save the frame</span>
                cv2.imwrite(<span style=color:#ed9d13>f</span><span style=color:#ed9d13>&#34;</span><span style=color:#ed9d13>{</span>path_save<span style=color:#ed9d13>}</span><span style=color:#ed9d13>/frame_</span><span style=color:#ed9d13>{</span>current_frame<span style=color:#ed9d13>}</span><span style=color:#ed9d13>.jpg&#34;</span>, frame)

            <span style=color:#6ab825;font-weight:700>elif</span> cv2.waitKey(speed) &amp; <span style=color:#3677a9>0xFF</span> == <span style=color:#24909d>ord</span>(<span style=color:#ed9d13>&#39;q&#39;</span>): <span style=color:#999;font-style:italic># press q to quit</span>
                <span style=color:#6ab825;font-weight:700>break</span>

            <span style=color:#6ab825;font-weight:700>elif</span> cv2.waitKey(speed) &amp; <span style=color:#3677a9>0xFF</span> == <span style=color:#24909d>ord</span>(<span style=color:#ed9d13>&#39;w&#39;</span>): <span style=color:#999;font-style:italic># play/pause</span>
                <span style=color:#6ab825;font-weight:700>if</span> speed != <span style=color:#3677a9>0</span>:
                    speed = <span style=color:#3677a9>0</span>
                <span style=color:#6ab825;font-weight:700>elif</span> speed == <span style=color:#3677a9>0</span>:
                    speed = speed_frame

        <span style=color:#6ab825;font-weight:700>else</span>:
            vidcap.release()
            cv2.destroyAllWindows()

<span style=color:#6ab825;font-weight:700>if</span> __name__ == <span style=color:#ed9d13>&#39;__main__&#39;</span>:
    Fire(main)
</code></pre></div><p>Every time we press <code>s</code>, it&rsquo;s gonna take the current frame at that time. For the speed, i usually go for 25 but if you want slower you could change it to 10 or lower (as long as it&rsquo;s not 0, please).</p>
<p>After the extraction process, i have 652 images/frames for training process. The 652 images/frames have this proportion (There&rsquo;re a few object in one frame):</p>
<table>
<thead>
<tr>
<th>Pavement Distress</th>
<th>Object</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>367</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>951</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>243</td>
</tr>
<tr>
<td>Potholes</td>
<td>161</td>
</tr>
</tbody>
</table>
<h3 id=labeling>Labeling</h3>
<p>For the labeling i use labelme, you could check the tutorial <a href=https://www.dlology.com/blog/how-to-create-custom-coco-data-set-for-instance-segmentation/ target=_blank rel="noreferrer noopener">here</a>
and to change labelme format to coco dataset format <a href=https://github.com/Tony607/labelme2coco target=_blank rel="noreferrer noopener">here</a>
. There&rsquo;s nothing much to explain about labeling, you just give box to an object and save with the label you want. So, let&rsquo;s move on.</p>
<h2 id=here-we-go-its-training-time>Here We Go, It&rsquo;s Training Time!</h2>
<p>For the training process i use google colaboratory (how to use google colaboratory is beyond this post, sorry) but you could also use other services such as <a href="https://www.paperspace.com/?utm_expid=.XZhCPCNrQCuE1jH9t8bIgg.0&utm_referrer=" target=_blank rel="noreferrer noopener">paperspace</a>
. Here&rsquo;s an example of command line if you use you local machine or cloud services: <br>
Local: <br></p>
<div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell>python train.py --config-file configs/config.yaml
</code></pre></div><p>Cloud: <br></p>
<pre tabindex=0><code>!python train.py --config-file configs/config.yaml
</code></pre><p>Basically there&rsquo;s no difference so i think it&rsquo;s not that difficult, good luck.</p>
<h3 id=loss-function-graph>Loss Function Graph</h3>
<p>As the training begin, please don&rsquo;t forget to check the loss function. The closer the loss function to zero the better but be carefull so that it doesn&rsquo;t overfitting (a model memorized the training data and have difficulty predicting the testing data). Here&rsquo;s the unscientific tips from me, stop the training process if you don&rsquo;t see any improvement in loss function. For example, if the loss function stuck at 0.9 - 0.5 for quite some time then you should stop the process. Here&rsquo;s my loss function graph:</p>
<p><img src=loss-function-graph.jpg alt="Loss function graph"></p>
<h2 id=testing-preparation>Testing Preparation</h2>
<p>Before testing the model, there&rsquo;re a few things we need to do:</p>
<ol>
<li>Copy or move video you want to use into folder <code>input</code>.</li>
<li>Copy or move configuration file (*.yaml) into folder <code>configs</code>.</li>
<li>Copy or move folder that has training result into folder <code>outputs</code> (in this project the folder name is <code>ssd_custom_coco_format</code>). The folder name must be the same as in configuration file <code>OUTPUT_DIR</code>.</li>
<li>If every file and folder in the right places, then let&rsquo;s move on.</li>
</ol>
<h2 id=its-testing-time>It&rsquo;s Testing Time!</h2>
<p>For this project, there&rsquo;s a problem with the counting. Because i have no idea how to implement tracking so i made the counting in the iteration frame (detection at every frame, which is insane) and that&rsquo;s makes the total counting more than the actual object. To fix this problem (kind of), i do the counting for every 20 frames. The reason was because at every 20 frames, the object detected was closer to the total of actual object than every 10, 15, 25, and 30 frames. So, for the evaluation i&rsquo;m gonna evaluate the detection result every 20 frames. Thanks.</p>
<h3 id=a-brief-showcase-and-explanation-of-the-results>A Brief Showcase and Explanation of The Results</h3>
<p>Below is the result:</p>
<h4 id=video-testing-1>Video Testing 1</h4>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>Counting Results</th>
<th>Actual Objects</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>4</td>
<td>29</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>8</td>
<td>11</td>
</tr>
<tr>
<td>Potholes</td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>True Positive</th>
<th>True Negative</th>
<th>False Positive</th>
<th>False Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>2</td>
<td>11</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>4</td>
<td>9</td>
<td>1</td>
<td>25</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>6</td>
<td>7</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>Potholes</td>
<td>1</td>
<td>12</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<h4 id=video-testing-2>Video Testing 2</h4>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>Counting Results</th>
<th>Actual Objects</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>14</td>
<td>8</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>5</td>
<td>6</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>Potholes</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>True Positive</th>
<th>True Negative</th>
<th>False Positive</th>
<th>False Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>7</td>
<td>3</td>
<td>7</td>
<td>1</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>2</td>
<td>8</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>1</td>
<td>9</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>Potholes</td>
<td>0</td>
<td>10</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id=video-testing-3>Video Testing 3</h4>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>Counting Results</th>
<th>Actual Objects</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>21</td>
<td>8</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>7</td>
<td>15</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Potholes</td>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>True Positive</th>
<th>True Negative</th>
<th>False Positive</th>
<th>False Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>8</td>
<td>7</td>
<td>10</td>
<td>0</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>4</td>
<td>11</td>
<td>2</td>
<td>11</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>1</td>
<td>14</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Potholes</td>
<td>2</td>
<td>13</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
<h4 id=video-testing-4>Video Testing 4</h4>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>Counting Results</th>
<th>Actual Objects</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>23</td>
<td>22</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>13</td>
<td>46</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>4</td>
<td>12</td>
</tr>
<tr>
<td>Potholes</td>
<td>5</td>
<td>22</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Class Name</th>
<th>True Positive</th>
<th>True Negative</th>
<th>False Positive</th>
<th>False Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alligator Crack</td>
<td>10</td>
<td>14</td>
<td>8</td>
<td>12</td>
</tr>
<tr>
<td>Longitudinal Crack</td>
<td>8</td>
<td>16</td>
<td>3</td>
<td>38</td>
</tr>
<tr>
<td>Transverse Crack</td>
<td>2</td>
<td>22</td>
<td>2</td>
<td>10</td>
</tr>
<tr>
<td>Potholes</td>
<td>4</td>
<td>20</td>
<td>0</td>
<td>18</td>
</tr>
</tbody>
</table>
<p>From the counting results above we can see that the model struggle to detect longitudinal crack and have a lot of alligator crack detections (detected two times or more). There&rsquo;re 2 reasons for that, the first is that there&rsquo;s not enough small-sized longitudinal crack in training dataset and the second is the frame field-of-view too narrow so that a lot of alligator crack devided into different frames and detected multiple times. Here&rsquo;s an example of that problem:</p>
<ul>
<li>Undetected Small Longitudinal Crack:</li>
</ul>
<p><img src=undetected-small-crack.jpg alt="Undetected small longitudinal crack"></p>
<ul>
<li>Multiple Detection of Alligator Crack in Different Frames (there&rsquo;re 2 frames below):</li>
</ul>
<p><img src=too-much-detection-1.jpg alt="Too much detection first">
<img src=too-much-detection.jpg alt="Too much detection second"></p>
<p>And then, we have the precision and recall of the model as below:</p>
<table>
<thead>
<tr>
<th>Video</th>
<th>Precision</th>
<th>Recall</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video Testing 1</td>
<td>91.43%</td>
<td>46.25%</td>
<td>60.69%</td>
</tr>
<tr>
<td>Video Testing 2</td>
<td>50%</td>
<td>36.45%</td>
<td>69.58%</td>
</tr>
<tr>
<td>Video Testing 3</td>
<td>77.78%</td>
<td>69.17%</td>
<td>75.45%</td>
</tr>
<tr>
<td>Video Testing 4</td>
<td>69.57%</td>
<td>24.42%</td>
<td>53.81%</td>
</tr>
</tbody>
</table>
<blockquote>
<p>At the time of making this post, i&rsquo;m still not sure whether to use accuracy or f1-score so for now i&rsquo;m gonna use accuracy.</p>
</blockquote>
<p>The difference between video testing 1 to 4 is the total of small-sized pavement distress and video testing 3 has the least total of small-sized pavement distress of all video. So that means, for this trained weight, we obtain the best accuracy when we have the least small-sized pavement distress.</p>
<h2 id=conclusion>Conclusion</h2>
<p>The perfomance of single shot detector is not bad or maybe you could say it&rsquo;s good. Considering the lack of training data, it still can produce above 50% accuracy so you might say that this project has the worst result possible. There&rsquo;re a lot of things you could improve, especially in the amount of training dataset. Good luck!</p>
<h2 id=future-suggestion>Future Suggestion</h2>
<p>By no means this is not the best implementation of SSD for pavement distress detection. I have only a few training dataset and a few testing dataset. So you could say what i did here is a minimum requirement that result in minimum performance. You can improve this project quite a lot.</p>
<p>If you want to improve this project, you can start from these things:</p>
<ol>
<li>Use a lot of training data and testing data.</li>
<li>Use a camera that has wide angle lens (because 50mm lens is not wide enough).</li>
</ol>
<h2 id=side-notes>Side Notes</h2>
<p>This is not really important, it&rsquo;s more like a momento for me. In the undergraduate thesis defence(?) there&rsquo;s this examiner who has a misconception about testing process and validation process. That examiner switch the possition of testing process as validation process and validation process as testing process, so that really confusing and we have quite an argument there. I even ask in <a href=https://stats.stackexchange.com/q/484584 target=_blank rel="noreferrer noopener">stackexchange</a>
if i&rsquo;m wrong or not and it turns out that examiner has switch the term for testing and validation. Now i feel stupid for having an argument with that examiner.</p>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p><a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html target=_blank rel="noreferrer noopener">Pytorch maxpool2d</a>
.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p><a href=https://arxiv.org/abs/1512.02325 target=_blank rel="noreferrer noopener">SSD: Single Shot Multibox Detector</a>
.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
<footer>
<p>
&copy; 2020-present <a style=text-decoration:none href=https://bruhtus.github.io/resume target=_blank rel="noreferrer noopener">Robertus Chris</a>
<span>&#183;</span>
<a style=text-decoration:none href=https://bruhtus.github.io/index.xml target=_blank rel="noreferrer noopener">RSS Feed</a>
<span>&#183;</span>
Based on <a style=text-decoration:none href=https://github.com/koirand/pulp/ target=_blank rel="noreferrer noopener">pulp</a> and <a style=text-decoration:none href=https://github.com/adityatelange/hugo-PaperMod target=_blank rel="noreferrer noopener">papermod</a> theme
</p>
</footer>
<button onclick=topFunction() id=myBtn title="Go to top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="#d0d0d0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 15l-6-6-6 6"/></svg>
</button>
<script>var mybutton=document.getElementById("myBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://bruhtus.github.io/js/custom.js></script>
</body>
</html>