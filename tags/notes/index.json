[{"ref":"https://bruhtus.github.io/posts/reflecting-on-intelligence/","title":"Reflecting on Intelligence","section":"posts","tags":["Notes"],"date":"2024.05.26","body":"Over the years i\u0026rsquo;ve been thinking whether having a high intelligence is a blessing or a curse. Humans, who supposedly have the highest intelligence over other living creatures on earth, seems like having more stress than other living creatures.\nWith those high intelligence, human make a lot of innovation and one of them is electricity. One of the benefit of electricity is that electricity enable human doing activity at night, and that can cause some problems, one of them is lack of sleep. With electricity, we can even be productive at night which can cause us to forget about sleep or rest in general. When we lack of sleep, our performance will drop the next day and often times can cause some trouble during day-to-day activities. If we don\u0026rsquo;t address our lack of sleep, it will cause us some health issue.\nThe point is, with every innovations human made, human life getting more complex. For example, electricity enable us to do activity at night easily which make us need to keeping track of time so that we can have enough sleep. I\u0026rsquo;m sure there are some people who is counting how much they get to sleep before actually sleeping and in turn make those people anxious about oversleep which make their sleep not peaceful.\nThose complexity in human day-to-day life make me thinking, is having a high intelligence a good thing or a bad thing? For example, i\u0026rsquo;ve never seen an animal having lack of sleep and even if there\u0026rsquo;s an animal with lack of sleep, those animal probably in the minority. Let\u0026rsquo;s take cat for an example, have you ever see a cat keeping track of time when to sleep? Personally, i\u0026rsquo;ve never seen that kind of cat before. The kind of cat i\u0026rsquo;ve seen is the one who sleep whenever they want.\nIt\u0026rsquo;s like human trying so hard to make their life better at a cost of their freedom. I know that living in total freedom can cause a lot of problems, but that does not mean we should give up on our freedom. For example, with the innovation of instant messaging app like whatsapp, we can get an instant feedback when messaging someone. Matter fact, because of those instant messaging app, a lot of people expect we respond immediately after they send their message which can cause anxiety for some people. Let\u0026rsquo;s say you are already clock out from work at 5 PM, and then you want to enjoy dinner with your friends. In the middle of dinner, you get a message from your boss asking about some details of the project you\u0026rsquo;re currently in. Some people have the authority to ignore those kind of messages outside of working hours, but most people don\u0026rsquo;t. The point is, because of instant messaging app, the distinction between working and non-working hours almost non-existent because a lot of people expecting a fast response for their message. Yeah you should quit from those kind of company, but then again, most company do that and slowly those kind of things become a norm. It\u0026rsquo;s especially hard if you met a hypocrite who say \u0026ldquo;you should have another activity outside of working\u0026rdquo; and then the next thing they do is message you with \u0026ldquo;please submit a report for project X before 10 PM today\u0026rdquo; at 9 PM.\nThat\u0026rsquo;s why i keep thinking, \u0026ldquo;is having a high intelligence a blessing or a curse?\u0026rdquo;. If we compare our life with cat\u0026rsquo;s life, our life seems more miserable than a cat. Most of the cat i\u0026rsquo;ve seen enjoying their life to the fullest without anxiety of the next day, unlike most human.\n"},{"ref":"https://bruhtus.github.io/posts/manage-dotfiles-with-git/","title":"Manage Dotfiles With Git Working Repository","section":"posts","tags":["Notes","Linux","Git"],"date":"2024.04.08","body":"A Brief Intro For those who read this article without knowing what dotfiles are, it\u0026rsquo;s basically a configuration files. The purpose of backing up dotfiles is so that we don\u0026rsquo;t need to reconfigure our tools on a new system.\nThere are a lot of ways to backup and manage dotfiles, and one of them is using git.\nIn this article we will explore how to backup and manage dotfiles using git, the full working git repository rather than the bare one which a lot of people using.\nHow? What i like to do on my free time is browsing random stuff on internet without a goal, just mindlessly browsing some topic that i\u0026rsquo;m interested in and see if i could find something interesting. Letting your mind wondering, some people said.\nWhile i\u0026rsquo;m browsing on the internet about zsh, i found this github issue discussion about managing dotifles . And when i read through those discussions, i wonder if i can use the full working git repository, which basically the default git repository, rather than the bare one. So let\u0026rsquo;s try that, i guess.\nBack when i first trying to backup my dotfiles, i only know two method:\n Using git bare repository from atlassian article . Using GNU stow which i forgot where i learn that from.  The method i pick for my first dotfiles manager was git bare repository. The reason i go with git bare repository instead of GNU stow is that i don\u0026rsquo;t really want to mindlessly spend my inodes by creating a bunch of symlinks. I have experience with running out of inodes, which basically means that you have some storage left but you cannot create a new file. I know that linux or maybe unix-like system these day has quite a large number of inodes, but that does not mean i need to be wasteful of inodes, right?\nWhat bugging me for quite a while when using git bare repository to manage my dotfiles is that, do i really need to use the bare version of git repository to manage my dotfiles? And just recently i found that the answer is no.\nSo, how do we use the full git working directory, the default one instead of the bare one? It turns out that we can separate the git directory (the .git directory) and the working directory without using the bare repository!\nPlease keep in mind that my experience with git at the time is less than 6 months, maybe around 3 months. So it took me everything i got to just following the git bare dotfiles manager guide. Now, fast forward 2 years later, i made my own dotfiles manager using git. You can check them on my dotfiles repo here .\nThe key point of using git working repository instead of bare repository is this:\ngit --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; init What the command above do is initialize git repository with the specified git directory and working directory. If we didn\u0026rsquo;t specified the working directory but specified the git directory, git will create the bare repository instead of working repository. The key part of this method is --git-dir=\u0026quot;...\u0026quot; --work-tree=\u0026quot;...\u0026quot;, so you might want to create an alias like this:\nalias config=\u0026#39;git --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34;\u0026#39; Now you might be wondering, why should we use working repository instead of bare repository? It\u0026rsquo;s because we will most likely change our dotfiles, which basically means we\u0026rsquo;re still \u0026ldquo;working\u0026rdquo; on the dotfiles. So it make more sense to use working repository instead of bare repository because we\u0026rsquo;re still changing something to repository. From what i know, git bare repository is good for sharing and backup files that we don\u0026rsquo;t really change. For example, let\u0026rsquo;s say you are afraid that github will suspend your account and you don\u0026rsquo;t have any backup for your private repository, so you basically lost your private repository. With git bare repository, you can setup a system that only save the git objects of the repo (the one inside .git directory) and update it regularly by pulling from the remote repository. And because you only save the git objects instead of the actual files, your storage usage is lower than storing the actual files. As far as i know, that\u0026rsquo;s the benefit of git bare repository. With that in mind, it\u0026rsquo;s make more sense to use a working repository instead of bare repository in this case.\nThere are 3 important step when we want to get the existing dotfiles on a new system:\n Initialize new git repo. Doing git reset, the default or --mixed one. Doing git checkout.  We already talking about the initialize part for a bit before, which basically just the usual git init. Now let\u0026rsquo;s talk about the git reset part. git reset or git reset --mixed is basically reset the index or staging area on current branch with specific commit. That means, if we don\u0026rsquo;t have any commit on current branch, git reset or git reset --mixed will fill up our index or staging area with commit we specify. Now after we fill up our index or staging area with specific commit, we can generate all or specific files with git checkout -- \u0026lt;path\u0026gt;.\nHonestly the most interesting part is the git checkout part. With git checkout, we can do full dotfiles init or partial init. What i mean by partial init is that when we don\u0026rsquo;t need all the files on the new system, let\u0026rsquo;s say we only our vim config, we can just do git checkout -- ~/.vim and git will only generate the .vim directory with all of the files in it. The benefit of that, is we can put all our config in one git repository and still able to only generate the files we need rather than all the files in the git repository. To initialize all the files in our dotfiles repository, we can do git checkout -- ~.\nSo, to sum up the 3 important command for this to work is:\ngit --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; init git --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; reset git --git-dir=\u0026#34;...\u0026#34; --work-tree=\u0026#34;...\u0026#34; checkout -- \u0026lt;path\u0026gt; Alright, that\u0026rsquo;s it for this article. I will let you explore the implementation and the possibilities of this method yourself. See you next time!\nReferences  Romkatv\u0026rsquo;s bootstrap dotfiles  List of dotfiles manager .  "},{"ref":"https://bruhtus.github.io/posts/the-time-artificial-intelligence-take-over-humanity/","title":"The Time Artificial Intelligence Take Over Humanity","section":"posts","tags":["Notes"],"date":"2024.02.19","body":"A Brief Intro One evening i read an article about if bill gates could ask a time traveller, he\u0026rsquo;d want to know whether AI eventually doomed or helped humanity .\nThose question is quite interesting, no one really know if artificial intelligence (AI) will helped or doomed humanity and here\u0026rsquo;s my take.\nMain Point Rather than explaining about the history of AI that only a few people cares, let\u0026rsquo;s talk about what make people scared about AI.\nThere\u0026rsquo;s this topic about AI will take over humanity, or doomed humanity, or ending the humankind, etc. In my opinion, those kind of topic can become reality on one condition, when the human intelligence is lower than the artificial intelligence.\nNow, you might be thinking, \u0026ldquo;well, the current AI is smarter than me because i can\u0026rsquo;t solve those specific problem\u0026rdquo;, and guess what? The AI might not be able to solve the problem that you can solve. The AI will give a better answer if they have a better material for their training, and you know what? Human is not that different, the AI might be able to solve the problem you can\u0026rsquo;t solve because they have resources that you don\u0026rsquo;t have as their learning material (which can be questionable from the legal perspective).\nThe point is, AI also need training to increase their intelligence. Now, this is something that scared me personally. As the AI become more advance, human\u0026rsquo;s training or learning slowing down. I\u0026rsquo;ve seen a few people mindlessly take any output from the AI and present them as their answer or solution. It\u0026rsquo;s like they use AI as a reason to decrease or stop learning altogether.\nI understand that laziness can be the source of innovation, and one of those innovation is automation. Most of the time, human use automation to handle repetitive task. Will automation doomed humanity? Not really, because the most advance automation still need human supervision from time to time. Some people might lose their job because of automation, but at the same time, those kind of jobs, which do repetitive task should be left to the machine instead of human to increase human\u0026rsquo;s quality of life. If you want an example of a job that get replaced by machine, you can check the history of glass bottle, how people back in the day make glass bottle manually.\nClearly teaching a machine or a system to do some task can be beneficial for human. You could say those machine or system that human teach, is more like an assistant instead of a replacement. We, as a human, still need to supervise those machine or system, and to be able to do that, we can\u0026rsquo;t really stop our learning process so that we can stop the machine or system when they misbehave and potentially endanger human life.\nSo, in this post i want to remind people that if they are scared AI will doomed humanity, then they should keep learning and prevent that from happening themselves. Do you really think that human intelligence will be lost to artificial intelligence? We will see.\n"},{"ref":"https://bruhtus.github.io/posts/postgres-export-query-result-from-remote-to-local-csv/","title":"Postgresql Export Query Result From Remote to Local in CSV file","section":"posts","tags":["Notes","Postgresql"],"date":"2023.07.25","body":"A Brief Intro Have you ever thought of how to get postgresql query result from remote server to our local machine?\nIf so, this short post might be for you.\nHow? Before we start, please keep in mind that there are other methods to get postgresql query result from remote server to local machine, the method that i mentioned after this is the easier method for me.\nAlright, let\u0026rsquo;s get to it!\nBefore we start, you need to configure the ssh stuff like private key and so on. So i will assume you already set it up in your ssh config.\nIn this post we\u0026rsquo;ll use psql to get the query result and copy() function from postgresql.\nNow, let\u0026rsquo;s say we have this query:\nSELECTcount(*)asuser_countFROMusers;and we want to put those query result in user-count.csv.\nThe first thing we need to do is make a query file with the sql query above and copy() like this:\ncopy(SELECTcount(*)asuser_countFROMusers;)TOstdoutWITHcsvheaderFor simplicity, we will put those sql query in a file called query. After that, we can use this command:\nssh server-config \u0026#39;psql -d postgresql://user:password@localhost:5432/db-name\u0026#39; \u0026lt; query Please keep in mind that server-config is our ssh configuration in ~/.ssh/config, and postgresql://user:password@localhost:5432/db-name is our postgresql data source name (DSN).\nIf our query has result, we will see them in our terminal. Now to put those result in csv file, we can use redirection like this:\nssh server-config \u0026#39;psql -d postgresql://user:password@localhost:5432/db-name\u0026#39; \u0026lt; query \u0026gt; user-count.csv And, that\u0026rsquo;s it. Now we have our query result in csv file. Alright, see you next time!\nReferences  Stackoverflow ssh with psql . Stackoverflow psql using stdin .  "},{"ref":"https://bruhtus.github.io/posts/postgres-exist-clause/","title":"Postgres Exist Clause","section":"posts","tags":["Notes","Postgresql"],"date":"2023.07.20","body":"A Brief Intro Have you ever wondering, rather than show the data, can we show the true or false if the data exist or not?\nIf so, then this short post might be for you!\nHow? In this post we will take a look at exists postgresql function. Before we start, please keep in mind that exists use subquery, so there is a penalty of subquery here.\nThe exists will evaluate if the subquery return any rows. If it returns at least one row, the result of exists is true, if not, then the result of exists is false.\nWe can use that in SELECT statement like this:\nSELECTexists(SELECTusers.emailFROMusersWHEREusers.email=participants.email)FROMparticipants;Or we can use it in WHERE statement like this:\nSELECTparticipants.emailFROMparticipantsWHEREexists(SELECTusers.emailFROMusersWHERE(users.role=\u0026#39;sponsor\u0026#39;ANDusers.email=participants.email))This can be useful if we don\u0026rsquo;t want to join the table. Alright, that\u0026rsquo;s it for this post. See you next time!\nReferences  Postgresql subquery expressions documentation . Stackoverflow return false if no rows .  "},{"ref":"https://bruhtus.github.io/posts/postgres-count-data-on-different-condition/","title":"Postgresql Count Data on Different Condition in The Same Query","section":"posts","tags":["Notes","Postgresql"],"date":"2023.07.18","body":"A Brief Intro Have you ever wondering how to count all the data and specific data with a certain condition on one query? If so, then this short post might be for you.\nLet\u0026rsquo;s get started!\nHow? Let\u0026rsquo;s say we organize an event and we store the participants data on our postgresql database.\nNow we want to know all the participants that registered for the event and all the participants that actually attended the event. That means, even though someone register for the event, they might not attend the event.\nWith that in mind, we can use FILTER clause in our query like this:\nSELECTcount(*)ASregistered_participants,count(*)FILTER(WHEREparticipants.status=\u0026#39;attended\u0026#39;)ASattended_participantsFROMparticipants;Alright, that\u0026rsquo;s it for now. See you next time!\nReferences  Postgresql sql expressions documentation . Stackexchange count with different condition on the same query .  "},{"ref":"https://bruhtus.github.io/posts/postgres-get-current-database-size/","title":"Postgresql Get Current Database Size","section":"posts","tags":["Notes","Postgresql"],"date":"2023.07.18","body":"A Brief Intro Have you ever wondering the size of our current postgresql database? If so, this short post might be for you.\nLet\u0026rsquo;s get started!\nHow? In postgresql, there\u0026rsquo;s a function called pg_database_size() which computes the total disk space used by the database with specified database name.\nWe can use pg_database_size() like this:\nSELECTpg_database_size(\u0026#39;db-name\u0026#39;);The output of pg_database_size() is in bytes, so if we want to display the output in a human-readable format (kb, mb, and so on), we can use function pg_size_pretty() like this:\nSELECTpg_size_pretty(pg_database_size(\u0026#39;db-name\u0026#39;));If we want to check all available database, we can use pg_database system table like this:\nSELECTpg_database.datnameASdb_name,pg_size_pretty(pg_database_size(pg_database.datname))ASdb_sizeFROMpg_databaseORDERBYpg_database_size(pg_database.datname)DESC;Alright, that\u0026rsquo;s all for now. See you next time!\nReferences  Postgresql System Administration Functions Documentation . Stackoverflow how to get db names and size .  "},{"ref":"https://bruhtus.github.io/posts/systemd-reset-failed-status/","title":"Systemd Reset Failed Status","section":"posts","tags":["Notes"],"date":"2023.07.18","body":"A Brief Intro Have you ever experience a failed systemd service and already trying to restart using systemctl restart \u0026lt;some-service\u0026gt; but the failed status still there?\nIf so, this short post might be for you! Let\u0026rsquo;s get started.\nReset Failed Status Now, if our systemd service has a failed status, we can try using command:\nsystemctl reset-failed \u0026lt;some-service\u0026gt; to remove the failed status and reset it to recent status.\nPlease keep in mind that reset-failed is dependent on Restart option, so if you use option Restart=always, there\u0026rsquo;s a chance that the failed status still there because systemd still trying to restart our service which result in another failed status.\nThere are still a lot more i need to learn about systemd service, so for now that\u0026rsquo;s all. See you next time!\n"},{"ref":"https://bruhtus.github.io/posts/postgres-count-weekly-with-bigint-data-type/","title":"Postgresql Count Weekly With Bigint Data Type","section":"posts","tags":["Notes","Postgresql"],"date":"2023.06.26","body":"A Brief Intro Let\u0026rsquo;s say we have a created_at column in our database with data type big integer that has a value with this equation:\nEXTRACT(EPOCHFROMNOW())*1000 Don\u0026rsquo;t ask me why the value of created_at like that because i have no idea.\n Now, the goals is to count every data per week.\nHow? How to do that? Because we use a unix timestamp which look like this 1687746597339, we need to convert those into timestamp using postgresql to_timestamp() function like this:\nto_timestamp(created_at/1000) We divide by 1000 because we multiple it by 1000 when we insert the value.\n And then, we need to truncate into a specific date. We can use date_trunc() function to truncate the date.\nAccording to the documentation, this is the parameter for date_trunc():\ndate_trunc(field,source[,time_zone])Valid values for field are:\n microseconds milliseconds second minute hour day week month quarter year decade century millennium  source is either timestamp or interval and return timestamp or interval depending on the source.\ntime_zone is an optional parameter to specify a different timezone. By default, truncation is done with the current timezone setting.\nIf we combine date_trunc() and to_timestamp(), we will get something like this:\ndate_trunc(\u0026#39;week\u0026#39;,to_timestamp(created_at/1000))If we make the entire query, it would be something like this (let\u0026rsquo;s say the table name is participants):\nSELECTdate_trunc(\u0026#39;week\u0026#39;,to_timestamp(created_at/1000))ASweekly,COUNT(*)AStotal_participantsGROUPBYweeklyORDERBYweeklyASC;And the result would be something like this:\nweekly|total_participants------------------------+-------------------------- 2023-06-1900:00:00+07|694202023-06-2600:00:00+07|69 00:00:00+07 is because the timezone setting on my database is asia/jakarta which is basically UTC+7. If your timezone setting is different, the timestamp might be different.\nStill not sure why the date_trunc() with field week start at monday, and, at the time of writing this post, i haven\u0026rsquo;t found a settings to change that.\n Alright, that\u0026rsquo;s it. See you next time!\nReferences  Postgresql Date/Time Functions and Operator Documentations . Postgresql tutorial date_trunc() function . Stackoverflow group by week in postgresql .  "},{"ref":"https://bruhtus.github.io/posts/postgres-update-multiple-rows-in-one-query/","title":"Postgresql Update Multiple Rows with One Query","section":"posts","tags":["Notes","Postgresql"],"date":"2023.06.01","body":"A Brief Intro Have you ever wondering how to update multiple rows with one query? Let\u0026rsquo;s say you want to change value on table A where the name is anu on year 2019, 2020, and 2021.\nRather than doing 3 query with different WHERE statement, we can do that with one query. Without further ado, let\u0026rsquo;s go straight to it!\nFROM statement Postgresql has FROM clause in UPDATE statement which let us use columns on other table, it use the same syntax as FROM clause in SELECT statement.\nWe can combine FROM clause with VALUES expression. VALUES in postgresql let us create a constant table which means we can generate a table with constant values that can be used in a query without having to actually create and populate the table on disk.\nNow here\u0026rsquo;s an example case:\nLet\u0026rsquo;s say we need to update the price rows on area tokyo at table meat with year 2019, 2020, and 2021. We can make the query like this:\nUPDATEmeatSETprice=new.priceFROM(values(6942,2019),(69420,2020),(69690,2021),)ASnew(price,year)WHEREmeat.areaILIKE\u0026#39;tokyo\u0026#39;ANDmeat.year=new.yearRETURNING*;You can change the new in new(price, year) with anything you like.\nAnother example case:\nLet\u0026rsquo;s say we need to update the city\u0026rsquo;s population on specific regencies. So we need to update data on population column in city table which has reference to regencies id. We can make the query like this:\nUPDATEcitySETpopulation=new.populationFROMregencies,(values(\u0026#39;city 1\u0026#39;,69420),(\u0026#39;city 2\u0026#39;,6969))ASnew(regency,population)WHEREregencies.id=city.regency_idANDregencies.regencyilikenew.regencyRETURNINGregencies.regency,city.population; The RETURNING clause is optional, it\u0026rsquo;s a way to check if we really changed the right rows or not. The ILIKE clause is an expression for insensitive case.\n Alright, that\u0026rsquo;s it. See you next time!\nReferences  Stackoverflow example . Postgresql update documentation . Postgresql values documentation .  "},{"ref":"https://bruhtus.github.io/posts/postgres-delete-using/","title":"Postgresql's USING Clause on DELETE Statement","section":"posts","tags":["Notes","Postgresql"],"date":"2023.06.01","body":"A Brief Intro The USING clause on DELETE statement is basically let us join multiple table and delete only the data from those join.\nIf you ever use mysql DELETE JOIN with INNER JOIN, then postgresql USING is kind of similar to that except it can only delete from one table by default.\nHow? How to do that? Here\u0026rsquo;s an example:\nLet\u0026rsquo;s say we have table A and table B which is referenced to table C by id. If we use SELECT statement on table C with INNER JOIN on table A, it would look like this:\nSELECT*FROMCINNERJOINAONA.C_id=C.id;Now, if we want to only delete all records from table C related to table A without deleting all records from table C related to table B, we can use DELETE with USING clause like this:\nDELETEFROMCUSINGAWHEREA.C_id=C.id; Please keep in mind that if you haven\u0026rsquo;t set the ON DELETE CASCADE for table A on reference C_id, that query will throw an error.\n Alright, that\u0026rsquo;s it. See you next time!\nReferences  Postgresql delete documentation . Postgresql delete using tutorial . MySQL delete join tutorial .  "},{"ref":"https://bruhtus.github.io/posts/docker-compose-down-without-yaml-file/","title":"Docker Compose Down Without Yaml File","section":"posts","tags":["Notes","Linux"],"date":"2023.03.22","body":"A Brief Intro Have you ever feel that you have too much docker container from spin up a bunch of docker-compose.yaml file? And when you want to take down all those docker container from the docker-compose.yml, you already delete the file?\nIf that\u0026rsquo;s the case, then this post might be for you.\nBefore we start, we will be using docker compose version 2.16.0 in this post. If you are using the older version of docker compose, then this method might not work. Please keep that in mind.\nCheck Docker Compose Project Name The first thing we need to do is to check the project name. We can do that with command:\ndocker-compose ls or if we use docker compose v2, we can use this command instead:\ndocker compose ls If the container already stopped, we can use flags -a or --all like this:\ndocker-compose ls -a or this:\ndocker compose ls -a By default, docker compose will use the docker-compose.yml directory name as project name. We can specify the project name with flag -p or --project-name like this:\ndocker-compose -p anu up -d or this:\ndocker compose -p anu up -d Please keep in mind that we need to use the flags -p or --project-name before the command such as up or down.\nTake Down Docker Compose Component After we know the project name, we can take down all docker component when we spin up the docker container from docker-compose.yml. The component in here means volume, network, or anything that we define inside docker-compose.yml.\nTo take down component from docker compose without the docker-compose.yml, we can use this command:\ndocker-compose -p project-name down \u0026lt;docker-compose-down-flags\u0026gt; or this:\ndocker compose -p project-name down \u0026lt;docker-compose-down-flags\u0026gt; For example, let\u0026rsquo;s say we want to take down docker component from project anu and also remove the volume from those project. We can do that with this command:\ndocker-compose -p anu down -v or this:\ndocker compose -p anu down -v Alright, that\u0026rsquo;s it!\nI am not sure since when the ls command appear in docker compose, so just to be safe i recommend using at least docker compose version 2.16.0.\nSide Note I\u0026rsquo;ve tried this method with docker compose version 1.18.0, but failed. After i downloaded the docker compose binary version 2.16.0 from github, i can use this method.\nStill not sure what was wrong. When i use command docker-compose --help, the flag -p in there. But when i actually spin up a new docker instance with command:\ndocker-compose -p project-name up -d service-name and then tried to take it down with command:\ndocker-compose -p project-name down -v i got an error docker-compose.yml not found or along those lines.\nIf someone know what\u0026rsquo;s going on, please let me know!\n"},{"ref":"https://bruhtus.github.io/posts/simple-guide-qmk-flashing-cli/","title":"Simple Guide Flashing Mechanical Keyboard with QMK CLI","section":"posts","tags":["Notes","Linux"],"date":"2023.02.26","body":"A Brief Intro So recently, around the time i wrote this post, i need to flash my mechanical keyboard. The harsh thing is that, the GUI QMK toolbox only available on windows and macOS, meanwhile i use linux.\nSo, i need to use the QMK CLI to flash my mechanical keyboard. This post is some kind of notes for my future self.\n\u0026ldquo;Humans live by forgetting\u0026rdquo;, some people said. Without further ado, let\u0026rsquo;s get started!\nInstall QMK Firmware First thing first, we need to check if QMK firmware already installed on our system. We can do that with the command below:\ncommand -v qmk If the output is empty, then we need to install QMK firmware first. If the output is not empty, we can skip this process.\nThere\u0026rsquo;s a newbie guide on how to install QMK firmware on linux, we can them out here .\nThere are a few options to install QMK firmware on linux. We can install QMK firmware with our linux distro package manager or from python pip.\nIn this post, we will use python pipx to install QMK firmware. We can install QMK firmware from python pipx with this command:\npipx install qmk  Make sure you already have pipx installed on your machine.\n Follow the instruction from the installation process, including installing dependencies on your system and copy the 50-qmk.rules to /etc/udev/rules.d/.\n Copying the 50-qmk.rules make QMK can detect the bootloader of our mechanical keyboard. If we didn\u0026rsquo;t do that, even if our mechanical keyboard already in bootloader mode, QMK won\u0026rsquo;t be able to detect our mechanical keyboard.\n After we install QMK firmware, we need to setup the QMK CLI. We can do that with this command:\nqmk setup By default, the installation directory will be on our home directory. Personally i don\u0026rsquo;t really want to put the QMK installation on my home directory, so i added -H flag to move the installation directory to somewhere else, like this:\nqmk setup -H all-repos/qmk_firmware If the setup already finished, try running:\nqmk doctor and see if there\u0026rsquo;s any issue with the installation.\nFlashing with QMK CLI There\u0026rsquo;s a newbie guide on how to flash our mechanical keyboard using QMK CLI, we can find it here .\nThe point is, we need to know if our mechanical keyboard supported by QMK. We can check the supported mechanical keyboard here .\nLet\u0026rsquo;s say our mechanical keyboard is dz60rgb with ansi layout version 1, we can flash our mechanical keyboard with this command:\nqmk flash -kb dztech/dz60rgb_ansi/v1 -km default  For more info about the flags, we can use qmk flash --help.\n Unfortunately, my current mechanical keyboard is not supported, which is synthesis60 version 2. I need to get the hex file from the mechanical keyboard designer (and fortunately he\u0026rsquo;s within reach).\n In case you also need the synthesis60 version 2 hex file, you can check it here .\n So, to use our own hex file rather than using the default that QMK supported, we can use this command:\nqmk flash \u0026lt;path-to-hex-file\u0026gt; Here\u0026rsquo;s an example:\nqmk flash ~/downloads/dyz_synthesis60_atmega_vial.hex All right, that\u0026rsquo;s it from me. See you next time!\nReferences  QMK newbie getting started guide . QMK newbie flashing guide . Using QMK flash with the external file .  "},{"ref":"https://bruhtus.github.io/posts/remove-specific-line-in-vim/","title":"Remove Specific Line in Vim","section":"posts","tags":["Notes","Vim"],"date":"2022.09.03","body":"A Brief Intro In this post, we will talk about how to remove specific line in vim. We will be using this javascript snippet code as an example:\nconst app = require(\u0026#39;./jest-example\u0026#39;); const math = require(\u0026#39;./math\u0026#39;); describe(\u0026#39;app operation\u0026#39;, () =\u0026gt; { const multiplyMock = jest.spyOn(math, \u0026#39;multiply\u0026#39;); multiplyMock.mockReturnValue(\u0026#39;itu\u0026#39;); test(\u0026#39;call math.add()\u0026#39;, () =\u0026gt; { const add = jest.spyOn(math, \u0026#39;add\u0026#39;); add.mockImplementation(() =\u0026gt; \u0026#39;anu\u0026#39;); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(\u0026#39;anu\u0026#39;); add.mockRestore(); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(3); }); test(\u0026#39;call math.subtract()\u0026#39;, () =\u0026gt; { const subtract = jest.spyOn(math, \u0026#39;subtract\u0026#39;); subtract.mockImplementation(() =\u0026gt; \u0026#39;nganu\u0026#39;); console.log(app.doSubtract(1, 2)); expect(app.doSubtract(1, 2)).toBe(\u0026#39;nganu\u0026#39;); }); test(\u0026#39;call math.subtract() again\u0026#39;, () =\u0026gt; { console.log(app.doSubtract(1, 2)); }); test(\u0026#39;call math.multiply()\u0026#39;, () =\u0026gt; { console.log(app.doMultiply(1, 2)); }); }); And the goal is to remove line that has console.log() in it. Let\u0026rsquo;s get started!\nGlobal Command The first solution is that, we can use global command to delete the line that has console.log() like this:\n:g/console.log/dPlease keep in mind that the term delete in vim, means cut. So, rather than deleting the content, vim will put that into the unnamed register. For more info about unnamed register, we can check on :help registers.\nSo, if we didn\u0026rsquo;t want to clutter our unnamed register, we can use black hole register (yup, that\u0026rsquo;s a thing) which actually delete rather than cut like this:\n:g/console.log/d_On a side note, we don\u0026rsquo;t need to use / as the separator between command and pattern, we can use most characters except \\, \u0026quot;, or |. If we use vim9 script, we can use # as a separator either, so please keep that in mind. For more info, we can check on :help pattern-delimiter.\nIn other words, we can write the previous global command, like this:\n:g;console.log;d_The global command that we just gone through will delete all the console.log in current file. If we want to specify the area of console.log we want to delete, we can add range to global command.\nLet\u0026rsquo;s take a look at the snippet we are using:\nconst app = require(\u0026#39;./jest-example\u0026#39;); const math = require(\u0026#39;./math\u0026#39;); describe(\u0026#39;app operation\u0026#39;, () =\u0026gt; { const multiplyMock = jest.spyOn(math, \u0026#39;multiply\u0026#39;); multiplyMock.mockReturnValue(\u0026#39;itu\u0026#39;); test(\u0026#39;call math.add()\u0026#39;, () =\u0026gt; { const add = jest.spyOn(math, \u0026#39;add\u0026#39;); add.mockImplementation(() =\u0026gt; \u0026#39;anu\u0026#39;); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(\u0026#39;anu\u0026#39;); add.mockRestore(); console.log(app.doAdd(1, 2)); expect(app.doAdd(1, 2)).toBe(3); }); test(\u0026#39;call math.subtract()\u0026#39;, () =\u0026gt; { const subtract = jest.spyOn(math, \u0026#39;subtract\u0026#39;); subtract.mockImplementation(() =\u0026gt; \u0026#39;nganu\u0026#39;); console.log(app.doSubtract(1, 2)); expect(app.doSubtract(1, 2)).toBe(\u0026#39;nganu\u0026#39;); }); test(\u0026#39;call math.subtract() again\u0026#39;, () =\u0026gt; { console.log(app.doSubtract(1, 2)); }); test(\u0026#39;call math.multiply()\u0026#39;, () =\u0026gt; { console.log(app.doMultiply(1, 2)); }); }); And let\u0026rsquo;s say that our cursor in the line test('call math.subtract()', () =\u0026gt; { and we want to delete the console.log in test('call math.add()', () =\u0026gt; { which is before the line we\u0026rsquo;re currently in.\nWith that in mind, we can do something like this:\n:?test?;/});/g;console.log;d_Here\u0026rsquo;s the breakdown of those command:\n ?test? will search word test on previous line. ; after ?test? is a special offset which tell vim that we will use another search command. For more info, we can check :help //;. /});/ will search after the result of previous search. g;console.log;d_ is a global command to delete console.log in the scope of previous search.  Substitute Command The downside of global command is that there\u0026rsquo;s no confirm option, so we might delete something we don\u0026rsquo;t want to. If we want a confirm option, we can use substitute command.\nIf we want to delete all the console.log in current file, we can do it like this:\n:%s;.*console.log.*\\n;;cHere\u0026rsquo;s the breakdown of those command:\n % symbol to tell vim that the range is the current file or all the line in the file. s is the abbreviation of substitute command. .*console.log.*\\n is the pattern that select the line that has console.log in it, which include the newline character at the end. ;; is basically telling vim to replace it with nothing. c is a confirm option.  We can also combine substitute command with global command, like this:\n:g;console.log;s;.*\\n;;cConclusion There\u0026rsquo;s a lot of possibility with global and substitute command and we won\u0026rsquo;t be able to cover all of those in one blog post. So, i will leave it to you to explore those possibility.\nYou might want to learn regular expression to enhance the global and substitute command experience.\nAlright, that\u0026rsquo;s it for this post. Thank you for reading!\n"},{"ref":"https://bruhtus.github.io/posts/clean-up-untracked-file-in-git-repo/","title":"Clean Up Untracked File in Git Repo","section":"posts","tags":["Notes","Linux","Shell","Git"],"date":"2022.08.20","body":"Have you ever feels like deleting all the untracked file or directory in git repository is such a pain?\nIf you do, then this post might be for you!\nOk, first thing first, there is a git command clean which help us delete the untracked file. We can even use it with interactive interface. To invoke the interactive interface, we can use this command:\ngit clean -i . What that command does is prompt us an interactive interface with all the untracked file in current directory.\nTo make it start from the root of git repository, we can use this command:\ngit clean -i $(git rev-parse --show-toplevel) The git rev-parse --show-toplevel will get us the path of current git repository root. If you don\u0026rsquo;t know what i mean by git repository root, we can think of it as the directory or path that has .git directory.\nBut, the clean command has a downside. The clean command does not let us select specific file under untracked directory. It will only let us delete the entire untracked directory with all the file in it.\nTo get around this issue, we can make a shell alias like this:\nalias gurm=\u0026#39;git ls-files --others --exclude-standard | fzf --multi | xargs -r rm -v\u0026#39;  You can change gurm to anything you want.\n The dependencies from shell alias above are:\n git fzf xargs  git ls-files --others --exclude-standard will list all the untracked file and also exclude the file from .gitignore.\nfzf is a fuzzy finder to select the untracked file we want to remove, we also give flags --multi so that we can select multiple file with tab key.\nFinally, we execute rm command with xargs -r. What xargs -r do is make sure that we have some input from standard input (stdin), if there is no input from stdin, xargs will not execute the command. It is useful to prevent an error from a command that require an argument.\nAlright, that\u0026rsquo;s all. Have a nice day!\n"},{"ref":"https://bruhtus.github.io/posts/awk-print-row-instead-of-column/","title":"Awk Print Row Instead of Column","section":"posts","tags":["Notes","Linux","Shell"],"date":"2022.06.18","body":"Main Course Let\u0026rsquo;s say we have an output from xrandr --listactivemonitor like this:\nMonitors: 2 0: +*eDP-1 1920/344x1080/194+1920+0 eDP-1 1: +HDMI-2 1920/480x1080/270+0+0 HDMI-2 Now, we want to display only the second row which is this line:\n0: +*eDP-1 1920/344x1080/194+1920+0 eDP-1 We can do that using awk with this command:\nxrandr --listactivemonitor | awk \u0026#39;NR==2\u0026#39; or using process substitution command, like this:\nawk \u0026#39;NR==2\u0026#39; \u0026lt;(xrandr --listactivemonitor) If we also want to limit the row and the column, let\u0026rsquo;s say the second row and the third column which is result in this:\n1920/344x1080/194+1920+0 We can do that with this command:\nxrandr --listactivemonitor | awk \u0026#39;NR==2 {print $3}\u0026#39; or using process substitution command, like this:\nawk \u0026#39;NR==2 {print $3}\u0026#39; \u0026lt;(xrandr --listactivemonitor) Now, if we want to display the second to last row, which is this line:\n0: +*eDP-1 1920/344x1080/194+1920+0 eDP-1 1: +HDMI-2 1920/480x1080/270+0+0 HDMI-2 We can do that with this command:\nxrandr --listactivemonitor | awk \u0026#39;NR\u0026gt;=2\u0026#39; or using process substitution command, like this:\nawk \u0026#39;NR\u0026gt;=2\u0026#39; \u0026lt;(xrandr --listactivemonitor) Alright, that\u0026rsquo;s all. Thanks for reading and happy shell scripting!\nReference  Stackoverflow answer about awk for specific row .  "},{"ref":"https://bruhtus.github.io/posts/shell-process-substitution-as-temp-file/","title":"Shell Process Substitution as Temp File","section":"posts","tags":["Notes","Linux","Shell"],"date":"2022.06.11","body":"A Brief Intro This is all started when i\u0026rsquo;m trying to make a shell alias using fzf and git add -p command. And i was surprised that the interactive selection from git add -p immediately terminated before i can even press any key. Let\u0026rsquo;s get started, shall we?\nFirst Attempt My first attempt to make that shell alias was something like this:\nalias gap=\u0026#39;git status -s | awk \u0026#34;{print \\$2}\u0026#34; | fzf | xargs -r git add -p\u0026#39; Let me briefly explain one by one the command that i use in my shell alias:\n  git status -s -\u0026gt; The short format of git status, without unnecessary info (at least for me).\n  awk \u0026quot;{print \\$2}\u0026quot; -\u0026gt; Only use the second column. The backslash is to prevent shell to expand $2 into a second argument instead of a second column in awk.\n  fzf -\u0026gt; Fuzzy finder by Junegunn1.\n  xargs -r -\u0026gt; Do not run the command if there\u0026rsquo;s no standard input.\n  git add -p -\u0026gt; To add a chunk of changes, only works if the file already tracked by git.\n  This alias is where the problem arise. After i select the file name using fzf, the interactive interface of git add -p only appear for a few seconds and then terminated.\nAccording to stackoverflow answer2:\n Without further arguments xargs does not work with interactive (command line) applications.\nThe reason for that is, by default xargs gets its input from stdin but interactive applications also expect input from stdin.\nTo prevent the applications from grabbing input that is intended for xargs, xargs redirects stdin from /dev/null for the applications it runs.\nThis leads to the application just receiving an EOF3.\n With that in mind, we need to use --arg-file=\u0026lt;file\u0026gt; or -a \u0026lt;file\u0026gt; flag, which means that xargs will read from a \u0026lt;file\u0026gt; instead of stdin ( standard input) so that the stdin remains unchanged. Alright, let\u0026rsquo;s go to the second attempt.\nSecond Attempt Following the stackoverflow answer2, my second attempt was something like this:\nalias gap=\u0026#39;xargs -a \u0026lt;(git status -s | awk \u0026#34;{print \\$2}\u0026#34; | fzf) git add -p\u0026#39; The expression \u0026lt;(git status -s | awk \u0026quot;{print \\$2}\u0026quot; | fzf) is what we called process substitution (or to be precise, shell process substitution).\nInstead of using a file, we use a command to act like a temporary file.\nNow, the problem is, in my experiment i can\u0026rsquo;t use fzf with the shell process substitution because shell process substitution produces a special file that can only be opened and read, but not written or seeked4.\nCommands that treat their arguments as pure streams will works with shell process substitution, but the commands that seek a file they are given (or write to a file) won\u0026rsquo;t work4.\nAnd that is why we can\u0026rsquo;t use interactive command such as fzf with shell process substitution.\nConclusion With all the information from my experiment before, i decided to remove fzf from the alias, which result with this alias:\nalias gap=\u0026#39;xargs -a \u0026lt;(git status -s | awk \u0026#34;{print \\$2}\u0026#34;) git add -p\u0026#39; Is it fulfilling what i need? Not really, i might replace that with shell function instead of shell alias soon. But, i learn something new about shell process substitution.\nExtra Note For more info about the xargs and git flags, check the manpage. For example: man xargs or man git-add.\nIf we use echo with shell process substitution like this:\necho \u0026lt;(git status -s) That will show us where the temporary file created.\n  Fuzzy finder by Junegunn .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Stackoverflow: xargs explanation .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Triggering EOF explanation .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Process substitution result in special file .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"ref":"https://bruhtus.github.io/posts/replacement-for-info-command/","title":"Replacement For Info Command","section":"posts","tags":["Notes","Linux","Shell"],"date":"2022.06.11","body":"A Brief Intro Sometime ago, someone posted a tweet about a replacement for rm -rf $HOME/directory that is mv $HOME/directory /dev/null. When i took a look of it, using the ls -l /dev/null command, i found something like this:\ncrw-rw-rw- 1 root root 1, 3 Jun 11 04:21 /dev/null Now, what is c in the crw below\ncrw-rw-rw- 1 root root 1, 3 Jun 11 04:21 /dev/null After going around on internet, i found an answer in stackexchange1. Which explain that c stands for character special file. Please keep in mind that everything in unix-like system is a file, even a directory (or some people called it folder) is a file with type directory.\n The file we usually use has a type of regular file.\n In the stackexchange, someone mentioned about info ls to show the file type with their respective character symbol. That\u0026rsquo;s what pique my interest, the info command.\nReplacement For info Command When i use the command info ls, i noticed that i can\u0026rsquo;t use j and k for navigation, which is a nightmare.\nSo, the first thing that i need to figure out is, how to change the pager used in info command. According to this stackexchange answer2, \u0026ldquo;info doesn\u0026rsquo;t use separate pager because it handles navigation\u0026rdquo;. So, basically there\u0026rsquo;s no hope with info command? Probably.\nAnd then, the person who answer on the stackexchange also give a suggestion about pinfo which, at least use j and k as down or up movement. Now i need to read the manpage about pinfo to configure it.\nWhat\u0026rsquo;s Next? Other than trying to configure pinfo, i might need to figure out what is the info documents is all about. This might be a new kind of documentation other than manpage that i can use (if the developer support it). Also, figure out about the character special file type.\n  Stackexchange: The meaning of c in crw-rw-rw- \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Stackexchange: Replacement for info command \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "}]