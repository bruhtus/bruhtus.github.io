[{"ref":"https://bruhtus.github.io/posts/pyv/","title":"Pyv: Minimalist Python Venv Management Tool","section":"posts","tags":["No one asked","Shell","Python"],"date":"2021.04.16","body":" This is a continuation from my previous post (you can check it here ). Long story short, this is a minimalist way to manage your python virtual environment. All you need is python and git.\n What Is Pyv? Here\u0026rsquo;s a brief intro to what is pyv:\n Pyv is a simple shell function that let you manage python virtual environment that decoupled from project directory.\n To put it simply, pyv move the virtual environment to $PYV_dir which by default is $HOME/.cache/pyv\nHow Pyv Manage Virtual Environment Pyv can create, remove, activate, deactivate, and list python virtual environment. Below is the explanation for each action that pyv can do.\nCreate Virtual Environment Pyv create python virtual environment using python default command python -m venv to $PYV_DIR/{given-name}. The given-name can be from git repo name or user input.\nIf the user give pyv command an argument then pyv gonna create python virtual environment with that argument name. For example:\n1pce something-big With the command above, pyv gonna create python virtual environment with the name something-big. pce is pyv command to to create python virtual environment (why would i made something with long command?).\nPyv also can create python virtual environment with git repo name. If you give no argument and just enter pce in a working git repo then pyv gonna create python virtual environment using the git repo name.\n\u0026ldquo;How does pyv do that?\u0026rdquo; you might ask, well, pyv use the command git rev-parse --show-toplevel to get the git root directory name (with a lot of trimming of course). For those who don\u0026rsquo;t know what git root directory, to make simple, git root directory is the directory where you first use git init command or the directory that have .git directory.\nPlease do remember tho, you can only use pce without any argument in git working directory and not in git bare directory. What i mean by git working directory is the normal git repo that has .git directory in it.\nHere\u0026rsquo;s an example of pce without an argument (in case you\u0026rsquo;re still confused):\n1pce Yup, only that. Simple right?\nRemove Virtual Environment After creating virtual environment, how do you delete the virtual environment?\nYou can remove the virtual environment using pre command with or without argument similar to creating virtual environment.\nFor example, we already create something-big python virtual environment. And now we want to remove those virtual environment. All we need to do is something like this:\n1pre something-big Those command gonna invoke the rm command to remove something-big virtual environment directory in $PYV_DIR. So the pre command depends on how you setup your rm command in your shell.\nIf you create virtual environment using the git repo name, and you want to remove those environment, you can use pre without an argument inside those git repo directory (assumming you haven\u0026rsquo;t changed the directory name). How to use the command is the same as when you create the virtual environment, just type\n1pre and you\u0026rsquo;re done.\nActivate Virtual Environment Ok, now you know how to create and remove the virtual environment using pyv. Now, how do you activate those virtual environment?\nBecause we use default python python -m venv command, we need to know how the activate the virtual environment created using those command.\nAccording to python venv documentation , we need to source the activate file in \u0026lt;venv-dir\u0026gt;/bin/activate.\n Did you know that the activate file is also a shell function? Now you know.\n So, the pyv command to activate the virtual environment is pae. Similar to previous command, you can use pae with or without an argument. pae command basically to source activate file in virtual environment directory that located in $PYV_DIR.\nLike previous command, let\u0026rsquo;s say we have something-big virtual env and we want to activate those command. All we need to do is\n1pae something-big or if you create a virtual environment using git repo name (assuming you didn\u0026rsquo;t change the git repo directory name), then you can just use\n1pae without any argument.\nDeactivate Virtual Environment After you know how to create, remove, and activate virtual environment with pyv. Now it\u0026rsquo;s time for you to know how to deactivate virtual environment.\nYou can deactivate the virtual environment either using default deactivate command or using pde. pde is just an alias for deactivate command provided by python venv. You do you.\nPlease keep in mind that you don\u0026rsquo;t need any argument to deactivate virtual environment.\nList Virtual Environment The last thing is how to list all the virtual environment that available?\nTo list all the virtual environment that ever created, you can use pve command without an argument or ls -l $PYV_DIR. pve is just an alias for ls -l $PYV_DIR.\nConclusions There\u0026rsquo;s always an upside/downside to a project, and this project is no exception.\nThe upside is that, it\u0026rsquo;s minimal. If you didn\u0026rsquo;t do anything fancy with your python virtual environment then pyv probably gonna fit your need.\nThe downside is also \u0026ldquo;it\u0026rsquo;s minimal\u0026rdquo;. If you use default options that come with default python venv command, then you can\u0026rsquo;t do that with pyv. Pyv only able to handle python -m venv \u0026lt;env-directory\u0026gt;. That\u0026rsquo;s all.\n At the time of writing this article, i\u0026rsquo;ve only tested pyv in bash and zsh. I might provide support for fish, csh/tcsh in the future.\n References  A few alternative to manage python virtual environment . Python venv documentation .  "},{"ref":"https://bruhtus.github.io/posts/python-venv/","title":"Managing Python Virtual Environment","section":"posts","tags":["No one asked","Python"],"date":"2021.03.02","body":" Have you ever want python virtual environment that decoupled from the project directory like conda but not actually conda (dejavu)? That\u0026rsquo;s what this post is about, a simple way to manage a python virtual environment similar to how conda manage virtual environment without python version management.\n A Brief Intro to Python Virtual Environment To makes thing simpler, python virtual environment is a self-contained directory tree that contains a python installation for a particular version of python plus a number of additional packages.\nWith virtual environment you can minimize the conflicting requirements for each python script you made. For example, application A can have its own virtual environment with python package at version 1.0 installed and application B can have its own virtual environment with python package at version 2.0. If applicatin B need to upgrade the python package to version 3.0 then this will not affect application A\u0026rsquo;s environment with python package at version 1.0.\nLet\u0026rsquo;s Get Started In this post we\u0026rsquo;re only going to use the default python venv to create a virtual environment, the command is something like this:\n1python -m venv \u0026lt;directory-name\u0026gt; In case you\u0026rsquo;re wondering, \u0026ldquo;if we only gonna use the default python command, then what\u0026rsquo;s so special about it?\u0026rdquo;. Let me tell you this, what special about the default python venv command is that we can specify the path of the directory and we can enhance that with a shell script. Please keep in mind that this is a minimalist approach to manage python virtual environment without installing other tools except python and git (we\u0026rsquo;ll get to that later).\nCreate Python Virtual Environment Here\u0026rsquo;s how we make a simple shell script to manage our python environment. First thing first, you should decide where you want all your virtual environment located. If you use miniconda, usually it\u0026rsquo;s in miniconda3/envs/ directory or something along those path, i forgot. I personally want to place my python virtual environment in .cache/python-venv directory because i rarely check my .cache directory and the virtual environment not gonna disturb other shell scripts i have.\nAfter that, we decide whether we give the name to virtual environment ourself or just use git repo root name. Like i told you before, we can use git repo to decide the name of our virtual environment. To make things simpler, what i mean git repo root is the directory where you use command git init to initialize git repo.\nFor example, if you use git init command in nganu directory then you can have your virtual environment named nganu without you enter any name, but if you didn\u0026rsquo;t want to use your git repo root name then you can also insert the name you want, similar to conda create -n \u0026lt;name-env\u0026gt;. With that brief intro, here\u0026rsquo;s the code:\n1#!/bin/sh 2 3gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; 4trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; 5projectdir=\u0026#34;${trim##*/}\u0026#34; 6venvdir=$HOME/.cache/python-venv 7 8mkdir -pv $venvdir 9 10if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then 11\tpython -m venv $venvdir/$1 2\u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Created $1python venv\u0026#34; 12 13elif [ \u0026#34;$projectdir\u0026#34; != \u0026#34;\u0026#34; ]; then 14\tpython -m venv $venvdir/$projectdir 2\u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Created $projectdirpython venv\u0026#34; 15 16else 17\techo \u0026#34;Not git repo, please insert a name for virtual env (for example: pce nganu)\u0026#34; 18 19fi We can get git repo root path using the command git rev-parse --show-toplevel but the problem is, those command give the full path to git repo but what i want is only the name of the git repo root so we need to trim the full path and only give the directory name we want. That\u0026rsquo;s what trim and projectdir in those code did. So the name of the git repo dir is in projectdir variable.\nAfter that, venvdir is for the path you want to save all your virtual environment and the command mkdir -pv $venvdir is to make sure if the directory doesn\u0026rsquo;t exist then it\u0026rsquo;s gonna create the directory.\nThe \u0026quot;$1\u0026quot; != \u0026quot;\u0026quot; to make sure if no argument is given then don\u0026rsquo;t create any virtual environment and skip to next statement, but if there\u0026rsquo;s an argument then make the virtual environment with the argument as the name.\nThe \u0026quot;$projectdir\u0026quot; != \u0026quot;\u0026quot; to make sure if it\u0026rsquo;s not a git repo then don\u0026rsquo;t create any virtual environment and skip to next statement, but if it\u0026rsquo;s a git repo then make the virtual environment with the git repo root as the name.\nLet\u0026rsquo;s say we save those script with the name pce (python create env), you can choose different name, you do you. And don\u0026rsquo;t forget to place those script into your PATH so you can use that without typing the full path. With that in mind, here\u0026rsquo;s a few scenario we can do:\n Without any argument, if it\u0026rsquo;s git repo then it\u0026rsquo;s create virtual env with git repo root name and if it\u0026rsquo;s not git repo then it\u0026rsquo;s not gonna create any virtual env.  1pce With an argument, it\u0026rsquo;s gonna create virtual env with the argument as virtual env name.  1pce nganu #create nganu virtual env Remove Python Virtual Environment After creating virtual environment, we can also delete virtual environment using a shell script. The concept is similar to create virtual environment, the difference is if there\u0026rsquo;s no virtual environment with the same name as user input or git repo root, then the script not gonna delete anything. With that in mind, here\u0026rsquo;s the code:\n1#!/bin/sh 2 3gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; 4trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; 5projectdir=\u0026#34;${trim##*/}\u0026#34; 6venvdir=$HOME/.cache/python-venv 7 8if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then 9\trm -r $venvdir/$1 2\u0026gt; /dev/null || \\ 10\techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; 11 12elif [ \u0026#34;$projectdir\u0026#34; != \u0026#34;\u0026#34; ]; then 13\trm -r $venvdir/$projectdir 2\u0026gt; /dev/null || \\ 14\techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; 15 16else 17\techo \u0026#34;No python virtual environment match, nothing deleted\u0026#34; 18 19fi Activate and Deactivate Python Virtual Environment For activating the virtual environment, i use shell function and then source that function with your shell config, like .bashrc or .zshrc or something else.\nI did that because i can\u0026rsquo;t really source inside a shell script (it\u0026rsquo;s just gonna create a subprocess and didn\u0026rsquo;t effect the current shell, so you can\u0026rsquo;t activate the virtual environment).\nThe concept is similar to create and remove script above, if the virtual environment name not found then give message \u0026ldquo;Python virtual environment not found, create new one\u0026rdquo;, the difference is that we\u0026rsquo;re sourcing the virtual environment to activate it. With that in mind, here\u0026rsquo;s the code:\n1function pae(){ 2\tlocal gitroot=\u0026#34;$(git rev-parse --show-toplevel 2\u0026gt; /dev/null)\u0026#34; 3\tlocal trim=\u0026#34;${gitroot%\u0026#34;${gitroot##*[!/]}\u0026#34;}\u0026#34; 4\tlocal projectdir=\u0026#34;${trim##*/}\u0026#34; 5\tlocal venvdir=$HOME/.cache/python-venv 6 7\tif [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ]; then 8\t. $venvdir/$1/bin/activate 2\u0026gt; /dev/null || echo \u0026#34;Python virtual environment not found, create new one\u0026#34; 9 10\telse 11\t. $venvdir/$projectdir/bin/activate 2\u0026gt; /dev/null || echo \u0026#34;Python virtual environment not found, create new one\u0026#34; 12 13\tfi 14} To deactivate the virtual environment, you just need to use deactivate command. And to list all the virtual environment you made, you can use ls command. For example, ls ~/.cache/python-venv (or the path you choose to hold all your virtual environment).\nThe Conclusion What i mean \u0026ldquo;simple\u0026rdquo; earlier is not \u0026ldquo;easier to setup\u0026rdquo;, what i mean is you only need the tools that you probably already have and make a workflow with only that tools to be similar to how conda handles virtual environment.\nThis post is only an alternative way to manage python virtual environment because most python virtual environment tools usually make the virtual environment inside the project directory and i don\u0026rsquo;t really like that, i prefer how conda manage virtual environment but i think conda is overkill to only manage virtual environment and that\u0026rsquo;s why i make those shell script and function to behave the same as conda (without python version control yet).\nI may make this a standalone project if i ever know how to make the installer. And also please keep in mind that this was based on my needs so maybe there\u0026rsquo;s some glitch that i haven\u0026rsquo;t found yet. Ok that\u0026rsquo;s all, hopefully you gain something from this (probably not, like always).\nReferences  A few python virtual environment tools . Python virtual environments documentatton .  "},{"ref":"https://bruhtus.github.io/posts/python-vs-shell/","title":"Python Script vs. Shell Script: Command Line Use Case","section":"posts","tags":["No one asked","Linux","Python","Shell"],"date":"2021.01.14","body":" In this post i\u0026rsquo;m comparing performance python script and shell script that i\u0026rsquo;ve made. The script objective is to check git status on every git repo directory.\n Skip-able Part  Just a background story why i made this post\n Have you wondered what performance between python script and shell script (or some people might say bash script) to run command on terminal? no? same, just kidding. I\u0026rsquo;m curious about shell script because i can do command line stuff with python script too which for my past self is easier to read.\nFor around a month, i\u0026rsquo;ve learned shell scripting from youtube (mostly luke smith videos ) and i gotta say that for command line use case, shell script is more efficient than python script. Please keep in mind that i\u0026rsquo;m not an expert when it comes to python scripting or shell scripting, so there might be some performance enhancement that you can make to my script.\nA Brief Intro to Shell Script You might be wondering \u0026ldquo;what is shell script?\u0026rdquo; well, to put it simpler, it\u0026rsquo;s a script that you can run with shell. Shell, like python, is a programming language and that\u0026rsquo;s why you can run for loop in a terminal like a psychopath, just kidding i did that sometimes (maybe i\u0026rsquo;m a pyschopath? 👀).\nThere are quite a few ways to run shell script, here\u0026rsquo;s some that i know:\n  Make script executable by doing chmod +x \u0026lt;script-name\u0026gt; and then run the script by doing ./\u0026lt;script-name\u0026gt; if you\u0026rsquo;re already in the same directory as the script or chmod +x \u0026lt;path-to-script\u0026gt;/\u0026lt;script-name\u0026gt; and then run the script by doing \u0026lt;path-to-script\u0026gt;/\u0026lt;script-name\u0026gt; if you\u0026rsquo;re not in the same directory as the script. For example: If i want to run my git-status-checker script, then what i would do is type chmod +x git-status-checker if i already in the same directory as git-status-checker script and then run the script by typing ./git-status-checker. But, if i\u0026rsquo;m not in the same directory as git-status-checker, let\u0026rsquo;s say i\u0026rsquo;m in Download directory meanwhile the script is in Script directory, then what i would do is type chmod +x ~/script/git-status-checker and then run the script by typing ~/script/git-status-checker.\n  Using sh or bash command. For example: If i want to run my git-status-checker script, then i run the script by typing sh git-status-checker or bash git-status-checker, both command basically do the same thing.\n  The Test In this post, i\u0026rsquo;m gonna test the runtime both python script and shell script that i\u0026rsquo;ve made. The script objective is to check git status in each git repo directory on my machine.\nThe Environment of The Test All my git repo directory is located in all_git directory on home directory, so i only need to focus on that directory which is gonna make the task of the script a little bit easier and faster than check all other directory too.\nThe amount of not staged for commit is the same when running both script, which is around 13 in total. So the performance may decrease as the amount of unstaged increase.\nI\u0026rsquo;m gonna run the in two way, the first test is including git status for git submodules and the second test is excluding git status for git submodules.\nThe Results First Test In the first test, the runtime of python script is around 0.08 - 0.12 seconds and the runtime of shell script is around 0.07 - 0.08 seconds. Here\u0026rsquo;s the example of runtime of both script (which at this time coincidentally the same):\nSecond Test In the second test, the runtime of python script is around 0.05 - 0.07 seconds and the runtime of shell script is around 0.01 - 0.02 seconds. Here\u0026rsquo;s the example of runtime of both script:\nThe Explanation The effect of checking git status in git submodules is quite a lot but still less than a second which gonna make people think \u0026ldquo;it\u0026rsquo;s not that much of a difference\u0026rdquo;, well, in case you forgot that this script objective is just to check git status on each git repo directory, it\u0026rsquo;s just a simple task. If you\u0026rsquo;re going to make a more complicated task with a lot of command, that\u0026rsquo;s where you\u0026rsquo;re gonna see the gap.\nI\u0026rsquo;m gonna explain the first test source code first, below is the python script for the first test:\n1import os 2 3home = \u0026#39;/home/bruhtus/\u0026#39; 4dir_list = [dirname for dirname in os.listdir(f\u0026#39;{home}/all_git\u0026#39;) if os.path.isdir(f\u0026#39;{home}/all_git/{dirname}\u0026#39;) == True] 5 6for dirname in dir_list: 7 path = f\u0026#39;{home}/all_git/{dirname}\u0026#39; 8 os.system(f\u0026#39;echo {path}\u0026#39;) 9 10 if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: 11 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) 12 os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) 13 else: 14 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) And below is the shell script for the first test:\n1#!/bin/sh 2 3for d in $(ls -d ~/all_git/*/); do 4 echo $d \u0026amp;\u0026amp; git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s 5done Let\u0026rsquo;s take step by step of the processes. The first process is to list all directory in all_git directory, because we already specify that all git repo directory gonna be in all_git directory so we don\u0026rsquo;t need to check if it\u0026rsquo;s a git repo or not (one less task). So, here\u0026rsquo;s a comparison of the python script and shell script:\nThe python part below:\n1home = \u0026#39;/home/bruhtus/\u0026#39; 2dir_list = [dirname for dirname in os.listdir(f\u0026#39;{home}/all_git\u0026#39;) if os.path.isdir(f\u0026#39;{home}/all_git/{dirname}\u0026#39;) == True] 3 4for dirname in dir_list: is equivalent to this part in shell script:\n1for d in $(ls -d ~/all_git/*/); do Both of the script take list of directory in all_git directory and do a for loop to check each folder. You might be able to optimize my python script but i think that the shell script is much simpler than python script. What i mean simpler is less lines of code, not easier to understand. If you need an easier to understand code then python is the way, but it\u0026rsquo;s not really the topic of this post (everyone knows that python script is human readable, right?).\nLet\u0026rsquo;s continue, in the python part below:\n1 path = f\u0026#39;{home}/all_git/{dirname}\u0026#39; 2 os.system(f\u0026#39;echo {path}\u0026#39;) 3 4 if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: 5 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) 6 os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) 7 else: 8 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) is equivalent to this part in shell script:\n1 echo $d \u0026amp;\u0026amp; git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s 2done That second process is to check the git status and git submodules status of all directories. The part where there\u0026rsquo;s echo is basically print out anything that we give, in this case it\u0026rsquo;s gonna print directory name so i\u0026rsquo;m not gonna go into detail for that part.\nNow i\u0026rsquo;m gonna explain a little bit about \u0026amp;\u0026amp; command in shell, it\u0026rsquo;s basically let you run the second command (on the right side) if the first command (on the left side) success. So, it\u0026rsquo;s basically an equivalent of if-else statement in most programming language. The shell script part below:\n1git -C $d status -s \u0026amp;\u0026amp; ls -a $d | grep -q .gitmodules \u0026amp;\u0026amp; git -C $d submodule foreach git status -s is equivalent to this part of python script:\n1 if os.path.exists(f\u0026#39;{path}/.gitmodules\u0026#39;) == True: 2 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) 3 os.system(f\u0026#39;git -C {path}submodule foreach git status -s\u0026#39;) 4 else: 5 os.system(f\u0026#39;git -C {path}status -s\u0026#39;) The ls -a $d | grep -q .gitmodules is equivalent to if os.path.exists(f'{path}/.gitmodules') == True, both command check if there\u0026rsquo;s a git submodules in the git repo directory or not. So in the second test, i\u0026rsquo;m removing those if-else statement and then run only git status which makes shell script faster than python script.\nThe Conclusion For command line interface stuff, you should probably use shell script instead of python script. I\u0026rsquo;m not saying that python is bad, it\u0026rsquo;s just not the right tool for command line interface stuff. Yeah python can do almost everything but that doesn\u0026rsquo;t mean python is the best at everything, at least not at command line interface use case which is shell script clearly better here.\nReferences  Luke smith youtube channel . Distrotube youtube channel . Git command without change directory . Git submodule documentation .  "},{"ref":"https://bruhtus.github.io/posts/instasaver/","title":"Instasaver: Save Your Chosen Instagram Posts","section":"posts","tags":["No one asked","Python"],"date":"2020.11.09","body":" A brief explanation about instasaver, a tool to save instagram post build with instaloader python module and streamlit.\n Background Story  Nothing fancy, you should probably skip this.\n To make things short, basically i was inspired by Kevin Hazy\u0026rsquo;s project here . After i see Kevin\u0026rsquo;s project, i was like \u0026ldquo;can i make almost the same thing with python?\u0026rdquo; and that was the trigger.\nIf you ask me why i want a tool (kind of) to save instagram post, that\u0026rsquo;s because i have a hard time to save video memes on instagram and i don\u0026rsquo;t really want to use a screen recorder (said the guy who made something that took longer than learning how to use screen recorder and edit the result 👀). So anyway, that\u0026rsquo;s my motivation to made this. For the memes!\nInstaloader Python Module  In this part, i\u0026rsquo;m only gonna explain the part that i\u0026rsquo;ve used and tried along with the problem that i\u0026rsquo;ve encountered. So for the full documentations, you can check here .\n Instaloader Feature That I Used There\u0026rsquo;re a lot of things you can do with instaloader like saving instagram stories, follower list, following list, and so on, but for this project i only use saving post feature. Why i only use that feature? because other features require login to instagram account, i\u0026rsquo;ll explain on that later.\nSo, even only saving post feature has a lot going on but i\u0026rsquo;m not quite sure how to implement that, the only thing i\u0026rsquo;m sure is how to implement saving post using a url (well, also other features that require login too). If you want to learn other features you can check their documentations which is quite confusing. Well, maybe i\u0026rsquo;m just a complete idiot but you know what, you\u0026rsquo;re gonna have other complete idiots that didn\u0026rsquo;t understand their documentations. I don\u0026rsquo;t really want to be hard on them but they could have done better, especially on the examples (not only advance examples but basic examples too).\nInstaloader Feature That I\u0026rsquo;ve Tried Apart from saving post feature, i\u0026rsquo;ve also implemented saving stories, following list, and follower list. As i\u0026rsquo;ve mentioned above, those three features require login to instagram account. Well, you can made a fake account to login to instagram but it\u0026rsquo;s not ideal to deploy it. There\u0026rsquo;s this problem when you login quite often so you need to wait around 15 minute to use it again, i mean if i want everyone else to access it then that problem gonna be a hassle. I\u0026rsquo;ve never tried to make a lot of fake account and rotate through all of them to login but i might do it later, who knows. If you want to see all the features that i didn\u0026rsquo;t implement in the public version, you can check my github repo here .\nImplementation With Streamlit  In this part, i\u0026rsquo;m gonna explain the implementation with streamlit and how to deploy on streamlit sharing.\n Instaloader (Main Class) For starter, in instaloader main class (Instaloader ) the parameters that used in this project was dirname_pattern, download_comments, download_geotags, download_video_thumbnails, and save_metadata.\ndirname_pattern was to make the default folder to save the file which in this case i use temporary folder because i don\u0026rsquo;t want to save the images or videos on my github repo but download it to my device (whether smartphone or pc).\ndownload_comments, download_geotags, download_video_thumbnails, and save_metadata is set to False just to make it simple, i just want to download the memes (whether images or videos) and i don\u0026rsquo;t want anything else. You could turn that on to get all that stuff tho.\nShort code To extract the short code, i used regular expression from this source and then using instaloader Post.from_shortcode and download_post to finally download the post (saved in temporary folder first and then generate download button to save to device).\nPublic Version What i mean by public version is the version that i deploy so that everyone can use it. For the public version, i only use saving post with url input. The diagram process is below:\nAs you see above, you can download one item or multiple items in one post. Implementation for download instagram post with streamlit is first you download the post from url using instaloader download_post and saved it to temporary folder and then define exception handling to detect whether it\u0026rsquo;s an image or a video. The implementation is below:\n1import instaloader 2import streamlit as st 3import os 4from zipfile import ZipFile 5 6post = instaloader.Post.from_shortcode(instaloader.Instaloader.context, shortcode) 7instaloader.Instaloader.download_post(post, target=temp) #temp is temporary folder 8file_list = [filename for filename in os.listdir] 9 10if len(file_list) == 1: #if only one item in one post 11 try: 12 st.image(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, use_column_width=True) #use_column_width is to resize the width of the image displayed 13 st.markdown(download_button(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, temp), unsafe_allow_html=True) #download_button is to generate link to download the file 14 except: 15 st.video(f\u0026#39;{temp}/{file_list[0]}\u0026#39;) 16 st.markdown(download_button(f\u0026#39;{temp}/{file_list[0]}\u0026#39;, temp), unsafe_allow_html=True) #download_button is to generate link to download the file 17 18else: #if more than one item in one post 19 with ZipFile(f\u0026#39;{temp}/{shortcode}_posts.zip\u0026#39;, \u0026#39;w\u0026#39;) as zip: #put all the posts into zip file 20 for filename in file_list: 21 try: 22 st.image(f\u0026#39;{temp}/{filename}\u0026#39;, use_column_width=True) 23 st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) 24 zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) 25 except: 26 st.video(f\u0026#39;{temp}/{filename}\u0026#39;) 27 st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) 28 zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) 29 30 st.markdown(download_button(f\u0026#39;{temp}/{filename}\u0026#39;, temp), unsafe_allow_html=True) please keep in mind that the implementation above is only a partial from all the code, it\u0026rsquo;s just a glimpse of what\u0026rsquo;s going on inside the process. For the full code you can check here .\nLocal Version What i mean by local version is the version that has a more features than the public version that requires login to instagram account. For the local version, i use download the post from url feature (like the public version), download stories (download all the stories of the user, public profile only or you already followed the private account), and download following and follower list.\nDownload stories For download stories, i used download_stories module with user id as input and zip all the stories. Below is a diagram process of the download stories:\nglimpse of the process:\n1import instaloader 2from zipfile import ZipFile 3 4load = instaloader.Instaloader() 5profile = instaloader.Profile.from_username(load.context, username) 6profile_id = instaloader.Instaloader.check_profile_id(username) 7load.download_stories(userids=[profile_id.userid]) 8 9with ZipFile(f\u0026#39;{temp}/{profile.username}_stories.zip\u0026#39;, \u0026#39;w\u0026#39;_) as zip: 10 for filename in file_list: 11 zip.write(f\u0026#39;{temp}/{filename}\u0026#39;) 12 13st.markdown(download_button(f\u0026#39;{temp}/{profile.username}_stories.zip\u0026#39;, temp), unsafe_allow_html=True) Download following and follower list For download following and follower list, i used get_followees and get_followers module with username as input and write all user\u0026rsquo;s following and follower username in .txt file.\nglimpse of the process (download following list):\n1import instaloader 2 3load = instaloader.Instaloader() 4profile = instaloader.Profile.from_username(load.context, username) 5 6with open(f\u0026#39;{temp}/{profile.username}_following.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: 7 for following in profile.get_followees(): 8 f.write(f\u0026#39;{following.username}\\n\u0026#39;) 9 10st.markdown(download_button(f\u0026#39;{temp}/{profile.username}_follower.txt\u0026#39;, temp), unsafe_allow_html=True) Deployment I use streamlit sharing and heroku to deploy this project, why i use two services to deploy this project? that\u0026rsquo;s because i want to try web app deployement on heroku and streamlit sharing (which is quite new).\nStreamlit Sharing To deploy on streamlit sharing you need to request an invite in their website and then after that you can deploy your streamlit app. It takes a few minutes to deploy the first time but after that it\u0026rsquo;s deploy in an instant. You can check the live demo on streamlit sharing here .\nHeroku To deploy on heroku, you need a few thing and here\u0026rsquo;s the list:\n runtime.txt -\u0026gt; to specify python version. Procfile -\u0026gt; to specify type of our application and run command, check here for more info. create_config.sh (or setup.sh, whatever you want) -\u0026gt; to make config.toml for streamlit to run.  I don\u0026rsquo;t really need to explain about runtime.txt, do i? you just need to type your python version, for example python-3.7.5, that\u0026rsquo;s all (i explain it anyway, dammit).\nFor .sh file (in this case i\u0026rsquo;m gonna name it create_config.sh because i\u0026rsquo;m not creative, sorry), type this:\n1mkdir -p ~/.streamlit 2 3echo \u0026#34;[server] 4headless = true 5port = $PORT6enableCORS = false 7\u0026#34; \u0026gt; ~/.streamlit/config.toml For Procfile, you don\u0026rsquo;t need to add an extention there. Just Procfile is enough. In the Procfile, type this:\nweb: sh create_config.sh \u0026amp;\u0026amp; streamlit run instasaver.py You don\u0026rsquo;t need to name it create_config.sh, be creative, don\u0026rsquo;t be like me.\nAfter all that, you can deploy it using git or from github or something else (i don\u0026rsquo;t remember all the choices). If you want to deploy it using git workflow, you can check here .\nFor live demo on heroku, you can check here .\nReferences  The one who trigger me to made this . Instaloader documentations . Streamlit . Download instagram stories . View public profile anonymous (more advance version) . Streamlit multiselect nested in if . Streamlit download file . Deploy streamlit app on heroku example .  "},{"ref":"https://bruhtus.github.io/posts/pavement-distress-detector/","title":"Pavement Distress Detector Using Single Shot Detector (SSD)","section":"posts","tags":["Deep learning","Python"],"date":"2020.10.11","body":" This was my graduation project, in this article i\u0026rsquo;m gonna explain what i did in my graduation project. This project was based on Congcong Li\u0026rsquo;s Project .\n Diagram Process of This Project A Brief Explanation About Single Shot Detector (SSD) Single shot detector is a deep learning method presented by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed4, Cheng-Yang Fu, Alexander C. Berg in their research paper SSD: Single Shot Multibox Detector . There are 2 commonly used SSD model, that is, SSD300 and SSD512. Here\u0026rsquo;s a brief explanation about SSD300 and SSD512:\n SSD300: More fast. SSD512: More accurate.  Long story short, SSD300 is about speed. If you need speed than you should probably use SSD300 (i haven\u0026rsquo;t tried the mobilenet as base network at the time to type this, so at this time knowledge SSD300 is faster than SSD512). Meanwhile, SSD512 is about accuracy. It doesn\u0026rsquo;t really show up in image processing but in video processing, i notice that there\u0026rsquo;s a frame rate drop while doing live object detection. To be fair, SSD300 has frame rate drop as well but it\u0026rsquo;s still usable (around 7-10 frame per second) but SSD512 has frame rate around 3-5 frame per second. Who want to watch a video with 3 fps?? If you\u0026rsquo;re that kind of person then, go ahead. You do you mate.\nFor the record, at that time when I try live detection, i use opencv to display live detection video. i\u0026rsquo;m not sure whether it is opencv fault or the model fault because if I save the video result, the video itself has no frame rate drop. It\u0026rsquo;s weird but it happens, so let\u0026rsquo;s go on with saving the video and forget about live detection (for now, until i find some way to do live detection).\nSo, in this project i\u0026rsquo;m not gonna make it live detection. Rather than live detections, we\u0026rsquo;re gonna save the video result first and then display it later. That way it could also reduce some computational cost.\nFor those who still confused about live detection, to make things simpler, live detection is when you process the video, detect the object, and play the video at the same time. After you detect the object, you immediately display the frame that just recently processed and then processed the next frame. Repeat.\nSingle Shot Detector (SSD) Architecture That\u0026rsquo;s Used in This Project As explained above, in this project we\u0026rsquo;re gonna use SSD512. SSD512 is basically SSD with input image 512x512. The basic architecture of SSD contains 2 part, base network and extra feature layers. The base network layers are based on standard architecture used for high quality image classification (truncated before classification layers). The extra feature layers used for multi-scale feature maps for detection and convolutional predictors for detection.\nHere is an architecture single shot detector that used in this project (made this with NN architecture maker ):\nInformation:\n Input image. Base Network (truncated before classification layers). Layer 6 and layer 7 of base network (from fully-connected layer turned into classification layer). Extra feature layers. Collection of boxes and scores.  Base Network The base network used in this project is Visual Geometry Group (VGG). I chose VGG because of transfer learning capability so that i could have a good result with small dataset. To be more specific, in this project i used VGG16, here is a brief explanation of each layers:\n In the first layer, there\u0026rsquo;s a convolutional process with kernel filter 3x3 and stride (total shift filter per pixel) 1 pixel. That process repeat 2 times and then did some max pooling with kernel filter 2x2 and stride 2 pixel. In the second layer until fourth layer, the model did the same thing as in first layer. The difference was in fifth layer. In fifth layer, the convolution process still the same as the other four layers but the max pooling process was different from the other four layers. The max pooling process used kernel filter 3x3 with stride 1 pixel with padding (adding zero value around pixel image) 1. You can check the illustration below to understand the process of max pooling with kernel filter 3x3, stride 1, and padding 1.  And here\u0026rsquo;s a VGG16 after truncated from classification layers:\nIf you want to calculate the result from max polling, you can use this equation 1:\nInformation:\n kernel_size, stride, padding, and dilation can be 1 integer (in this case, the value for height and width are the same) or 2 integer (in this case, the first integer is height and the second integer is width). For more info you can see pytorch page .  Here\u0026rsquo;s some example of max pooling calculation with input 32x32, kernel filter 3x3, stride 1, padding 1, and dilation 1:\nLayer 6 and Layer 7 After feature extraction process in base network, the next layers is to change layer 6 and 7 of base network from fully-connected into convolutional layer with subsample parameters from fully-connected 6 (fc6) and fully-connected 7 (fc7). The convolution operation used in layer 6 and layer 7 is atrous convolution, you can see atrous convolution shift below:\nWith atrous convolution we can expand area of observation for feature extraction while maintaning the amount of parameters fewer than traditional convolution operation.\nExtra Feature Layers Extra feature layers is a prediction layers. In this layer, the model predict the object using default box. Default box is a box with various aspect ratio in every location of feature maps with different size. You can see an example of default box below 2\nIn the last layer is a collection of default boxes which closer to ground truth box with confidence score from that default boxes.\nTake A Video (Training Video and Testing Video) In this part, i\u0026rsquo;m gonna explain about the video used in this project. The camera configuration, the place where the video taken, the camera angle and height from the road.\nThe place where the video taken was in Surabaya, at Kertajaya Indah Timur IX, Kertajaya Indah Timur X, and Kertajaya Indah Timur XI. The camera angle was perpendicular(?) with the road (90 degrees) and the camera position from the road was 200 cm.\nThere\u0026rsquo;re 7 video taken, 3 for training and 4 for testing. The format of the video was *.mp4. You can check the location partition of the video taken below:\nThe black block is for testing and the white block is for training. You can check the position of the camera below:\nSetting Up The Config File For more detailed configuration please check develop guide by Congcong Li . In this post i\u0026rsquo;m gonna explain it the easiest way.\nBasic Configuration To make things easier, copy the format dataset you want. For example, in this project i want to use COCO dataset format. Then, i copied the coco.py in the path ssd/data/datasets/ and rename it to my_dataset.py. After that, edit the class names for your classification class. In this project, the class i\u0026rsquo;m gonna use is alligator crack, longitudinal crack, transverse crack, and pothole. Also, don\u0026rsquo;t forget to change the class COCODataset to MyDataset.\nThe next step is to add those configuration to __init__.py in ssd/data/datasets/. For example:\n1from .my_dataset import MyDataset 2 3_DATASETS = { 4 \u0026#39;VOCDataset\u0026#39;: VOCDataset, 5 \u0026#39;COCODataset\u0026#39;: COCODataset, 6 \u0026#39;MyDataset\u0026#39;: MyDataset, 7} Another next step is to add the path of your datasets and anotations to the path_catlog.py in ssd/config/. For example:\n1import os 2 3class DatasetCatalog: 4 DATA_DIR = \u0026#39;datasets\u0026#39; 5 DATASETS = { 6 \u0026#39;my_custom_train_dataset\u0026#39;: { 7 \u0026#34;data_dir\u0026#34;: \u0026#34;train\u0026#34;, 8 \u0026#34;ann_file\u0026#34;: \u0026#34;annotations/train.json\u0026#34; 9 }, 10 11 \u0026#39;my_custom_validation_dataset\u0026#39;: { 12 \u0026#34;data_dir\u0026#34;: \u0026#34;validation\u0026#34;, 13 \u0026#34;ann_file\u0026#34;: \u0026#34;annotations/validation.json\u0026#34; 14 }, 15 } 16 17 @staticmethod 18 def get(name): 19 if \u0026#34;my_custom_train_dataset\u0026#34; in name: 20 my_custom_train_dataset = DatasetCatalog.DATA_DIR 21 attrs = DatasetCatalog.DATASETS[name] 22 args = dict( 23 data_dir = os.path.join(my_custom_train_dataset, attrs[\u0026#39;data_dir\u0026#39;]), 24 ann_file = os.path.join(my_custom_train_dataset, attrs[\u0026#39;ann_file\u0026#39;]), 25 ) 26 return dict(factory=\u0026#34;MyDataset\u0026#34;, args=args) 27 28 elif \u0026#34;my_custom_test_dataset\u0026#34; in name: 29 my_custom_train_dataset = DatasetCatalog.DATA_DIR 30 attrs = DatasetCatalog.DATASETS[name] 31 args = dict( 32 data_dir = os.path.join(my_custom_train_dataset, attrs[\u0026#39;data_dir\u0026#39;]), 33 ann_file = os.path.join(my_custom_train_dataset, attrs[\u0026#39;ann_file\u0026#39;]), 34 ) 35 return dict(factory=\u0026#34;MyDataset\u0026#34;, args=args) And finally, for the *.yaml file for configuration i copied vgg_ssd512_coco_trainval35k.yaml in configs folder and rename it to config.yaml. What i changed from that file was the train and test (or more like validation) like in path_catlog.py, the batch size, and num_classes. I changed batch size because my laptop gpu only capable of 4 batch size. Here\u0026rsquo;s an example:\n1Model: 2 num_classes: 5 #the __background__ counted 3 ... 4 DATASETS: 5 TRAIN: (\u0026#34;my_custom_train_dataset\u0026#34;, ) 6 TEST: (\u0026#34;my_custom_test_dataset\u0026#34;, ) 7 SOLVER: 8 ... 9 BATCH_SIZE: 4 10 ... 11 12 OUTPUT_DIR: \u0026#39;outputs/ssd_custom_coco_format\u0026#39; You don\u0026rsquo;t need to create folder ssd_custom_coco_format, when the training begin the folder gonna created automatically (if the folder didn\u0026rsquo;t exist).\nValidation Configuration First of all, copy coco folder in ssd/data/datasets/evaluation/ and rename it to my_dataset. Rename the def coco_evaluation to def my_dataset_evaluation in file __init__.py. After that, add folder my_dataset to file __init__.py in ssd/data/datasets/evaluation/. For example:\n1from ssd.data.datasets import VOCDataset, COCODataset, MyDataset 2... 3from .my_dataset import my_dataset_evaluation 4 5def evaluate(dataset, predictions, output_dir, **kwargs): 6 ... 7 elif isinstance(dataset, MyDataset); 8 return my_dataset_evaluation(**args) 9 else: 10 raise NotImplementError The Interface For the interface, i\u0026rsquo;m using python library streamlit. Streamlit is more like web interface rather than common graphical user interface (GUI). Here\u0026rsquo;s how the interface looks like:\nThere\u0026rsquo;s a problem with file uploader at the time (streamlit version 0.59.0), i can\u0026rsquo;t upload file larger than 100 mb meanwhile the limit of file uploader should be 200 mb at that time. You can check the issue here , it seems like they\u0026rsquo;re already fixed it but i haven\u0026rsquo;t try it yet. So, when making this project i\u0026rsquo;m using the dropdown menu bar.\nTraining Preparation Before training the model, we need to do some preparation. There\u0026rsquo;re two steps in this process, frame extraction and labeling. Without further ado, let\u0026rsquo;s get started.\nFrame Extraction In this process, i used python library opencv to extract some frame. Here\u0026rsquo;s the script:\n1import cv2 2import time 3from fire import Fire 4from tqdm import tqdm 5 6def main(video_file, path_save, speed): # the lower the speed the fastest the frame_rates, speed = 0 (pause) 7 vidcap = cv2.VideoCapture(video_file) 8 current_frame = 0 9 speed_frame = speed 10 11 while (vidcap.isOpened()): 12 success, frame = vidcap.read() # success = retrival value for frame 13 length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) 14 print(f\u0026#39;Current Frame: {current_frame}/{length}\u0026#39;) 15 current_frame += 1 16 17 if success == True: 18 cv2.imshow(\u0026#39;Video\u0026#39;, frame) 19 if cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;s\u0026#39;): # press s to save the frame 20 cv2.imwrite(f\u0026#34;{path_save}/frame_{current_frame}.jpg\u0026#34;, frame) 21 22 elif cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): # press q to quit 23 break 24 25 elif cv2.waitKey(speed) \u0026amp; 0xFF == ord(\u0026#39;w\u0026#39;): # play/pause 26 if speed != 0: 27 speed = 0 28 elif speed == 0: 29 speed = speed_frame 30 31 else: 32 vidcap.release() 33 cv2.destroyAllWindows() 34 35if __name__ == \u0026#39;__main__\u0026#39;: 36 Fire(main) Every time we press s, it\u0026rsquo;s gonna take the current frame at that time. For the speed, i usually go for 25 but if you want slower you could change it to 10 or lower (as long as it\u0026rsquo;s not 0, please).\nAfter the extraction process, i have 652 images/frames for training process. The 652 images/frames have this proportion (There\u0026rsquo;re a few object in one frame):\n   Pavement Distress Object     Alligator Crack 367   Longitudinal Crack 951   Transverse Crack 243   Potholes 161    Labeling For the labeling i use labelme, you could check the tutorial here and to change labelme format to coco dataset format here . There\u0026rsquo;s nothing much to explain about labeling, you just give box to an object and save with the label you want. So, let\u0026rsquo;s move on.\nHere We Go, It\u0026rsquo;s Training Time! For the training process i use google colaboratory (how to use google colaboratory is beyond this post, sorry) but you could also use other services such as paperspace . Here\u0026rsquo;s an example of command line if you use you local machine or cloud services: Local: 1python train.py --config-file configs/config.yaml Cloud: !python train.py --config-file configs/config.yaml Basically there\u0026rsquo;s no difference so i think it\u0026rsquo;s not that difficult, good luck.\nLoss Function Graph As the training begin, please don\u0026rsquo;t forget to check the loss function. The closer the loss function to zero the better but be carefull so that it doesn\u0026rsquo;t overfitting (a model memorized the training data and have difficulty predicting the testing data). Here\u0026rsquo;s the unscientific tips from me, stop the training process if you don\u0026rsquo;t see any improvement in loss function. For example, if the loss function stuck at 0.9 - 0.5 for quite some time then you should stop the process. Here\u0026rsquo;s my loss function graph:\nTesting Preparation Before testing the model, there\u0026rsquo;re a few things we need to do:\n Copy or move video you want to use into folder input. Copy or move configuration file (*.yaml) into folder configs. Copy or move folder that has training result into folder outputs (in this project the folder name is ssd_custom_coco_format). The folder name must be the same as in configuration file OUTPUT_DIR. If every file and folder in the right places, then let\u0026rsquo;s move on.  It\u0026rsquo;s Testing Time! For this project, there\u0026rsquo;s a problem with the counting. Because i have no idea how to implement tracking so i made the counting in the iteration frame (detection at every frame, which is insane) and that\u0026rsquo;s makes the total counting more than the actual object. To fix this problem (kind of), i do the counting for every 20 frames. The reason was because at every 20 frames, the object detected was closer to the total of actual object than every 10, 15, 25, and 30 frames. So, for the evaluation i\u0026rsquo;m gonna evaluate the detection result every 20 frames. Thanks.\nA Brief Showcase and Explanation of The Results Below is the result:\nVideo Testing 1    Class Name Counting Results Actual Objects     Alligator Crack 2 3   Longitudinal Crack 4 29   Transverse Crack 8 11   Potholes 1 2       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 2 11 0 1   Longitudinal Crack 4 9 1 25   Transverse Crack 6 7 1 5   Potholes 1 12 0 1    Video Testing 2    Class Name Counting Results Actual Objects     Alligator Crack 14 8   Longitudinal Crack 5 6   Transverse Crack 1 4   Potholes 0 2       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 7 3 7 1   Longitudinal Crack 2 8 2 4   Transverse Crack 1 9 0 3   Potholes 0 10 0 2    Video Testing 3    Class Name Counting Results Actual Objects     Alligator Crack 21 8   Longitudinal Crack 7 15   Transverse Crack 1 1   Potholes 2 4       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 8 7 10 0   Longitudinal Crack 4 11 2 11   Transverse Crack 1 14 0 0   Potholes 2 13 0 2    Video Testing 4    Class Name Counting Results Actual Objects     Alligator Crack 23 22   Longitudinal Crack 13 46   Transverse Crack 4 12   Potholes 5 22       Class Name True Positive True Negative False Positive False Negative     Alligator Crack 10 14 8 12   Longitudinal Crack 8 16 3 38   Transverse Crack 2 22 2 10   Potholes 4 20 0 18    From the counting results above we can see that the model struggle to detect longitudinal crack and have a lot of alligator crack detections (detected two times or more). There\u0026rsquo;re 2 reasons for that, the first is that there\u0026rsquo;s not enough small-sized longitudinal crack in training dataset and the second is the frame field-of-view too narrow so that a lot of alligator crack devided into different frames and detected multiple times. Here\u0026rsquo;s an example of that problem:\n Undetected Small Longitudinal Crack:   Multiple Detection of Alligator Crack in Different Frames (there\u0026rsquo;re 2 frames below):  And then, we have the precision and recall of the model as below:\n   Video Precision Recall Accuracy     Video Testing 1 91.43% 46.25% 60.69%   Video Testing 2 50% 36.45% 69.58%   Video Testing 3 77.78% 69.17% 75.45%   Video Testing 4 69.57% 24.42% 53.81%     At the time of making this post, i\u0026rsquo;m still not sure whether to use accuracy or f1-score so for now i\u0026rsquo;m gonna use accuracy.\n The difference between video testing 1 to 4 is the total of small-sized pavement distress and video testing 3 has the least total of small-sized pavement distress of all video. So that means, for this trained weight, we obtain the best accuracy when we have the least small-sized pavement distress.\nConclusion The perfomance of single shot detector is not bad or maybe you could say it\u0026rsquo;s good. Considering the lack of training data, it still can produce above 50% accuracy so you might say that this project has the worst result possible. There\u0026rsquo;re a lot of things you could improve, especially in the amount of training dataset. Good luck!\nFuture Suggestion By no means this is not the best implementation of SSD for pavement distress detection. I have only a few training dataset and a few testing dataset. So you could say what i did here is a minimum requirement that result in minimum performance. You can improve this project quite a lot.\nIf you want to improve this project, you can start from these things:\n Use a lot of training data and testing data. Use a camera that has wide angle lens (because 50mm lens is not wide enough).  Side Notes This is not really important, it\u0026rsquo;s more like a momento for me. In the undergraduate thesis defence(?) there\u0026rsquo;s this examiner who has a misconception about testing process and validation process. That examiner switch the possition of testing process as validation process and validation process as testing process, so that really confusing and we have quite an argument there. I even ask in stackexchange if i\u0026rsquo;m wrong or not and it turns out that examiner has switch the term for testing and validation. Now i feel stupid for having an argument with that examiner.\n  Pytorch maxpool2d .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n SSD: Single Shot Multibox Detector .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "}]